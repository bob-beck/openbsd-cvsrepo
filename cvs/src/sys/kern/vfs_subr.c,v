head	1.260;
access;
symbols
	OPENBSD_6_1:1.257.0.4
	OPENBSD_6_1_BASE:1.257
	OPENBSD_6_0:1.249.0.2
	OPENBSD_6_0_BASE:1.249
	OPENBSD_5_9:1.238.0.2
	OPENBSD_5_9_BASE:1.238
	OPENBSD_5_8:1.232.0.4
	OPENBSD_5_8_BASE:1.232
	OPENBSD_5_7:1.229.0.2
	OPENBSD_5_7_BASE:1.229
	OPENBSD_5_6:1.218.0.4
	OPENBSD_5_6_BASE:1.218
	OPENBSD_5_5:1.211.0.4
	OPENBSD_5_5_BASE:1.211
	OPENBSD_5_4:1.204.0.2
	OPENBSD_5_4_BASE:1.204
	OPENBSD_5_3:1.202.0.2
	OPENBSD_5_3_BASE:1.202
	OPENBSD_5_2:1.197.0.2
	OPENBSD_5_2_BASE:1.197
	OPENBSD_5_1_BASE:1.195
	OPENBSD_5_1:1.195.0.4
	OPENBSD_5_0:1.195.0.2
	OPENBSD_5_0_BASE:1.195
	OPENBSD_4_9:1.193.0.2
	OPENBSD_4_9_BASE:1.193
	OPENBSD_4_8:1.187.0.2
	OPENBSD_4_8_BASE:1.187
	OPENBSD_4_7:1.184.0.2
	OPENBSD_4_7_BASE:1.184
	OPENBSD_4_6:1.179.0.4
	OPENBSD_4_6_BASE:1.179
	OPENBSD_4_5:1.175.0.2
	OPENBSD_4_5_BASE:1.175
	OPENBSD_4_4:1.173.0.2
	OPENBSD_4_4_BASE:1.173
	OPENBSD_4_3:1.161.0.2
	OPENBSD_4_3_BASE:1.161
	OPENBSD_4_2:1.155.0.2
	OPENBSD_4_2_BASE:1.155
	OPENBSD_4_1:1.139.0.2
	OPENBSD_4_1_BASE:1.139
	OPENBSD_4_0:1.133.0.2
	OPENBSD_4_0_BASE:1.133
	OPENBSD_3_9:1.122.0.2
	OPENBSD_3_9_BASE:1.122
	OPENBSD_3_8:1.114.0.2
	OPENBSD_3_8_BASE:1.114
	OPENBSD_3_7:1.109.0.2
	OPENBSD_3_7_BASE:1.109
	OPENBSD_3_6:1.103.0.2
	OPENBSD_3_6_BASE:1.103
	SMP_SYNC_A:1.100
	SMP_SYNC_B:1.100
	OPENBSD_3_5:1.97.0.2
	OPENBSD_3_5_BASE:1.97
	OPENBSD_3_4:1.95.0.2
	OPENBSD_3_4_BASE:1.95
	UBC_SYNC_A:1.93
	OPENBSD_3_3:1.88.0.4
	OPENBSD_3_3_BASE:1.88
	OPENBSD_3_2:1.88.0.2
	OPENBSD_3_2_BASE:1.88
	OPENBSD_3_1:1.83.0.2
	OPENBSD_3_1_BASE:1.83
	UBC_SYNC_B:1.88
	UBC:1.79.0.2
	UBC_BASE:1.79
	OPENBSD_3_0:1.68.0.2
	OPENBSD_3_0_BASE:1.68
	OPENBSD_2_9_BASE:1.58
	OPENBSD_2_9:1.58.0.2
	OPENBSD_2_8:1.47.0.2
	OPENBSD_2_8_BASE:1.47
	OPENBSD_2_7:1.45.0.2
	OPENBSD_2_7_BASE:1.45
	SMP:1.43.0.2
	SMP_BASE:1.43
	kame_19991208:1.43
	OPENBSD_2_6:1.41.0.2
	OPENBSD_2_6_BASE:1.41
	OPENBSD_2_5:1.36.0.2
	OPENBSD_2_5_BASE:1.36
	OPENBSD_2_4:1.23.0.2
	OPENBSD_2_4_BASE:1.23
	OPENBSD_2_3:1.19.0.2
	OPENBSD_2_3_BASE:1.19
	OPENBSD_2_2:1.12.0.2
	OPENBSD_2_2_BASE:1.12
	OPENBSD_2_1:1.10.0.2
	OPENBSD_2_1_BASE:1.10
	OPENBSD_2_0:1.5.0.2
	OPENBSD_2_0_BASE:1.5
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.260
date	2017.07.31.16.47.03;	author florian;	state Exp;
branches;
next	1.259;
commitid	HFwUb6riXpWnlR73;

1.259
date	2017.04.20.14.13.00;	author visa;	state Exp;
branches;
next	1.258;
commitid	GnoPKa34InShCqYl;

1.258
date	2017.04.04.18.17.02;	author deraadt;	state Exp;
branches;
next	1.257;
commitid	jocBB7XkbgkXWf7K;

1.257
date	2017.01.15.23.18.05;	author bluhm;	state Exp;
branches;
next	1.256;
commitid	0SyY1JeavaPEHaok;

1.256
date	2017.01.10.19.56.34;	author bluhm;	state Exp;
branches;
next	1.255;
commitid	eR3oTWRZObY3zDAD;

1.255
date	2017.01.10.19.48.32;	author bluhm;	state Exp;
branches;
next	1.254;
commitid	nzxicVheAaFpfTKW;

1.254
date	2016.09.28.22.22.52;	author kettenis;	state Exp;
branches;
next	1.253;
commitid	AXOSqc8zBpjaXViw;

1.253
date	2016.09.16.03.21.16;	author dlg;	state Exp;
branches;
next	1.252;
commitid	qpxurnuozCzNjzBV;

1.252
date	2016.09.16.02.54.51;	author dlg;	state Exp;
branches;
next	1.251;
commitid	KstuxUpRI6RRN5mJ;

1.251
date	2016.09.15.02.00.16;	author dlg;	state Exp;
branches;
next	1.250;
commitid	RlO92XR575sygHqm;

1.250
date	2016.08.25.00.01.13;	author dlg;	state Exp;
branches;
next	1.249;
commitid	qGDK47LQhxLxADuT;

1.249
date	2016.07.22.09.54.09;	author kettenis;	state Exp;
branches;
next	1.248;
commitid	FOkUXyTuGtfcMCbq;

1.248
date	2016.06.19.11.54.33;	author natano;	state Exp;
branches;
next	1.247;
commitid	wHLNY5GFNXJSFYaC;

1.247
date	2016.05.26.16.03.29;	author natano;	state Exp;
branches;
next	1.246;
commitid	ehJudmeVJQv8BMD4;

1.246
date	2016.04.26.18.23.07;	author natano;	state Exp;
branches;
next	1.245;
commitid	EMAmsJV33qy6S3QQ;

1.245
date	2016.04.26.09.51.22;	author beck;	state Exp;
branches;
next	1.244;
commitid	gli39GfaNJ5gbI9G;

1.244
date	2016.04.07.09.58.11;	author natano;	state Exp;
branches;
next	1.243;
commitid	KsGV8YRYa3aWDVd1;

1.243
date	2016.04.05.19.26.15;	author natano;	state Exp;
branches;
next	1.242;
commitid	yk8EkL7swzNNGn3c;

1.242
date	2016.04.01.11.51.55;	author mikeb;	state Exp;
branches;
next	1.241;
commitid	WRmLf3alaeLrWDOA;

1.241
date	2016.03.31.20.00.17;	author natano;	state Exp;
branches;
next	1.240;
commitid	MnVY7egfe4wB5xs8;

1.240
date	2016.03.19.12.04.15;	author natano;	state Exp;
branches;
next	1.239;
commitid	gAjwyca5TfuoJAhn;

1.239
date	2016.03.14.23.08.06;	author krw;	state Exp;
branches;
next	1.238;
commitid	kCz5QgxnxRMKOzNf;

1.238
date	2015.12.05.10.11.53;	author tedu;	state Exp;
branches
	1.238.2.1;
next	1.237;
commitid	Cl55DD2g2xm69E6W;

1.237
date	2015.11.16.18.23.50;	author deraadt;	state Exp;
branches;
next	1.236;
commitid	02UpPapRARDDbRKo;

1.236
date	2015.10.13.09.11.48;	author guenther;	state Exp;
branches;
next	1.235;
commitid	xOyGCZ1NV2X6Cpsy;

1.235
date	2015.10.08.08.41.58;	author mpi;	state Exp;
branches;
next	1.234;
commitid	OesGi9XmSKvtapL6;

1.234
date	2015.10.07.11.57.44;	author mpi;	state Exp;
branches;
next	1.233;
commitid	LJy1v4RiHUSLry9o;

1.233
date	2015.09.04.08.43.39;	author mpi;	state Exp;
branches;
next	1.232;
commitid	qAevExm24QrBjVNL;

1.232
date	2015.07.16.18.17.27;	author claudio;	state Exp;
branches
	1.232.4.1;
next	1.231;
commitid	VvtF6H2REt7RsEKf;

1.231
date	2015.05.12.09.30.35;	author mikeb;	state Exp;
branches;
next	1.230;
commitid	A2ab5qVexJuqZwB3;

1.230
date	2015.03.14.03.38.51;	author jsg;	state Exp;
branches;
next	1.229;
commitid	p4LJxGKbi0BU2cG6;

1.229
date	2015.03.02.20.46.50;	author guenther;	state Exp;
branches;
next	1.228;
commitid	zq3ATDtt3EFolfIj;

1.228
date	2015.01.09.05.01.56;	author tedu;	state Exp;
branches;
next	1.227;
commitid	KWogeIYA2sxG3IjB;

1.227
date	2014.12.19.05.59.21;	author tedu;	state Exp;
branches;
next	1.226;
commitid	zdJTCwdpqRUwO1SL;

1.226
date	2014.12.17.19.42.15;	author tedu;	state Exp;
branches;
next	1.225;
commitid	G4ldVK4QwvfU3tRp;

1.225
date	2014.12.16.18.30.04;	author tedu;	state Exp;
branches;
next	1.224;
commitid	P6Av4XGqOi3rFasL;

1.224
date	2014.12.10.02.44.47;	author tedu;	state Exp;
branches;
next	1.223;
commitid	tsoJBlEBSyYO22RG;

1.223
date	2014.11.21.07.14.17;	author tedu;	state Exp;
branches;
next	1.222;
commitid	A39DyiLJKg2zWQeB;

1.222
date	2014.11.19.18.04.54;	author tedu;	state Exp;
branches;
next	1.221;
commitid	DhpzIJGhCsFp0uYg;

1.221
date	2014.11.14.23.26.48;	author tedu;	state Exp;
branches;
next	1.220;
commitid	Xw9eD6Y0JOIFIwDy;

1.220
date	2014.11.03.03.08.00;	author deraadt;	state Exp;
branches;
next	1.219;
commitid	3SGDR5EjcwE01W8S;

1.219
date	2014.09.13.16.06.37;	author doug;	state Exp;
branches;
next	1.218;
commitid	jdBY2kKXhfcoQitp;

1.218
date	2014.07.13.15.00.40;	author tedu;	state Exp;
branches;
next	1.217;
commitid	7E1o7NIDvSyD48ls;

1.217
date	2014.07.12.18.43.32;	author tedu;	state Exp;
branches;
next	1.216;
commitid	QlVV51SZgNFxsXxC;

1.216
date	2014.07.10.12.21.08;	author mpi;	state Exp;
branches;
next	1.215;
commitid	rMrMHPG4SqHlsP5W;

1.215
date	2014.07.08.17.19.25;	author deraadt;	state Exp;
branches;
next	1.214;
commitid	EF98ch02VpFassUi;

1.214
date	2014.06.04.07.58.14;	author claudio;	state Exp;
branches;
next	1.213;
commitid	UIUmMgTXqZXBplEA;

1.213
date	2014.04.10.13.48.24;	author tedu;	state Exp;
branches;
next	1.212;

1.212
date	2014.03.24.00.19.48;	author guenther;	state Exp;
branches;
next	1.211;

1.211
date	2014.01.21.01.48.45;	author tedu;	state Exp;
branches;
next	1.210;

1.210
date	2013.12.01.16.40.56;	author krw;	state Exp;
branches;
next	1.209;

1.209
date	2013.11.27.15.50.52;	author jsing;	state Exp;
branches;
next	1.208;

1.208
date	2013.10.02.21.29.21;	author sf;	state Exp;
branches;
next	1.207;

1.207
date	2013.10.01.20.15.56;	author sf;	state Exp;
branches;
next	1.206;

1.206
date	2013.08.08.23.25.06;	author syl;	state Exp;
branches;
next	1.205;

1.205
date	2013.07.30.17.07.56;	author beck;	state Exp;
branches;
next	1.204;

1.204
date	2013.06.24.18.52.37;	author beck;	state Exp;
branches;
next	1.203;

1.203
date	2013.04.15.15.32.19;	author jsing;	state Exp;
branches;
next	1.202;

1.202
date	2013.02.17.17.39.29;	author miod;	state Exp;
branches;
next	1.201;

1.201
date	2013.02.09.20.56.35;	author miod;	state Exp;
branches;
next	1.200;

1.200
date	2012.11.17.23.08.22;	author beck;	state Exp;
branches;
next	1.199;

1.199
date	2012.10.01.00.08.43;	author guenther;	state Exp;
branches;
next	1.198;

1.198
date	2012.09.19.00.53.13;	author guenther;	state Exp;
branches;
next	1.197;

1.197
date	2012.07.16.15.31.17;	author deraadt;	state Exp;
branches;
next	1.196;

1.196
date	2012.07.16.15.20.39;	author deraadt;	state Exp;
branches;
next	1.195;

1.195
date	2011.07.04.20.35.35;	author deraadt;	state Exp;
branches;
next	1.194;

1.194
date	2011.07.02.15.52.25;	author thib;	state Exp;
branches;
next	1.193;

1.193
date	2010.12.21.20.14.43;	author thib;	state Exp;
branches;
next	1.192;

1.192
date	2010.12.06.18.44.49;	author jasper;	state Exp;
branches;
next	1.191;

1.191
date	2010.09.10.16.34.08;	author thib;	state Exp;
branches;
next	1.190;

1.190
date	2010.09.06.23.44.10;	author thib;	state Exp;
branches;
next	1.189;

1.189
date	2010.08.12.15.00.17;	author oga;	state Exp;
branches;
next	1.188;

1.188
date	2010.08.11.14.35.34;	author beck;	state Exp;
branches;
next	1.187;

1.187
date	2010.06.29.04.09.32;	author tedu;	state Exp;
branches;
next	1.186;

1.186
date	2010.06.28.18.50.36;	author claudio;	state Exp;
branches;
next	1.185;

1.185
date	2010.05.06.06.53.09;	author mpf;	state Exp;
branches;
next	1.184;

1.184
date	2009.12.17.16.44.12;	author oga;	state Exp;
branches;
next	1.183;

1.183
date	2009.08.17.13.11.58;	author jasper;	state Exp;
branches;
next	1.182;

1.182
date	2009.08.13.13.49.20;	author thib;	state Exp;
branches;
next	1.181;

1.181
date	2009.08.12.16.42.24;	author beck;	state Exp;
branches;
next	1.180;

1.180
date	2009.08.02.16.28.40;	author beck;	state Exp;
branches;
next	1.179;

1.179
date	2009.06.25.15.49.26;	author thib;	state Exp;
branches;
next	1.178;

1.178
date	2009.06.15.17.01.26;	author beck;	state Exp;
branches;
next	1.177;

1.177
date	2009.06.06.18.06.22;	author art;	state Exp;
branches;
next	1.176;

1.176
date	2009.06.03.04.30.57;	author beck;	state Exp;
branches;
next	1.175;

1.175
date	2008.11.10.11.53.16;	author pedro;	state Exp;
branches;
next	1.174;

1.174
date	2008.11.01.20.34.09;	author deraadt;	state Exp;
branches;
next	1.173;

1.173
date	2008.07.05.12.48.03;	author thib;	state Exp;
branches;
next	1.172;

1.172
date	2008.06.14.10.55.21;	author mk;	state Exp;
branches;
next	1.171;

1.171
date	2008.06.13.03.45.39;	author beck;	state Exp;
branches;
next	1.170;

1.170
date	2008.06.12.06.58.39;	author deraadt;	state Exp;
branches;
next	1.169;

1.169
date	2008.06.11.12.35.46;	author deraadt;	state Exp;
branches;
next	1.168;

1.168
date	2008.06.10.20.14.36;	author beck;	state Exp;
branches;
next	1.167;

1.167
date	2008.06.09.23.38.37;	author millert;	state Exp;
branches;
next	1.166;

1.166
date	2008.05.07.14.08.37;	author thib;	state Exp;
branches;
next	1.165;

1.165
date	2008.05.07.05.14.21;	author claudio;	state Exp;
branches;
next	1.164;

1.164
date	2008.05.06.17.19.40;	author thib;	state Exp;
branches;
next	1.163;

1.163
date	2008.03.23.12.32.44;	author miod;	state Exp;
branches;
next	1.162;

1.162
date	2008.03.16.19.42.57;	author otto;	state Exp;
branches;
next	1.161;

1.161
date	2007.12.13.18.22.36;	author blambert;	state Exp;
branches;
next	1.160;

1.160
date	2007.11.16.13.47.27;	author deraadt;	state Exp;
branches;
next	1.159;

1.159
date	2007.11.15.16.50.28;	author deraadt;	state Exp;
branches;
next	1.158;

1.158
date	2007.10.29.14.12.19;	author chl;	state Exp;
branches;
next	1.157;

1.157
date	2007.09.15.19.22.18;	author bluhm;	state Exp;
branches;
next	1.156;

1.156
date	2007.09.07.15.00.20;	author art;	state Exp;
branches;
next	1.155;

1.155
date	2007.08.07.04.32.45;	author beck;	state Exp;
branches;
next	1.154;

1.154
date	2007.06.01.17.29.10;	author beck;	state Exp;
branches;
next	1.153;

1.153
date	2007.05.31.17.00.51;	author tedu;	state Exp;
branches;
next	1.152;

1.152
date	2007.05.31.05.12.41;	author pedro;	state Exp;
branches;
next	1.151;

1.151
date	2007.05.30.04.27.42;	author beck;	state Exp;
branches;
next	1.150;

1.150
date	2007.05.29.05.28.54;	author beck;	state Exp;
branches;
next	1.149;

1.149
date	2007.05.28.21.05.21;	author thib;	state Exp;
branches;
next	1.148;

1.148
date	2007.05.26.20.26.51;	author pedro;	state Exp;
branches;
next	1.147;

1.147
date	2007.05.26.18.42.21;	author thib;	state Exp;
branches;
next	1.146;

1.146
date	2007.05.17.23.46.28;	author thib;	state Exp;
branches;
next	1.145;

1.145
date	2007.05.09.01.09.16;	author deraadt;	state Exp;
branches;
next	1.144;

1.144
date	2007.04.13.17.09.22;	author thib;	state Exp;
branches;
next	1.143;

1.143
date	2007.04.13.10.44.07;	author bluhm;	state Exp;
branches;
next	1.142;

1.142
date	2007.04.11.16.08.50;	author thib;	state Exp;
branches;
next	1.141;

1.141
date	2007.03.21.17.29.31;	author thib;	state Exp;
branches;
next	1.140;

1.140
date	2007.03.12.19.25.58;	author mickey;	state Exp;
branches;
next	1.139;

1.139
date	2007.02.20.17.42.47;	author deraadt;	state Exp;
branches;
next	1.138;

1.138
date	2007.02.17.23.57.16;	author mickey;	state Exp;
branches;
next	1.137;

1.137
date	2007.02.14.00.53.48;	author jsg;	state Exp;
branches;
next	1.136;

1.136
date	2007.02.13.17.04.14;	author mickey;	state Exp;
branches;
next	1.135;

1.135
date	2006.11.20.12.52.54;	author tom;	state Exp;
branches;
next	1.134;

1.134
date	2006.10.30.00.34.01;	author thib;	state Exp;
branches;
next	1.133;

1.133
date	2006.07.11.21.17.58;	author mickey;	state Exp;
branches;
next	1.132;

1.132
date	2006.07.09.23.20.50;	author pedro;	state Exp;
branches;
next	1.131;

1.131
date	2006.07.08.20.01.13;	author thib;	state Exp;
branches;
next	1.130;

1.130
date	2006.07.03.12.39.52;	author mickey;	state Exp;
branches;
next	1.129;

1.129
date	2006.06.25.15.01.53;	author sturm;	state Exp;
branches;
next	1.128;

1.128
date	2006.06.14.20.01.50;	author sturm;	state Exp;
branches;
next	1.127;

1.127
date	2006.06.02.20.25.09;	author pedro;	state Exp;
branches;
next	1.126;

1.126
date	2006.05.28.04.03.28;	author pedro;	state Exp;
branches;
next	1.125;

1.125
date	2006.05.07.14.12.15;	author sturm;	state Exp;
branches;
next	1.124;

1.124
date	2006.04.30.14.20.07;	author sturm;	state Exp;
branches;
next	1.123;

1.123
date	2006.04.19.11.55.55;	author pedro;	state Exp;
branches;
next	1.122;

1.122
date	2006.01.09.12.43.16;	author pedro;	state Exp;
branches;
next	1.121;

1.121
date	2005.11.30.10.35.07;	author pedro;	state Exp;
branches;
next	1.120;

1.120
date	2005.11.24.12.08.16;	author pedro;	state Exp;
branches;
next	1.119;

1.119
date	2005.11.19.02.18.01;	author pedro;	state Exp;
branches;
next	1.118;

1.118
date	2005.11.18.13.25.40;	author pedro;	state Exp;
branches;
next	1.117;

1.117
date	2005.11.08.15.50.01;	author pedro;	state Exp;
branches;
next	1.116;

1.116
date	2005.11.07.23.15.00;	author pedro;	state Exp;
branches;
next	1.115;

1.115
date	2005.10.19.16.50.46;	author pedro;	state Exp;
branches;
next	1.114;

1.114
date	2005.05.26.00.33.45;	author pedro;	state Exp;
branches
	1.114.2.1;
next	1.113;

1.113
date	2005.05.24.05.34.54;	author pedro;	state Exp;
branches;
next	1.112;

1.112
date	2005.05.22.21.12.42;	author pedro;	state Exp;
branches;
next	1.111;

1.111
date	2005.05.01.12.28.18;	author pedro;	state Exp;
branches;
next	1.110;

1.110
date	2005.03.24.02.40.26;	author tedu;	state Exp;
branches;
next	1.109;

1.109
date	2005.01.10.11.58.34;	author pedro;	state Exp;
branches
	1.109.2.1;
next	1.108;

1.108
date	2004.12.31.15.28.40;	author pedro;	state Exp;
branches;
next	1.107;

1.107
date	2004.12.31.12.13.53;	author pedro;	state Exp;
branches;
next	1.106;

1.106
date	2004.12.28.15.14.37;	author deraadt;	state Exp;
branches;
next	1.105;

1.105
date	2004.12.26.21.22.13;	author miod;	state Exp;
branches;
next	1.104;

1.104
date	2004.12.09.22.36.40;	author pedro;	state Exp;
branches;
next	1.103;

1.103
date	2004.08.04.20.36.27;	author art;	state Exp;
branches;
next	1.102;

1.102
date	2004.08.04.03.05.25;	author pedro;	state Exp;
branches;
next	1.101;

1.101
date	2004.08.02.03.26.50;	author pedro;	state Exp;
branches;
next	1.100;

1.100
date	2004.05.27.20.48.46;	author tedu;	state Exp;
branches;
next	1.99;

1.99
date	2004.05.27.08.25.53;	author tedu;	state Exp;
branches;
next	1.98;

1.98
date	2004.04.25.02.48.03;	author itojun;	state Exp;
branches;
next	1.97;

1.97
date	2004.01.09.03.01.03;	author tedu;	state Exp;
branches;
next	1.96;

1.96
date	2004.01.06.04.22.59;	author tedu;	state Exp;
branches;
next	1.95;

1.95
date	2003.07.21.22.44.50;	author tedu;	state Exp;
branches;
next	1.94;

1.94
date	2003.06.02.23.28.07;	author millert;	state Exp;
branches;
next	1.93;

1.93
date	2003.05.13.09.31.06;	author naddy;	state Exp;
branches;
next	1.92;

1.92
date	2003.05.13.02.30.01;	author tedu;	state Exp;
branches;
next	1.91;

1.91
date	2003.05.06.20.52.14;	author tedu;	state Exp;
branches;
next	1.90;

1.90
date	2003.05.01.21.13.05;	author tedu;	state Exp;
branches;
next	1.89;

1.89
date	2003.04.06.18.54.20;	author ho;	state Exp;
branches;
next	1.88;

1.88
date	2002.08.11.22.32.31;	author art;	state Exp;
branches;
next	1.87;

1.87
date	2002.07.12.14.02.22;	author art;	state Exp;
branches;
next	1.86;

1.86
date	2002.06.16.16.54.25;	author miod;	state Exp;
branches;
next	1.85;

1.85
date	2002.06.08.18.36.45;	author art;	state Exp;
branches;
next	1.84;

1.84
date	2002.05.16.00.03.05;	author art;	state Exp;
branches;
next	1.83;

1.83
date	2002.03.14.01.27.06;	author millert;	state Exp;
branches;
next	1.82;

1.82
date	2002.02.04.19.38.20;	author miod;	state Exp;
branches;
next	1.81;

1.81
date	2002.01.23.00.39.48;	author art;	state Exp;
branches;
next	1.80;

1.80
date	2001.12.19.08.58.06;	author art;	state Exp;
branches;
next	1.79;

1.79
date	2001.12.10.18.47.16;	author art;	state Exp;
branches
	1.79.2.1;
next	1.78;

1.78
date	2001.12.10.04.45.31;	author art;	state Exp;
branches;
next	1.77;

1.77
date	2001.12.10.02.19.34;	author art;	state Exp;
branches;
next	1.76;

1.76
date	2001.12.05.00.24.36;	author art;	state Exp;
branches;
next	1.75;

1.75
date	2001.11.29.02.54.10;	author art;	state Exp;
branches;
next	1.74;

1.74
date	2001.11.29.01.58.57;	author art;	state Exp;
branches;
next	1.73;

1.73
date	2001.11.27.05.27.12;	author art;	state Exp;
branches;
next	1.72;

1.72
date	2001.11.21.21.13.34;	author csapuntz;	state Exp;
branches;
next	1.71;

1.71
date	2001.11.15.06.40.39;	author art;	state Exp;
branches;
next	1.70;

1.70
date	2001.11.12.23.05.52;	author art;	state Exp;
branches;
next	1.69;

1.69
date	2001.11.06.19.53.20;	author miod;	state Exp;
branches;
next	1.68;

1.68
date	2001.10.02.17.21.02;	author csapuntz;	state Exp;
branches;
next	1.67;

1.67
date	2001.09.19.22.52.41;	author csapuntz;	state Exp;
branches;
next	1.66;

1.66
date	2001.09.16.00.42.44;	author millert;	state Exp;
branches;
next	1.65;

1.65
date	2001.08.02.08.16.45;	author assar;	state Exp;
branches;
next	1.64;

1.64
date	2001.07.26.22.27.45;	author miod;	state Exp;
branches;
next	1.63;

1.63
date	2001.06.27.04.49.48;	author art;	state Exp;
branches;
next	1.62;

1.62
date	2001.06.22.14.14.10;	author deraadt;	state Exp;
branches;
next	1.61;

1.61
date	2001.06.05.21.47.07;	author provos;	state Exp;
branches;
next	1.60;

1.60
date	2001.05.16.13.54.37;	author art;	state Exp;
branches;
next	1.59;

1.59
date	2001.04.29.20.42.45;	author art;	state Exp;
branches;
next	1.58;

1.58
date	2001.03.22.00.31.56;	author art;	state Exp;
branches
	1.58.2.1;
next	1.57;

1.57
date	2001.03.21.17.05.29;	author art;	state Exp;
branches;
next	1.56;

1.56
date	2001.03.16.16.10.31;	author art;	state Exp;
branches;
next	1.55;

1.55
date	2001.03.16.16.05.28;	author art;	state Exp;
branches;
next	1.54;

1.54
date	2001.03.16.15.51.04;	author art;	state Exp;
branches;
next	1.53;

1.53
date	2001.02.26.02.36.39;	author csapuntz;	state Exp;
branches;
next	1.52;

1.52
date	2001.02.26.00.18.33;	author csapuntz;	state Exp;
branches;
next	1.51;

1.51
date	2001.02.24.19.07.08;	author csapuntz;	state Exp;
branches;
next	1.50;

1.50
date	2001.02.23.14.42.37;	author csapuntz;	state Exp;
branches;
next	1.49;

1.49
date	2001.02.21.23.24.30;	author csapuntz;	state Exp;
branches;
next	1.48;

1.48
date	2001.02.08.00.32.11;	author mickey;	state Exp;
branches;
next	1.47;

1.47
date	2000.09.27.09.37.16;	author art;	state Exp;
branches
	1.47.2.1;
next	1.46;

1.46
date	2000.07.17.14.54.26;	author art;	state Exp;
branches;
next	1.45;

1.45
date	2000.04.25.22.41.57;	author csapuntz;	state Exp;
branches;
next	1.44;

1.44
date	2000.04.21.16.33.12;	author mickey;	state Exp;
branches;
next	1.43;

1.43
date	99.12.05.07.54.44;	author art;	state Exp;
branches
	1.43.2.1;
next	1.42;

1.42
date	99.12.05.07.39.28;	author art;	state Exp;
branches;
next	1.41;

1.41
date	99.08.20.15.37.13;	author art;	state Exp;
branches;
next	1.40;

1.40
date	99.08.08.00.34.38;	author niklas;	state Exp;
branches;
next	1.39;

1.39
date	99.05.31.17.34.48;	author millert;	state Exp;
branches;
next	1.38;

1.38
date	99.05.06.15.59.40;	author mickey;	state Exp;
branches;
next	1.37;

1.37
date	99.04.30.08.21.52;	author art;	state Exp;
branches;
next	1.36;

1.36
date	99.03.11.19.47.25;	author deraadt;	state Exp;
branches;
next	1.35;

1.35
date	99.03.11.18.55.24;	author deraadt;	state Exp;
branches;
next	1.34;

1.34
date	99.03.11.18.30.49;	author mickey;	state Exp;
branches;
next	1.33;

1.33
date	99.03.11.18.28.55;	author mickey;	state Exp;
branches;
next	1.32;

1.32
date	99.02.26.05.17.43;	author art;	state Exp;
branches;
next	1.31;

1.31
date	99.02.19.17.15.45;	author art;	state Exp;
branches;
next	1.30;

1.30
date	98.12.28.19.35.35;	author art;	state Exp;
branches;
next	1.29;

1.29
date	98.12.22.10.43.37;	author art;	state Exp;
branches;
next	1.28;

1.28
date	98.12.10.21.46.58;	author art;	state Exp;
branches;
next	1.27;

1.27
date	98.12.05.16.50.40;	author csapuntz;	state Exp;
branches;
next	1.26;

1.26
date	98.12.04.15.57.01;	author csapuntz;	state Exp;
branches;
next	1.25;

1.25
date	98.11.20.01.35.32;	author art;	state Exp;
branches;
next	1.24;

1.24
date	98.11.12.04.30.02;	author csapuntz;	state Exp;
branches;
next	1.23;

1.23
date	98.10.13.16.42.01;	author csapuntz;	state Exp;
branches;
next	1.22;

1.22
date	98.08.30.23.34.36;	author csapuntz;	state Exp;
branches;
next	1.21;

1.21
date	98.08.06.19.34.26;	author csapuntz;	state Exp;
branches;
next	1.20;

1.20
date	98.04.25.07.04.11;	author niklas;	state Exp;
branches;
next	1.19;

1.19
date	98.02.20.14.47.51;	author niklas;	state Exp;
branches;
next	1.18;

1.18
date	98.01.11.02.10.44;	author csapuntz;	state Exp;
branches;
next	1.17;

1.17
date	98.01.10.23.41.19;	author csapuntz;	state Exp;
branches;
next	1.16;

1.16
date	97.11.24.22.42.38;	author niklas;	state Exp;
branches;
next	1.15;

1.15
date	97.11.07.23.01.37;	author csapuntz;	state Exp;
branches;
next	1.14;

1.14
date	97.11.06.22.46.19;	author csapuntz;	state Exp;
branches;
next	1.13;

1.13
date	97.11.06.05.58.28;	author csapuntz;	state Exp;
branches;
next	1.12;

1.12
date	97.10.06.20.20.12;	author deraadt;	state Exp;
branches;
next	1.11;

1.11
date	97.10.06.15.12.42;	author csapuntz;	state Exp;
branches;
next	1.10;

1.10
date	97.04.25.09.33.24;	author deraadt;	state Exp;
branches;
next	1.9;

1.9
date	97.04.14.04.23.26;	author tholo;	state Exp;
branches;
next	1.8;

1.8
date	97.02.24.14.20.02;	author niklas;	state Exp;
branches;
next	1.7;

1.7
date	97.02.11.06.59.25;	author millert;	state Exp;
branches;
next	1.6;

1.6
date	97.01.04.17.10.03;	author kstailey;	state Exp;
branches;
next	1.5;

1.5
date	96.08.08.06.36.46;	author tholo;	state Exp;
branches;
next	1.4;

1.4
date	96.05.02.13.12.37;	author deraadt;	state Exp;
branches;
next	1.3;

1.3
date	96.04.21.22.27.36;	author deraadt;	state Exp;
branches;
next	1.2;

1.2
date	96.02.29.13.38.57;	author niklas;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.52.47;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.52.47;	author deraadt;	state Exp;
branches;
next	;

1.43.2.1
date	2001.05.14.22.32.46;	author niklas;	state Exp;
branches;
next	1.43.2.2;

1.43.2.2
date	2001.07.04.10.48.52;	author niklas;	state Exp;
branches;
next	1.43.2.3;

1.43.2.3
date	2001.10.31.03.26.29;	author nate;	state Exp;
branches;
next	1.43.2.4;

1.43.2.4
date	2001.11.13.23.04.23;	author niklas;	state Exp;
branches;
next	1.43.2.5;

1.43.2.5
date	2001.12.05.01.02.39;	author niklas;	state Exp;
branches;
next	1.43.2.6;

1.43.2.6
date	2002.03.06.02.13.24;	author niklas;	state Exp;
branches;
next	1.43.2.7;

1.43.2.7
date	2002.03.28.11.43.04;	author niklas;	state Exp;
branches;
next	1.43.2.8;

1.43.2.8
date	2003.03.28.00.41.27;	author niklas;	state Exp;
branches;
next	1.43.2.9;

1.43.2.9
date	2003.05.13.19.21.28;	author ho;	state Exp;
branches;
next	1.43.2.10;

1.43.2.10
date	2003.06.07.11.03.41;	author ho;	state Exp;
branches;
next	1.43.2.11;

1.43.2.11
date	2004.02.19.10.56.39;	author niklas;	state Exp;
branches;
next	1.43.2.12;

1.43.2.12
date	2004.06.05.23.13.03;	author niklas;	state Exp;
branches;
next	;

1.47.2.1
date	2001.09.16.15.22.03;	author miod;	state Exp;
branches;
next	;

1.58.2.1
date	2001.10.14.20.44.49;	author jason;	state Exp;
branches;
next	;

1.79.2.1
date	2002.01.31.22.55.41;	author niklas;	state Exp;
branches;
next	1.79.2.2;

1.79.2.2
date	2002.02.02.03.28.25;	author art;	state Exp;
branches;
next	1.79.2.3;

1.79.2.3
date	2002.06.11.03.29.40;	author art;	state Exp;
branches;
next	1.79.2.4;

1.79.2.4
date	2002.10.29.00.36.44;	author art;	state Exp;
branches;
next	1.79.2.5;

1.79.2.5
date	2002.11.04.18.02.31;	author art;	state Exp;
branches;
next	1.79.2.6;

1.79.2.6
date	2003.05.19.22.31.57;	author tedu;	state Exp;
branches;
next	1.79.2.7;

1.79.2.7
date	2004.02.21.00.20.21;	author tedu;	state Exp;
branches;
next	;

1.109.2.1
date	2005.11.21.23.22.12;	author brad;	state Exp;
branches;
next	;

1.114.2.1
date	2005.11.21.23.12.25;	author brad;	state Exp;
branches;
next	;

1.232.4.1
date	2016.07.29.19.06.03;	author tedu;	state Exp;
branches;
next	;
commitid	uCUVgwGuKrYS4V1v;

1.238.2.1
date	2016.07.29.19.06.50;	author tedu;	state Exp;
branches;
next	;
commitid	NiJlUjSoyX11CJLu;


desc
@@


1.260
log
@Give back some space to the ramdisk by compiling net/radix.c only
if we compile pf, ipsec, pipex or nfsserver.
Suggested by mpi some time ago.
Tweak & OK bluhm
deraadt assumes it's fair
@
text
@/*	$OpenBSD: vfs_subr.c,v 1.259 2017/04/20 14:13:00 visa Exp $	*/
/*	$NetBSD: vfs_subr.c,v 1.53 1996/04/22 01:39:13 christos Exp $	*/

/*
 * Copyright (c) 1989, 1993
 *	The Regents of the University of California.  All rights reserved.
 * (c) UNIX System Laboratories, Inc.
 * All or some portions of this file are derived from material licensed
 * to the University of California by American Telephone and Telegraph
 * Co. or Unix System Laboratories, Inc. and are reproduced herein with
 * the permission of UNIX System Laboratories, Inc.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)vfs_subr.c	8.13 (Berkeley) 4/18/94
 */

/*
 * External virtual filesystem routines
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/sysctl.h>
#include <sys/mount.h>
#include <sys/time.h>
#include <sys/fcntl.h>
#include <sys/kernel.h>
#include <sys/conf.h>
#include <sys/vnode.h>
#include <sys/lock.h>
#include <sys/stat.h>
#include <sys/acct.h>
#include <sys/namei.h>
#include <sys/ucred.h>
#include <sys/buf.h>
#include <sys/errno.h>
#include <sys/malloc.h>
#include <sys/mbuf.h>
#include <sys/syscallargs.h>
#include <sys/pool.h>
#include <sys/tree.h>
#include <sys/specdev.h>

#include <netinet/in.h>

#include <uvm/uvm_extern.h>
#include <uvm/uvm_vnode.h>

#include "softraid.h"

void sr_shutdown(void);

enum vtype iftovt_tab[16] = {
	VNON, VFIFO, VCHR, VNON, VDIR, VNON, VBLK, VNON,
	VREG, VNON, VLNK, VNON, VSOCK, VNON, VNON, VBAD,
};

int	vttoif_tab[9] = {
	0, S_IFREG, S_IFDIR, S_IFBLK, S_IFCHR, S_IFLNK,
	S_IFSOCK, S_IFIFO, S_IFMT,
};

int prtactive = 0;		/* 1 => print out reclaim of active vnodes */
int suid_clear = 1;		/* 1 => clear SUID / SGID on owner change */

/*
 * Insq/Remq for the vnode usage lists.
 */
#define	bufinsvn(bp, dp)	LIST_INSERT_HEAD(dp, bp, b_vnbufs)
#define	bufremvn(bp) {							\
	LIST_REMOVE(bp, b_vnbufs);					\
	LIST_NEXT(bp, b_vnbufs) = NOLIST;				\
}

struct freelst vnode_hold_list;	/* list of vnodes referencing buffers */
struct freelst vnode_free_list;	/* vnode free list */

struct mntlist mountlist;	/* mounted filesystem list */

void	vclean(struct vnode *, int, struct proc *);

void insmntque(struct vnode *, struct mount *);
int getdevvp(dev_t, struct vnode **, enum vtype);

int vfs_hang_addrlist(struct mount *, struct netexport *,
				  struct export_args *);
int vfs_free_netcred(struct radix_node *, void *, u_int);
void vfs_free_addrlist(struct netexport *);
void vputonfreelist(struct vnode *);

int vflush_vnode(struct vnode *, void *);
int maxvnodes;

#ifdef DEBUG
void printlockedvnodes(void);
#endif

struct pool vnode_pool;
struct pool uvm_vnode_pool;

static inline int rb_buf_compare(const struct buf *b1, const struct buf *b2);
RBT_GENERATE(buf_rb_bufs, buf, b_rbbufs, rb_buf_compare);

static inline int
rb_buf_compare(const struct buf *b1, const struct buf *b2)
{
	if (b1->b_lblkno < b2->b_lblkno)
		return(-1);
	if (b1->b_lblkno > b2->b_lblkno)
		return(1);
	return(0);
}

/*
 * Initialize the vnode management data structures.
 */
void
vntblinit(void)
{
	/* buffer cache may need a vnode for each buffer */
	maxvnodes = 2 * initialvnodes;
	pool_init(&vnode_pool, sizeof(struct vnode), 0, IPL_NONE,
	    PR_WAITOK, "vnodes", NULL);
	pool_init(&uvm_vnode_pool, sizeof(struct uvm_vnode), 0, IPL_NONE,
	    PR_WAITOK, "uvmvnodes", NULL);
	TAILQ_INIT(&vnode_hold_list);
	TAILQ_INIT(&vnode_free_list);
	TAILQ_INIT(&mountlist);
	/*
	 * Initialize the filesystem syncer.
	 */
	vn_initialize_syncerd();

#ifdef NFSSERVER
	rn_init(sizeof(struct sockaddr_in));
#endif /* NFSSERVER */
}

/*
 * Mark a mount point as busy. Used to synchronize access and to delay
 * unmounting.
 *
 * Default behaviour is to attempt getting a READ lock and in case of an
 * ongoing unmount, to wait for it to finish and then return failure.
 */
int
vfs_busy(struct mount *mp, int flags)
{
	int rwflags = 0;

	/* new mountpoints need their lock initialised */
	if (mp->mnt_lock.rwl_name == NULL)
		rw_init_flags(&mp->mnt_lock, "vfslock", RWL_IS_VNODE);

	if (flags & VB_WRITE)
		rwflags |= RW_WRITE;
	else
		rwflags |= RW_READ;

	if (flags & VB_WAIT)
		rwflags |= RW_SLEEPFAIL;
	else
		rwflags |= RW_NOSLEEP;

	if (rw_enter(&mp->mnt_lock, rwflags))
		return (EBUSY);

	return (0);
}

/*
 * Free a busy file system
 */
void
vfs_unbusy(struct mount *mp)
{
	rw_exit(&mp->mnt_lock);
}

int
vfs_isbusy(struct mount *mp) 
{
	if (RWLOCK_OWNER(&mp->mnt_lock) > 0)
		return (1);
	else
		return (0);
}

/*
 * Lookup a filesystem type, and if found allocate and initialize
 * a mount structure for it.
 *
 * Devname is usually updated by mount(8) after booting.
 */
int
vfs_rootmountalloc(char *fstypename, char *devname, struct mount **mpp)
{
	struct vfsconf *vfsp;
	struct mount *mp;

	for (vfsp = vfsconf; vfsp; vfsp = vfsp->vfc_next)
		if (!strcmp(vfsp->vfc_name, fstypename))
			break;
	if (vfsp == NULL)
		return (ENODEV);
	mp = malloc(sizeof(*mp), M_MOUNT, M_WAITOK|M_ZERO);
	(void)vfs_busy(mp, VB_READ|VB_NOWAIT);
	LIST_INIT(&mp->mnt_vnodelist);
	mp->mnt_vfc = vfsp;
	mp->mnt_op = vfsp->vfc_vfsops;
	mp->mnt_flag = MNT_RDONLY;
	mp->mnt_vnodecovered = NULLVP;
	vfsp->vfc_refcount++;
	mp->mnt_flag |= vfsp->vfc_flags & MNT_VISFLAGMASK;
	strncpy(mp->mnt_stat.f_fstypename, vfsp->vfc_name, MFSNAMELEN);
	mp->mnt_stat.f_mntonname[0] = '/';
	copystr(devname, mp->mnt_stat.f_mntfromname, MNAMELEN, 0);
	copystr(devname, mp->mnt_stat.f_mntfromspec, MNAMELEN, 0);
	*mpp = mp;
 	return (0);
 }

/*
 * Lookup a mount point by filesystem identifier.
 */
struct mount *
vfs_getvfs(fsid_t *fsid)
{
	struct mount *mp;

	TAILQ_FOREACH(mp, &mountlist, mnt_list) {
		if (mp->mnt_stat.f_fsid.val[0] == fsid->val[0] &&
		    mp->mnt_stat.f_fsid.val[1] == fsid->val[1]) {
			return (mp);
		}
	}

	return (NULL);
}


/*
 * Get a new unique fsid
 */
void
vfs_getnewfsid(struct mount *mp)
{
	static u_short xxxfs_mntid;

	fsid_t tfsid;
	int mtype;

	mtype = mp->mnt_vfc->vfc_typenum;
	mp->mnt_stat.f_fsid.val[0] = makedev(nblkdev + mtype, 0);
	mp->mnt_stat.f_fsid.val[1] = mtype;
	if (xxxfs_mntid == 0)
		++xxxfs_mntid;
	tfsid.val[0] = makedev(nblkdev + mtype, xxxfs_mntid);
	tfsid.val[1] = mtype;
	if (!TAILQ_EMPTY(&mountlist)) {
		while (vfs_getvfs(&tfsid)) {
			tfsid.val[0]++;
			xxxfs_mntid++;
		}
	}
	mp->mnt_stat.f_fsid.val[0] = tfsid.val[0];
}

/*
 * Set vnode attributes to VNOVAL
 */
void
vattr_null(struct vattr *vap)
{

	vap->va_type = VNON;
	/*
	 * Don't get fancy: u_quad_t = u_int = VNOVAL leaves the u_quad_t
	 * with 2^31-1 instead of 2^64-1.  Just write'm out and let
	 * the compiler do its job.
	 */
	vap->va_mode = VNOVAL;
	vap->va_nlink = VNOVAL;
	vap->va_uid = VNOVAL;
	vap->va_gid = VNOVAL;
	vap->va_fsid = VNOVAL;
	vap->va_fileid = VNOVAL;
	vap->va_size = VNOVAL;
	vap->va_blocksize = VNOVAL;
	vap->va_atime.tv_sec = VNOVAL;
	vap->va_atime.tv_nsec = VNOVAL;
	vap->va_mtime.tv_sec = VNOVAL;
	vap->va_mtime.tv_nsec = VNOVAL;
	vap->va_ctime.tv_sec = VNOVAL;
	vap->va_ctime.tv_nsec = VNOVAL;
	vap->va_gen = VNOVAL;
	vap->va_flags = VNOVAL;
	vap->va_rdev = VNOVAL;
	vap->va_bytes = VNOVAL;
	vap->va_filerev = VNOVAL;
	vap->va_vaflags = 0;
}

/*
 * Routines having to do with the management of the vnode table.
 */
long numvnodes;

/*
 * Return the next vnode from the free list.
 */
int
getnewvnode(enum vtagtype tag, struct mount *mp, struct vops *vops,
    struct vnode **vpp)
{
	struct proc *p = curproc;
	struct freelst *listhd;
	static int toggle;
	struct vnode *vp;
	int s;

	/*
	 * allow maxvnodes to increase if the buffer cache itself
	 * is big enough to justify it. (we don't shrink it ever)
	 */
	maxvnodes = maxvnodes < bcstats.numbufs ? bcstats.numbufs
	    : maxvnodes;

	/*
	 * We must choose whether to allocate a new vnode or recycle an
	 * existing one. The criterion for allocating a new one is that
	 * the total number of vnodes is less than the number desired or
	 * there are no vnodes on either free list. Generally we only
	 * want to recycle vnodes that have no buffers associated with
	 * them, so we look first on the vnode_free_list. If it is empty,
	 * we next consider vnodes with referencing buffers on the
	 * vnode_hold_list. The toggle ensures that half the time we
	 * will use a buffer from the vnode_hold_list, and half the time
	 * we will allocate a new one unless the list has grown to twice
	 * the desired size. We are reticent to recycle vnodes from the
	 * vnode_hold_list because we will lose the identity of all its
	 * referencing buffers.
	 */
	toggle ^= 1;
	if (numvnodes / 2 > maxvnodes)
		toggle = 0;

	s = splbio();
	if ((numvnodes < maxvnodes) ||
	    ((TAILQ_FIRST(listhd = &vnode_free_list) == NULL) &&
	    ((TAILQ_FIRST(listhd = &vnode_hold_list) == NULL) || toggle))) {
		splx(s);
		vp = pool_get(&vnode_pool, PR_WAITOK | PR_ZERO);
		vp->v_uvm = pool_get(&uvm_vnode_pool, PR_WAITOK | PR_ZERO);
		vp->v_uvm->u_vnode = vp;
		RBT_INIT(buf_rb_bufs, &vp->v_bufs_tree);
		cache_tree_init(&vp->v_nc_tree);
		TAILQ_INIT(&vp->v_cache_dst);
		numvnodes++;
	} else {
		TAILQ_FOREACH(vp, listhd, v_freelist) {
			if (VOP_ISLOCKED(vp) == 0)
				break;
		}
		/*
		 * Unless this is a bad time of the month, at most
		 * the first NCPUS items on the free list are
		 * locked, so this is close enough to being empty.
		 */
		if (vp == NULL) {
			splx(s);
			tablefull("vnode");
			*vpp = 0;
			return (ENFILE);
		}

#ifdef DIAGNOSTIC
		if (vp->v_usecount) {
			vprint("free vnode", vp);
			panic("free vnode isn't");
		}
#endif

		TAILQ_REMOVE(listhd, vp, v_freelist);
		vp->v_bioflag &= ~VBIOONFREELIST;
		splx(s);

		if (vp->v_type != VBAD)
			vgonel(vp, p);
#ifdef DIAGNOSTIC
		if (vp->v_data) {
			vprint("cleaned vnode", vp);
			panic("cleaned vnode isn't");
		}
		s = splbio();
		if (vp->v_numoutput)
			panic("Clean vnode has pending I/O's");
		splx(s);
#endif
		vp->v_flag = 0;
		vp->v_socket = 0;
	}
	cache_purge(vp);
	vp->v_type = VNON;
	vp->v_tag = tag;
	vp->v_op = vops;
	insmntque(vp, mp);
	*vpp = vp;
	vp->v_usecount = 1;
	vp->v_data = 0;
	return (0);
}

/*
 * Move a vnode from one mount queue to another.
 */
void
insmntque(struct vnode *vp, struct mount *mp)
{
	/*
	 * Delete from old mount point vnode list, if on one.
	 */
	if (vp->v_mount != NULL)
		LIST_REMOVE(vp, v_mntvnodes);
	/*
	 * Insert into list of vnodes for the new mount point, if available.
	 */
	if ((vp->v_mount = mp) != NULL)
		LIST_INSERT_HEAD(&mp->mnt_vnodelist, vp, v_mntvnodes);
}

/*
 * Create a vnode for a block device.
 * Used for root filesystem, argdev, and swap areas.
 * Also used for memory file system special devices.
 */
int
bdevvp(dev_t dev, struct vnode **vpp)
{
	return (getdevvp(dev, vpp, VBLK));
}

/*
 * Create a vnode for a character device.
 * Used for console handling.
 */
int
cdevvp(dev_t dev, struct vnode **vpp)
{
	return (getdevvp(dev, vpp, VCHR));
}

/*
 * Create a vnode for a device.
 * Used by bdevvp (block device) for root file system etc.,
 * and by cdevvp (character device) for console.
 */
int
getdevvp(dev_t dev, struct vnode **vpp, enum vtype type)
{
	struct vnode *vp;
	struct vnode *nvp;
	int error;

	if (dev == NODEV) {
		*vpp = NULLVP;
		return (0);
	}
	error = getnewvnode(VT_NON, NULL, &spec_vops, &nvp);
	if (error) {
		*vpp = NULLVP;
		return (error);
	}
	vp = nvp;
	vp->v_type = type;
	if ((nvp = checkalias(vp, dev, NULL)) != 0) {
		vput(vp);
		vp = nvp;
	}
	if (vp->v_type == VCHR && cdevsw[major(vp->v_rdev)].d_type == D_TTY)
		vp->v_flag |= VISTTY;
	*vpp = vp;
	return (0);
}

/*
 * Check to see if the new vnode represents a special device
 * for which we already have a vnode (either because of
 * bdevvp() or because of a different vnode representing
 * the same block device). If such an alias exists, deallocate
 * the existing contents and return the aliased vnode. The
 * caller is responsible for filling it with its new contents.
 */
struct vnode *
checkalias(struct vnode *nvp, dev_t nvp_rdev, struct mount *mp)
{
	struct proc *p = curproc;
	struct vnode *vp;
	struct vnode **vpp;

	if (nvp->v_type != VBLK && nvp->v_type != VCHR)
		return (NULLVP);

	vpp = &speclisth[SPECHASH(nvp_rdev)];
loop:
	for (vp = *vpp; vp; vp = vp->v_specnext) {
		if (nvp_rdev != vp->v_rdev || nvp->v_type != vp->v_type) {
			continue;
		}
		/*
		 * Alias, but not in use, so flush it out.
		 */
		if (vp->v_usecount == 0) {
			vgonel(vp, p);
			goto loop;
		}
		if (vget(vp, LK_EXCLUSIVE, p)) {
			goto loop;
		}
		break;
	}

	/*
	 * Common case is actually in the if statement
	 */
	if (vp == NULL || !(vp->v_tag == VT_NON && vp->v_type == VBLK)) {
		nvp->v_specinfo = malloc(sizeof(struct specinfo), M_VNODE,
			M_WAITOK);
		nvp->v_rdev = nvp_rdev;
		nvp->v_hashchain = vpp;
		nvp->v_specnext = *vpp;
		nvp->v_specmountpoint = NULL;
		nvp->v_speclockf = NULL;
		nvp->v_specbitmap = NULL;
		if (nvp->v_type == VCHR &&
		    (cdevsw[major(nvp_rdev)].d_flags & D_CLONE) &&
		    (minor(nvp_rdev) >> CLONE_SHIFT == 0)) {
			if (vp != NULLVP)
				nvp->v_specbitmap = vp->v_specbitmap;
			else
				nvp->v_specbitmap = malloc(CLONE_MAPSZ,
				    M_VNODE, M_WAITOK | M_ZERO);
		}
		*vpp = nvp;
		if (vp != NULLVP) {
			nvp->v_flag |= VALIASED;
			vp->v_flag |= VALIASED;
			vput(vp);
		}
		return (NULLVP);
	}

	/*
	 * This code is the uncommon case. It is called in case
	 * we found an alias that was VT_NON && vtype of VBLK
	 * This means we found a block device that was created
	 * using bdevvp.
	 * An example of such a vnode is the root partition device vnode
	 * created in ffs_mountroot.
	 *
	 * The vnodes created by bdevvp should not be aliased (why?).
	 */

	VOP_UNLOCK(vp, p);
	vclean(vp, 0, p);
	vp->v_op = nvp->v_op;
	vp->v_tag = nvp->v_tag;
	nvp->v_type = VNON;
	insmntque(vp, mp);
	return (vp);
}

/*
 * Grab a particular vnode from the free list, increment its
 * reference count and lock it. If the vnode lock bit is set,
 * the vnode is being eliminated in vgone. In that case, we
 * cannot grab it, so the process is awakened when the
 * transition is completed, and an error code is returned to
 * indicate that the vnode is no longer usable, possibly
 * having been changed to a new file system type.
 */
int
vget(struct vnode *vp, int flags, struct proc *p)
{
	int error, s, onfreelist;

	/*
	 * If the vnode is in the process of being cleaned out for
	 * another use, we wait for the cleaning to finish and then
	 * return failure. Cleaning is determined by checking that
	 * the VXLOCK flag is set.
	 */

	if (vp->v_flag & VXLOCK) {
		if (flags & LK_NOWAIT) {
			return (EBUSY);
		}

		vp->v_flag |= VXWANT;
		tsleep(vp, PINOD, "vget", 0);
		return (ENOENT);
	}

	onfreelist = vp->v_bioflag & VBIOONFREELIST;
	if (vp->v_usecount == 0 && onfreelist) {
		s = splbio();
		if (vp->v_holdcnt > 0)
			TAILQ_REMOVE(&vnode_hold_list, vp, v_freelist);
		else
			TAILQ_REMOVE(&vnode_free_list, vp, v_freelist);
		vp->v_bioflag &= ~VBIOONFREELIST;
		splx(s);
	}

 	vp->v_usecount++;
	if (flags & LK_TYPE_MASK) {
		if ((error = vn_lock(vp, flags, p)) != 0) {
			vp->v_usecount--;
			if (vp->v_usecount == 0 && onfreelist)
				vputonfreelist(vp);
		}
		return (error);
	}

	return (0);
}


/* Vnode reference. */
void
vref(struct vnode *vp)
{
#ifdef DIAGNOSTIC
	if (vp->v_usecount == 0)
		panic("vref used where vget required");
	if (vp->v_type == VNON)
		panic("vref on a VNON vnode");
#endif
	vp->v_usecount++;
}

void
vputonfreelist(struct vnode *vp)
{
	int s;
	struct freelst *lst;

	s = splbio();
#ifdef DIAGNOSTIC
	if (vp->v_usecount != 0)
		panic("Use count is not zero!");

	if (vp->v_bioflag & VBIOONFREELIST) {
		vprint("vnode already on free list: ", vp);
		panic("vnode already on free list");
	}
#endif

	vp->v_bioflag |= VBIOONFREELIST;

	if (vp->v_holdcnt > 0)
		lst = &vnode_hold_list;
	else
		lst = &vnode_free_list;

	if (vp->v_type == VBAD)
		TAILQ_INSERT_HEAD(lst, vp, v_freelist);
	else
		TAILQ_INSERT_TAIL(lst, vp, v_freelist);

	splx(s);
}

/*
 * vput(), just unlock and vrele()
 */
void
vput(struct vnode *vp)
{
	struct proc *p = curproc;

#ifdef DIAGNOSTIC
	if (vp == NULL)
		panic("vput: null vp");
#endif

#ifdef DIAGNOSTIC
	if (vp->v_usecount == 0) {
		vprint("vput: bad ref count", vp);
		panic("vput: ref cnt");
	}
#endif
	vp->v_usecount--;
	if (vp->v_usecount > 0) {
		VOP_UNLOCK(vp, p);
		return;
	}

#ifdef DIAGNOSTIC
	if (vp->v_writecount != 0) {
		vprint("vput: bad writecount", vp);
		panic("vput: v_writecount != 0");
	}
#endif

	VOP_INACTIVE(vp, p);

	if (vp->v_usecount == 0 && !(vp->v_bioflag & VBIOONFREELIST))
		vputonfreelist(vp);
}

/*
 * Vnode release - use for active VNODES.
 * If count drops to zero, call inactive routine and return to freelist.
 * Returns 0 if it did not sleep.
 */
int
vrele(struct vnode *vp)
{
	struct proc *p = curproc;

#ifdef DIAGNOSTIC
	if (vp == NULL)
		panic("vrele: null vp");
#endif
#ifdef DIAGNOSTIC
	if (vp->v_usecount == 0) {
		vprint("vrele: bad ref count", vp);
		panic("vrele: ref cnt");
	}
#endif
	vp->v_usecount--;
	if (vp->v_usecount > 0) {
		return (0);
	}

#ifdef DIAGNOSTIC
	if (vp->v_writecount != 0) {
		vprint("vrele: bad writecount", vp);
		panic("vrele: v_writecount != 0");
	}
#endif

	if (vn_lock(vp, LK_EXCLUSIVE, p)) {
#ifdef DIAGNOSTIC
		vprint("vrele: cannot lock", vp);
#endif
		return (1);
	}

	VOP_INACTIVE(vp, p);

	if (vp->v_usecount == 0 && !(vp->v_bioflag & VBIOONFREELIST))
		vputonfreelist(vp);
	return (1);
}

/* Page or buffer structure gets a reference. */
void
vhold(struct vnode *vp)
{
	/*
	 * If it is on the freelist and the hold count is currently
	 * zero, move it to the hold list.
	 */
	if ((vp->v_bioflag & VBIOONFREELIST) &&
	    vp->v_holdcnt == 0 && vp->v_usecount == 0) {
		TAILQ_REMOVE(&vnode_free_list, vp, v_freelist);
		TAILQ_INSERT_TAIL(&vnode_hold_list, vp, v_freelist);
	}
	vp->v_holdcnt++;
}

/* Lose interest in a vnode. */
void
vdrop(struct vnode *vp)
{
#ifdef DIAGNOSTIC
	if (vp->v_holdcnt == 0)
		panic("vdrop: zero holdcnt");
#endif

	vp->v_holdcnt--;

	/*
	 * If it is on the holdlist and the hold count drops to
	 * zero, move it to the free list.
	 */
	if ((vp->v_bioflag & VBIOONFREELIST) &&
	    vp->v_holdcnt == 0 && vp->v_usecount == 0) {
		TAILQ_REMOVE(&vnode_hold_list, vp, v_freelist);
		TAILQ_INSERT_TAIL(&vnode_free_list, vp, v_freelist);
	}
}

/*
 * Remove any vnodes in the vnode table belonging to mount point mp.
 *
 * If MNT_NOFORCE is specified, there should not be any active ones,
 * return error if any are found (nb: this is a user error, not a
 * system error). If MNT_FORCE is specified, detach any active vnodes
 * that are found.
 */
#ifdef DEBUG
int busyprt = 0;	/* print out busy vnodes */
struct ctldebug debug1 = { "busyprt", &busyprt };
#endif

int
vfs_mount_foreach_vnode(struct mount *mp, 
    int (*func)(struct vnode *, void *), void *arg) {
	struct vnode *vp, *nvp;
	int error = 0;

loop:
	LIST_FOREACH_SAFE(vp , &mp->mnt_vnodelist, v_mntvnodes, nvp) {
		if (vp->v_mount != mp)
			goto loop;

		error = func(vp, arg);

		if (error != 0)
			break;
	}

	return (error);
}

struct vflush_args {
	struct vnode *skipvp;
	int busy;
	int flags;
};

int
vflush_vnode(struct vnode *vp, void *arg) {
	struct vflush_args *va = arg;
	struct proc *p = curproc;

	if (vp == va->skipvp) {
		return (0);
	}

	if ((va->flags & SKIPSYSTEM) && (vp->v_flag & VSYSTEM)) {
		return (0);
	}

	/*
	 * If WRITECLOSE is set, only flush out regular file
	 * vnodes open for writing.
	 */
	if ((va->flags & WRITECLOSE) &&
	    (vp->v_writecount == 0 || vp->v_type != VREG)) {
		return (0);
	}

	/*
	 * With v_usecount == 0, all we need to do is clear
	 * out the vnode data structures and we are done.
	 */
	if (vp->v_usecount == 0) {
		vgonel(vp, p);
		return (0);
	}

	/*
	 * If FORCECLOSE is set, forcibly close the vnode.
	 * For block or character devices, revert to an
	 * anonymous device. For all other files, just kill them.
	 */
	if (va->flags & FORCECLOSE) {
		if (vp->v_type != VBLK && vp->v_type != VCHR) {
			vgonel(vp, p);
		} else {
			vclean(vp, 0, p);
			vp->v_op = &spec_vops;
			insmntque(vp, NULL);
		}
		return (0);
	}

#ifdef DEBUG
	if (busyprt)
		vprint("vflush: busy vnode", vp);
#endif
	va->busy++;
	return (0);
}

int
vflush(struct mount *mp, struct vnode *skipvp, int flags)
{
	struct vflush_args va;
	va.skipvp = skipvp;
	va.busy = 0;
	va.flags = flags;

	vfs_mount_foreach_vnode(mp, vflush_vnode, &va);

	if (va.busy)
		return (EBUSY);
	return (0);
}

/*
 * Disassociate the underlying file system from a vnode.
 */
void
vclean(struct vnode *vp, int flags, struct proc *p)
{
	int active;

	/*
	 * Check to see if the vnode is in use.
	 * If so we have to reference it before we clean it out
	 * so that its count cannot fall to zero and generate a
	 * race against ourselves to recycle it.
	 */
	if ((active = vp->v_usecount) != 0)
		vp->v_usecount++;

	/*
	 * Prevent the vnode from being recycled or
	 * brought into use while we clean it out.
	 */
	if (vp->v_flag & VXLOCK)
		panic("vclean: deadlock");
	vp->v_flag |= VXLOCK;
	/*
	 * Even if the count is zero, the VOP_INACTIVE routine may still
	 * have the object locked while it cleans it out. The VOP_LOCK
	 * ensures that the VOP_INACTIVE routine is done with its work.
	 * For active vnodes, it ensures that no other activity can
	 * occur while the underlying object is being cleaned out.
	 */
	VOP_LOCK(vp, LK_DRAIN | LK_EXCLUSIVE, p);

	/*
	 * Clean out any VM data associated with the vnode.
	 */
	uvm_vnp_terminate(vp);
	/*
	 * Clean out any buffers associated with the vnode.
	 */
	if (flags & DOCLOSE)
		vinvalbuf(vp, V_SAVE, NOCRED, p, 0, 0);
	/*
	 * If purging an active vnode, it must be closed and
	 * deactivated before being reclaimed. Note that the
	 * VOP_INACTIVE will unlock the vnode
	 */
	if (active) {
		if (flags & DOCLOSE)
			VOP_CLOSE(vp, FNONBLOCK, NOCRED, p);
		VOP_INACTIVE(vp, p);
	} else {
		/*
		 * Any other processes trying to obtain this lock must first
		 * wait for VXLOCK to clear, then call the new lock operation.
		 */
		VOP_UNLOCK(vp, p);
	}

	/*
	 * Reclaim the vnode.
	 */
	if (VOP_RECLAIM(vp, p))
		panic("vclean: cannot reclaim");
	if (active) {
		vp->v_usecount--;
		if (vp->v_usecount == 0) {
			if (vp->v_holdcnt > 0)
				panic("vclean: not clean");
			vputonfreelist(vp);
		}
	}
	cache_purge(vp);

	/*
	 * Done with purge, notify sleepers of the grim news.
	 */
	vp->v_op = &dead_vops;
	VN_KNOTE(vp, NOTE_REVOKE);
	vp->v_tag = VT_NON;
	vp->v_flag &= ~VXLOCK;
#ifdef VFSLCKDEBUG
	vp->v_flag &= ~VLOCKSWORK;
#endif
	if (vp->v_flag & VXWANT) {
		vp->v_flag &= ~VXWANT;
		wakeup(vp);
	}
}

/*
 * Recycle an unused vnode to the front of the free list.
 */
int
vrecycle(struct vnode *vp, struct proc *p)
{
	if (vp->v_usecount == 0) {
		vgonel(vp, p);
		return (1);
	}
	return (0);
}

/*
 * Eliminate all activity associated with a vnode
 * in preparation for reuse.
 */
void
vgone(struct vnode *vp)
{
	struct proc *p = curproc;
	vgonel(vp, p);
}

/*
 * vgone, with struct proc.
 */
void
vgonel(struct vnode *vp, struct proc *p)
{
	struct vnode *vq;
	struct vnode *vx;

	/*
	 * If a vgone (or vclean) is already in progress,
	 * wait until it is done and return.
	 */
	if (vp->v_flag & VXLOCK) {
		vp->v_flag |= VXWANT;
		tsleep(vp, PINOD, "vgone", 0);
		return;
	}

	/*
	 * Clean out the filesystem specific data.
	 */
	vclean(vp, DOCLOSE, p);
	/*
	 * Delete from old mount point vnode list, if on one.
	 */
	if (vp->v_mount != NULL)
		insmntque(vp, NULL);
	/*
	 * If special device, remove it from special device alias list
	 * if it is on one.
	 */
	if ((vp->v_type == VBLK || vp->v_type == VCHR) && vp->v_specinfo != 0) {
		if ((vp->v_flag & VALIASED) == 0 && vp->v_type == VCHR &&
		    (cdevsw[major(vp->v_rdev)].d_flags & D_CLONE) &&
		    (minor(vp->v_rdev) >> CLONE_SHIFT == 0)) {
			free(vp->v_specbitmap, M_VNODE, CLONE_MAPSZ);
		}
		if (*vp->v_hashchain == vp) {
			*vp->v_hashchain = vp->v_specnext;
		} else {
			for (vq = *vp->v_hashchain; vq; vq = vq->v_specnext) {
				if (vq->v_specnext != vp)
					continue;
				vq->v_specnext = vp->v_specnext;
				break;
			}
			if (vq == NULL)
				panic("missing bdev");
		}
		if (vp->v_flag & VALIASED) {
			vx = NULL;
			for (vq = *vp->v_hashchain; vq; vq = vq->v_specnext) {
				if (vq->v_rdev != vp->v_rdev ||
				    vq->v_type != vp->v_type)
					continue;
				if (vx)
					break;
				vx = vq;
			}
			if (vx == NULL)
				panic("missing alias");
			if (vq == NULL)
				vx->v_flag &= ~VALIASED;
			vp->v_flag &= ~VALIASED;
		}
		free(vp->v_specinfo, M_VNODE, sizeof(struct specinfo));
		vp->v_specinfo = NULL;
	}
	/*
	 * If it is on the freelist and not already at the head,
	 * move it to the head of the list.
	 */
	vp->v_type = VBAD;

	/*
	 * Move onto the free list, unless we were called from
	 * getnewvnode and we're not on any free list
	 */
	if (vp->v_usecount == 0 &&
	    (vp->v_bioflag & VBIOONFREELIST)) {
		int s;

		s = splbio();

		if (vp->v_holdcnt > 0)
			panic("vgonel: not clean");

		if (TAILQ_FIRST(&vnode_free_list) != vp) {
			TAILQ_REMOVE(&vnode_free_list, vp, v_freelist);
			TAILQ_INSERT_HEAD(&vnode_free_list, vp, v_freelist);
		}
		splx(s);
	}
}

/*
 * Lookup a vnode by device number.
 */
int
vfinddev(dev_t dev, enum vtype type, struct vnode **vpp)
{
	struct vnode *vp;
	int rc =0;

	for (vp = speclisth[SPECHASH(dev)]; vp; vp = vp->v_specnext) {
		if (dev != vp->v_rdev || type != vp->v_type)
			continue;
		*vpp = vp;
		rc = 1;
		break;
	}
	return (rc);
}

/*
 * Revoke all the vnodes corresponding to the specified minor number
 * range (endpoints inclusive) of the specified major.
 */
void
vdevgone(int maj, int minl, int minh, enum vtype type)
{
	struct vnode *vp;
	int mn;

	for (mn = minl; mn <= minh; mn++)
		if (vfinddev(makedev(maj, mn), type, &vp))
			VOP_REVOKE(vp, REVOKEALL);
}

/*
 * Calculate the total number of references to a special device.
 */
int
vcount(struct vnode *vp)
{
	struct vnode *vq, *vnext;
	int count;

loop:
	if ((vp->v_flag & VALIASED) == 0)
		return (vp->v_usecount);
	for (count = 0, vq = *vp->v_hashchain; vq; vq = vnext) {
		vnext = vq->v_specnext;
		if (vq->v_rdev != vp->v_rdev || vq->v_type != vp->v_type)
			continue;
		/*
		 * Alias, but not in use, so flush it out.
		 */
		if (vq->v_usecount == 0 && vq != vp) {
			vgone(vq);
			goto loop;
		}
		count += vq->v_usecount;
	}
	return (count);
}

#if defined(DEBUG) || defined(DIAGNOSTIC)
/*
 * Print out a description of a vnode.
 */
static char *typename[] =
   { "VNON", "VREG", "VDIR", "VBLK", "VCHR", "VLNK", "VSOCK", "VFIFO", "VBAD" };

void
vprint(char *label, struct vnode *vp)
{
	char buf[64];

	if (label != NULL)
		printf("%s: ", label);
	printf("%p, type %s, use %u, write %u, hold %u,",
		vp, typename[vp->v_type], vp->v_usecount, vp->v_writecount,
		vp->v_holdcnt);
	buf[0] = '\0';
	if (vp->v_flag & VROOT)
		strlcat(buf, "|VROOT", sizeof buf);
	if (vp->v_flag & VTEXT)
		strlcat(buf, "|VTEXT", sizeof buf);
	if (vp->v_flag & VSYSTEM)
		strlcat(buf, "|VSYSTEM", sizeof buf);
	if (vp->v_flag & VXLOCK)
		strlcat(buf, "|VXLOCK", sizeof buf);
	if (vp->v_flag & VXWANT)
		strlcat(buf, "|VXWANT", sizeof buf);
	if (vp->v_bioflag & VBIOWAIT)
		strlcat(buf, "|VBIOWAIT", sizeof buf);
	if (vp->v_bioflag & VBIOONFREELIST)
		strlcat(buf, "|VBIOONFREELIST", sizeof buf);
	if (vp->v_bioflag & VBIOONSYNCLIST)
		strlcat(buf, "|VBIOONSYNCLIST", sizeof buf);
	if (vp->v_flag & VALIASED)
		strlcat(buf, "|VALIASED", sizeof buf);
	if (buf[0] != '\0')
		printf(" flags (%s)", &buf[1]);
	if (vp->v_data == NULL) {
		printf("\n");
	} else {
		printf("\n\t");
		VOP_PRINT(vp);
	}
}
#endif /* DEBUG || DIAGNOSTIC */

#ifdef DEBUG
/*
 * List all of the locked vnodes in the system.
 * Called when debugging the kernel.
 */
void
printlockedvnodes(void)
{
	struct mount *mp;
	struct vnode *vp;

	printf("Locked vnodes\n");

	TAILQ_FOREACH(mp, &mountlist, mnt_list) {
		if (vfs_busy(mp, VB_READ|VB_NOWAIT))
			continue;
		LIST_FOREACH(vp, &mp->mnt_vnodelist, v_mntvnodes) {
			if (VOP_ISLOCKED(vp))
				vprint(NULL, vp);
		}
		vfs_unbusy(mp);
 	}

}
#endif

/*
 * Top level filesystem related information gathering.
 */
int
vfs_sysctl(int *name, u_int namelen, void *oldp, size_t *oldlenp, void *newp,
    size_t newlen, struct proc *p)
{
	struct vfsconf *vfsp, *tmpvfsp;
	int ret;

	/* all sysctl names at this level are at least name and field */
	if (namelen < 2)
		return (ENOTDIR);		/* overloaded */

	if (name[0] != VFS_GENERIC) {
		for (vfsp = vfsconf; vfsp; vfsp = vfsp->vfc_next)
			if (vfsp->vfc_typenum == name[0])
				break;

		if (vfsp == NULL || vfsp->vfc_vfsops->vfs_sysctl == NULL)
			return (EOPNOTSUPP);

		return ((*vfsp->vfc_vfsops->vfs_sysctl)(&name[1], namelen - 1,
		    oldp, oldlenp, newp, newlen, p));
	}

	switch (name[1]) {
	case VFS_MAXTYPENUM:
		return (sysctl_rdint(oldp, oldlenp, newp, maxvfsconf));

	case VFS_CONF:
		if (namelen < 3)
			return (ENOTDIR);	/* overloaded */

		for (vfsp = vfsconf; vfsp; vfsp = vfsp->vfc_next)
			if (vfsp->vfc_typenum == name[2])
				break;

		if (vfsp == NULL)
			return (EOPNOTSUPP);

		/* Make a copy, clear out kernel pointers */
		tmpvfsp = malloc(sizeof(*tmpvfsp), M_TEMP, M_WAITOK|M_ZERO);
		memcpy(tmpvfsp, vfsp, sizeof(*tmpvfsp));
		tmpvfsp->vfc_vfsops = NULL;
		tmpvfsp->vfc_next = NULL;

		ret = sysctl_rdstruct(oldp, oldlenp, newp, tmpvfsp,
		    sizeof(struct vfsconf));

		free(tmpvfsp, M_TEMP, sizeof(*tmpvfsp));
		return (ret);
	case VFS_BCACHESTAT:	/* buffer cache statistics */
		ret = sysctl_rdstruct(oldp, oldlenp, newp, &bcstats,
		    sizeof(struct bcachestats));
		return(ret);
	}
	return (EOPNOTSUPP);
}

/*
 * Check to see if a filesystem is mounted on a block device.
 */
int
vfs_mountedon(struct vnode *vp)
{
	struct vnode *vq;
	int error = 0;

 	if (vp->v_specmountpoint != NULL)
		return (EBUSY);
	if (vp->v_flag & VALIASED) {
		for (vq = *vp->v_hashchain; vq; vq = vq->v_specnext) {
			if (vq->v_rdev != vp->v_rdev ||
			    vq->v_type != vp->v_type)
				continue;
			if (vq->v_specmountpoint != NULL) {
				error = EBUSY;
				break;
			}
 		}
	}
	return (error);
}

#ifdef NFSSERVER
/*
 * Build hash lists of net addresses and hang them off the mount point.
 * Called by vfs_export() to set up the lists of export addresses.
 */
int
vfs_hang_addrlist(struct mount *mp, struct netexport *nep,
    struct export_args *argp)
{
	struct netcred *np;
	struct radix_node_head *rnh;
	int nplen, i;
	struct radix_node *rn;
	struct sockaddr *saddr, *smask = 0;
	int error;

	if (argp->ex_addrlen == 0) {
		if (mp->mnt_flag & MNT_DEFEXPORTED)
			return (EPERM);
		np = &nep->ne_defexported;
		/* fill in the kernel's ucred from userspace's xucred */
		if ((error = crfromxucred(&np->netc_anon, &argp->ex_anon)))
			return (error);
		mp->mnt_flag |= MNT_DEFEXPORTED;
		goto finish;
	}
	if (argp->ex_addrlen > MLEN || argp->ex_masklen > MLEN ||
	    argp->ex_addrlen < 0 || argp->ex_masklen < 0)
		return (EINVAL);
	nplen = sizeof(struct netcred) + argp->ex_addrlen + argp->ex_masklen;
	np = (struct netcred *)malloc(nplen, M_NETADDR, M_WAITOK|M_ZERO);
	saddr = (struct sockaddr *)(np + 1);
	error = copyin(argp->ex_addr, saddr, argp->ex_addrlen);
	if (error)
		goto out;
	if (saddr->sa_len > argp->ex_addrlen)
		saddr->sa_len = argp->ex_addrlen;
	if (argp->ex_masklen) {
		smask = (struct sockaddr *)((caddr_t)saddr + argp->ex_addrlen);
		error = copyin(argp->ex_mask, smask, argp->ex_masklen);
		if (error)
			goto out;
		if (smask->sa_len > argp->ex_masklen)
			smask->sa_len = argp->ex_masklen;
	}
	/* fill in the kernel's ucred from userspace's xucred */
	if ((error = crfromxucred(&np->netc_anon, &argp->ex_anon)))
		goto out;
	i = saddr->sa_family;
	switch (i) {
	case AF_INET:
		if ((rnh = nep->ne_rtable_inet) == NULL) {
			if (!rn_inithead((void **)&nep->ne_rtable_inet,
			    offsetof(struct sockaddr_in, sin_addr))) {
				error = ENOBUFS;
				goto out;
			}
			rnh = nep->ne_rtable_inet;
		}
		break;
	default:
		error = EINVAL;
		goto out;
	}
	rn = rn_addroute(saddr, smask, rnh, np->netc_rnodes, 0);
	if (rn == 0 || np != (struct netcred *)rn) { /* already exists */
		error = EPERM;
		goto out;
	}
finish:
	np->netc_exflags = argp->ex_flags;
	return (0);
out:
	free(np, M_NETADDR, nplen);
	return (error);
}

int
vfs_free_netcred(struct radix_node *rn, void *w, u_int id)
{
	struct radix_node_head *rnh = (struct radix_node_head *)w;

	rn_delete(rn->rn_key, rn->rn_mask, rnh, NULL);
	free(rn, M_NETADDR, 0);
	return (0);
}

/*
 * Free the net address hash lists that are hanging off the mount points.
 */
void
vfs_free_addrlist(struct netexport *nep)
{
	struct radix_node_head *rnh;

	if ((rnh = nep->ne_rtable_inet) != NULL) {
		rn_walktree(rnh, vfs_free_netcred, rnh);
		free(rnh, M_RTABLE, 0);
		nep->ne_rtable_inet = NULL;
	}
}
#endif /* NFSSERVER */

int
vfs_export(struct mount *mp, struct netexport *nep, struct export_args *argp)
{
#ifdef NFSSERVER
	int error;

	if (argp->ex_flags & MNT_DELEXPORT) {
		vfs_free_addrlist(nep);
		mp->mnt_flag &= ~(MNT_EXPORTED | MNT_DEFEXPORTED);
	}
	if (argp->ex_flags & MNT_EXPORTED) {
		if ((error = vfs_hang_addrlist(mp, nep, argp)) != 0)
			return (error);
		mp->mnt_flag |= MNT_EXPORTED;
	}
	return (0);
#else
	return (ENOTSUP);
#endif /* NFSSERVER */
}

struct netcred *
vfs_export_lookup(struct mount *mp, struct netexport *nep, struct mbuf *nam)
{
#ifdef NFSSERVER
	struct netcred *np;
	struct radix_node_head *rnh;
	struct sockaddr *saddr;

	np = NULL;
	if (mp->mnt_flag & MNT_EXPORTED) {
		/*
		 * Lookup in the export list first.
		 */
		if (nam != NULL) {
			saddr = mtod(nam, struct sockaddr *);
			switch(saddr->sa_family) {
			case AF_INET:
				rnh = nep->ne_rtable_inet;
				break;
			default:
				rnh = NULL;
				break;
			}
			if (rnh != NULL)
				np = (struct netcred *)rn_match(saddr, rnh);
		}
		/*
		 * If no address match, use the default if it exists.
		 */
		if (np == NULL && mp->mnt_flag & MNT_DEFEXPORTED)
			np = &nep->ne_defexported;
	}
	return (np);
#else
	return (NULL);
#endif /* NFSSERVER */
}

/*
 * Do the usual access checking.
 * file_mode, uid and gid are from the vnode in question,
 * while acc_mode and cred are from the VOP_ACCESS parameter list
 */
int
vaccess(enum vtype type, mode_t file_mode, uid_t uid, gid_t gid,
    mode_t acc_mode, struct ucred *cred)
{
	mode_t mask;

	/* User id 0 always gets read/write access. */
	if (cred->cr_uid == 0) {
		/* For VEXEC, at least one of the execute bits must be set. */
		if ((acc_mode & VEXEC) && type != VDIR &&
		    (file_mode & (S_IXUSR|S_IXGRP|S_IXOTH)) == 0)
			return EACCES;
		return 0;
	}

	mask = 0;

	/* Otherwise, check the owner. */
	if (cred->cr_uid == uid) {
		if (acc_mode & VEXEC)
			mask |= S_IXUSR;
		if (acc_mode & VREAD)
			mask |= S_IRUSR;
		if (acc_mode & VWRITE)
			mask |= S_IWUSR;
		return (file_mode & mask) == mask ? 0 : EACCES;
	}

	/* Otherwise, check the groups. */
	if (groupmember(gid, cred)) {
		if (acc_mode & VEXEC)
			mask |= S_IXGRP;
		if (acc_mode & VREAD)
			mask |= S_IRGRP;
		if (acc_mode & VWRITE)
			mask |= S_IWGRP;
		return (file_mode & mask) == mask ? 0 : EACCES;
	}

	/* Otherwise, check everyone else. */
	if (acc_mode & VEXEC)
		mask |= S_IXOTH;
	if (acc_mode & VREAD)
		mask |= S_IROTH;
	if (acc_mode & VWRITE)
		mask |= S_IWOTH;
	return (file_mode & mask) == mask ? 0 : EACCES;
}

/*
 * Unmount all file systems.
 * We traverse the list in reverse order under the assumption that doing so
 * will avoid needing to worry about dependencies.
 */
void
vfs_unmountall(void)
{
	struct mount *mp, *nmp;
	int allerror, error, again = 1;

 retry:
	allerror = 0;
	TAILQ_FOREACH_REVERSE_SAFE(mp, &mountlist, mntlist, mnt_list, nmp) {
		if (vfs_busy(mp, VB_WRITE|VB_NOWAIT))
			continue;
		/* XXX Here is a race, the next pointer is not locked. */
		if ((error = dounmount(mp, MNT_FORCE, curproc)) != 0) {
			printf("unmount of %s failed with error %d\n",
			    mp->mnt_stat.f_mntonname, error);
			allerror = 1;
		}
	}

	if (allerror) {
		printf("WARNING: some file systems would not unmount\n");
		if (again) {
			printf("retrying\n");
			again = 0;
			goto retry;
		}
	}
}

/*
 * Sync and unmount file systems before shutting down.
 */
void
vfs_shutdown(void)
{
#ifdef ACCOUNTING
	acct_shutdown();
#endif

	/* XXX Should suspend scheduling. */
	(void) spl0();

	printf("syncing disks... ");

	if (panicstr == 0) {
		/* Sync before unmount, in case we hang on something. */
		sys_sync(&proc0, NULL, NULL);

		/* Unmount file systems. */
		vfs_unmountall();
	}

	if (vfs_syncwait(1))
		printf("giving up\n");
	else
		printf("done\n");

#if NSOFTRAID > 0
	sr_shutdown();
#endif
}

/*
 * perform sync() operation and wait for buffers to flush.
 * assumptions: called w/ scheduler disabled and physical io enabled
 * for now called at spl0() XXX
 */
int
vfs_syncwait(int verbose)
{
	struct buf *bp;
	int iter, nbusy, dcount, s;
	struct proc *p;
#ifdef MULTIPROCESSOR
	int hold_count;
#endif

	p = curproc? curproc : &proc0;
	sys_sync(p, NULL, NULL);

	/* Wait for sync to finish. */
	dcount = 10000;
	for (iter = 0; iter < 20; iter++) {
		nbusy = 0;
		LIST_FOREACH(bp, &bufhead, b_list) {
			if ((bp->b_flags & (B_BUSY|B_INVAL|B_READ)) == B_BUSY)
				nbusy++;
			/*
			 * With soft updates, some buffers that are
			 * written will be remarked as dirty until other
			 * buffers are written.
			 */
			if (bp->b_flags & B_DELWRI) {
				s = splbio();
				bremfree(bp);
				buf_acquire(bp);
				splx(s);
				nbusy++;
				bawrite(bp);
				if (dcount-- <= 0) {
					if (verbose)
						printf("softdep ");
					return 1;
				}
			}
		}
		if (nbusy == 0)
			break;
		if (verbose)
			printf("%d ", nbusy);
#ifdef MULTIPROCESSOR
		if (__mp_lock_held(&kernel_lock))
			hold_count = __mp_release_all(&kernel_lock);
		else
			hold_count = 0;
#endif
		DELAY(40000 * iter);
#ifdef MULTIPROCESSOR
		if (hold_count)
			__mp_acquire_count(&kernel_lock, hold_count);
#endif
	}

	return nbusy;
}

/*
 * posix file system related system variables.
 */
int
fs_posix_sysctl(int *name, u_int namelen, void *oldp, size_t *oldlenp,
    void *newp, size_t newlen, struct proc *p)
{
	/* all sysctl names at this level are terminal */
	if (namelen != 1)
		return (ENOTDIR);

	switch (name[0]) {
	case FS_POSIX_SETUID:
		if (newp && securelevel > 0)
			return (EPERM);
		return(sysctl_int(oldp, oldlenp, newp, newlen, &suid_clear));
	default:
		return (EOPNOTSUPP);
	}
	/* NOTREACHED */
}

/*
 * file system related system variables.
 */
int
fs_sysctl(int *name, u_int namelen, void *oldp, size_t *oldlenp, void *newp,
    size_t newlen, struct proc *p)
{
	sysctlfn *fn;

	switch (name[0]) {
	case FS_POSIX:
		fn = fs_posix_sysctl;
		break;
	default:
		return (EOPNOTSUPP);
	}
	return (*fn)(name + 1, namelen - 1, oldp, oldlenp, newp, newlen, p);
}


/*
 * Routines dealing with vnodes and buffers
 */

/*
 * Wait for all outstanding I/Os to complete
 *
 * Manipulates v_numoutput. Must be called at splbio()
 */
int
vwaitforio(struct vnode *vp, int slpflag, char *wmesg, int timeo)
{
	int error = 0;

	splassert(IPL_BIO);

	while (vp->v_numoutput) {
		vp->v_bioflag |= VBIOWAIT;
		error = tsleep(&vp->v_numoutput,
		    slpflag | (PRIBIO + 1), wmesg, timeo);
		if (error)
			break;
	}

	return (error);
}

/*
 * Update outstanding I/O count and do wakeup if requested.
 *
 * Manipulates v_numoutput. Must be called at splbio()
 */
void
vwakeup(struct vnode *vp)
{
	splassert(IPL_BIO);

	if (vp != NULL) {
		if (vp->v_numoutput-- == 0)
			panic("vwakeup: neg numoutput");
		if ((vp->v_bioflag & VBIOWAIT) && vp->v_numoutput == 0) {
			vp->v_bioflag &= ~VBIOWAIT;
			wakeup(&vp->v_numoutput);
		}
	}
}

/*
 * Flush out and invalidate all buffers associated with a vnode.
 * Called with the underlying object locked.
 */
int
vinvalbuf(struct vnode *vp, int flags, struct ucred *cred, struct proc *p,
    int slpflag, int slptimeo)
{
	struct buf *bp;
	struct buf *nbp, *blist;
	int s, error;

#ifdef VFSLCKDEBUG
	if ((vp->v_flag & VLOCKSWORK) && !VOP_ISLOCKED(vp))
		panic("vinvalbuf(): vp isn't locked");
#endif

	if (flags & V_SAVE) {
		s = splbio();
		vwaitforio(vp, 0, "vinvalbuf", 0);
		if (!LIST_EMPTY(&vp->v_dirtyblkhd)) {
			splx(s);
			if ((error = VOP_FSYNC(vp, cred, MNT_WAIT, p)) != 0)
				return (error);
			s = splbio();
			if (vp->v_numoutput > 0 ||
			    !LIST_EMPTY(&vp->v_dirtyblkhd))
				panic("vinvalbuf: dirty bufs");
		}
		splx(s);
	}
loop:
	s = splbio();
	for (;;) {
		if ((blist = LIST_FIRST(&vp->v_cleanblkhd)) &&
		    (flags & V_SAVEMETA))
			while (blist && blist->b_lblkno < 0)
				blist = LIST_NEXT(blist, b_vnbufs);
		if (blist == NULL &&
		    (blist = LIST_FIRST(&vp->v_dirtyblkhd)) &&
		    (flags & V_SAVEMETA))
			while (blist && blist->b_lblkno < 0)
				blist = LIST_NEXT(blist, b_vnbufs);
		if (!blist)
			break;

		for (bp = blist; bp; bp = nbp) {
			nbp = LIST_NEXT(bp, b_vnbufs);
			if (flags & V_SAVEMETA && bp->b_lblkno < 0)
				continue;
			if (bp->b_flags & B_BUSY) {
				bp->b_flags |= B_WANTED;
				error = tsleep(bp, slpflag | (PRIBIO + 1),
				    "vinvalbuf", slptimeo);
				if (error) {
					splx(s);
					return (error);
				}
				break;
			}
			bremfree(bp);
			/*
			 * XXX Since there are no node locks for NFS, I believe
			 * there is a slight chance that a delayed write will
			 * occur while sleeping just above, so check for it.
			 */
			if ((bp->b_flags & B_DELWRI) && (flags & V_SAVE)) {
				buf_acquire(bp);
				splx(s);
				(void) VOP_BWRITE(bp);
				goto loop;
			}
			buf_acquire_nomap(bp);
			bp->b_flags |= B_INVAL;
			brelse(bp);
		}
	}
	if (!(flags & V_SAVEMETA) &&
	    (!LIST_EMPTY(&vp->v_dirtyblkhd) || !LIST_EMPTY(&vp->v_cleanblkhd)))
		panic("vinvalbuf: flush failed");
	splx(s);
	return (0);
}

void
vflushbuf(struct vnode *vp, int sync)
{
	struct buf *bp, *nbp;
	int s;

loop:
	s = splbio();
	LIST_FOREACH_SAFE(bp, &vp->v_dirtyblkhd, b_vnbufs, nbp) {
		if ((bp->b_flags & B_BUSY))
			continue;
		if ((bp->b_flags & B_DELWRI) == 0)
			panic("vflushbuf: not dirty");
		bremfree(bp);
		buf_acquire(bp);
		splx(s);
		/*
		 * Wait for I/O associated with indirect blocks to complete,
		 * since there is no way to quickly wait for them below.
		 */
		if (bp->b_vp == vp || sync == 0)
			(void) bawrite(bp);
		else
			(void) bwrite(bp);
		goto loop;
	}
	if (sync == 0) {
		splx(s);
		return;
	}
	vwaitforio(vp, 0, "vflushbuf", 0);
	if (!LIST_EMPTY(&vp->v_dirtyblkhd)) {
		splx(s);
#ifdef DIAGNOSTIC
		vprint("vflushbuf: dirty", vp);
#endif
		goto loop;
	}
	splx(s);
}

/*
 * Associate a buffer with a vnode.
 *
 * Manipulates buffer vnode queues. Must be called at splbio().
 */
void
bgetvp(struct vnode *vp, struct buf *bp)
{
	splassert(IPL_BIO);


	if (bp->b_vp)
		panic("bgetvp: not free");
	vhold(vp);
	bp->b_vp = vp;
	if (vp->v_type == VBLK || vp->v_type == VCHR)
		bp->b_dev = vp->v_rdev;
	else
		bp->b_dev = NODEV;
	/*
	 * Insert onto list for new vnode.
	 */
	bufinsvn(bp, &vp->v_cleanblkhd);
}

/*
 * Disassociate a buffer from a vnode.
 *
 * Manipulates vnode buffer queues. Must be called at splbio().
 */
void
brelvp(struct buf *bp)
{
	struct vnode *vp;

	splassert(IPL_BIO);

	if ((vp = bp->b_vp) == (struct vnode *) 0)
		panic("brelvp: NULL");
	/*
	 * Delete from old vnode list, if on one.
	 */
	if (LIST_NEXT(bp, b_vnbufs) != NOLIST)
		bufremvn(bp);
	if ((vp->v_bioflag & VBIOONSYNCLIST) &&
	    LIST_FIRST(&vp->v_dirtyblkhd) == NULL) {
		vp->v_bioflag &= ~VBIOONSYNCLIST;
		LIST_REMOVE(vp, v_synclist);
	}
	bp->b_vp = NULL;

	vdrop(vp);
}

/*
 * Replaces the current vnode associated with the buffer, if any,
 * with a new vnode.
 *
 * If an output I/O is pending on the buffer, the old vnode
 * I/O count is adjusted.
 *
 * Ignores vnode buffer queues. Must be called at splbio().
 */
void
buf_replacevnode(struct buf *bp, struct vnode *newvp)
{
	struct vnode *oldvp = bp->b_vp;

	splassert(IPL_BIO);

	if (oldvp)
		brelvp(bp);

	if ((bp->b_flags & (B_READ | B_DONE)) == 0) {
		newvp->v_numoutput++;	/* put it on swapdev */
		vwakeup(oldvp);
	}

	bgetvp(newvp, bp);
	bufremvn(bp);
}

/*
 * Used to assign buffers to the appropriate clean or dirty list on
 * the vnode and to add newly dirty vnodes to the appropriate
 * filesystem syncer list.
 *
 * Manipulates vnode buffer queues. Must be called at splbio().
 */
void
reassignbuf(struct buf *bp)
{
	struct buflists *listheadp;
	int delay;
	struct vnode *vp = bp->b_vp;

	splassert(IPL_BIO);

	/*
	 * Delete from old vnode list, if on one.
	 */
	if (LIST_NEXT(bp, b_vnbufs) != NOLIST)
		bufremvn(bp);

	/*
	 * If dirty, put on list of dirty buffers;
	 * otherwise insert onto list of clean buffers.
	 */
	if ((bp->b_flags & B_DELWRI) == 0) {
		listheadp = &vp->v_cleanblkhd;
		if ((vp->v_bioflag & VBIOONSYNCLIST) &&
		    LIST_FIRST(&vp->v_dirtyblkhd) == NULL) {
			vp->v_bioflag &= ~VBIOONSYNCLIST;
			LIST_REMOVE(vp, v_synclist);
		}
	} else {
		listheadp = &vp->v_dirtyblkhd;
		if ((vp->v_bioflag & VBIOONSYNCLIST) == 0) {
			switch (vp->v_type) {
			case VDIR:
				delay = syncdelay / 2;
				break;
			case VBLK:
				if (vp->v_specmountpoint != NULL) {
					delay = syncdelay / 3;
					break;
				}
				/* FALLTHROUGH */
			default:
				delay = syncdelay;
			}
			vn_syncer_add_to_worklist(vp, delay);
		}
	}
	bufinsvn(bp, listheadp);
}

int
vfs_register(struct vfsconf *vfs)
{
	struct vfsconf *vfsp;
	struct vfsconf **vfspp;

#ifdef DIAGNOSTIC
	/* Paranoia? */
	if (vfs->vfc_refcount != 0)
		printf("vfs_register called with vfc_refcount > 0\n");
#endif

	/* Check if filesystem already known */
	for (vfspp = &vfsconf, vfsp = vfsconf; vfsp;
	    vfspp = &vfsp->vfc_next, vfsp = vfsp->vfc_next)
		if (strcmp(vfsp->vfc_name, vfs->vfc_name) == 0)
			return (EEXIST);

	if (vfs->vfc_typenum > maxvfsconf)
		maxvfsconf = vfs->vfc_typenum;

	vfs->vfc_next = NULL;

	/* Add to the end of the list */
	*vfspp = vfs;

	/* Call vfs_init() */
	if (vfs->vfc_vfsops->vfs_init)
		(*(vfs->vfc_vfsops->vfs_init))(vfs);

	return 0;
}

int
vfs_unregister(struct vfsconf *vfs)
{
	struct vfsconf *vfsp;
	struct vfsconf **vfspp;
	int maxtypenum;

	/* Find our vfsconf struct */
	for (vfspp = &vfsconf, vfsp = vfsconf; vfsp;
	    vfspp = &vfsp->vfc_next, vfsp = vfsp->vfc_next) {
		if (strcmp(vfsp->vfc_name, vfs->vfc_name) == 0)
			break;
	}

	if (!vfsp)			/* Not found */
		return (ENOENT);

	if (vfsp->vfc_refcount)		/* In use */
		return (EBUSY);

	/* Remove from list and free */
	*vfspp = vfsp->vfc_next;

	maxtypenum = 0;

	for (vfsp = vfsconf; vfsp; vfsp = vfsp->vfc_next)
		if (vfsp->vfc_typenum > maxtypenum)
			maxtypenum = vfsp->vfc_typenum;

	maxvfsconf = maxtypenum;
	return 0;
}

/*
 * Check if vnode represents a disk device
 */
int
vn_isdisk(struct vnode *vp, int *errp)
{
	if (vp->v_type != VBLK && vp->v_type != VCHR)
		return (0);

	return (1);
}

#ifdef DDB
#include <machine/db_machdep.h>
#include <ddb/db_interface.h>

void
vfs_buf_print(void *b, int full,
    int (*pr)(const char *, ...) __attribute__((__format__(__kprintf__,1,2))))
{
	struct buf *bp = b;

	(*pr)("  vp %p lblkno 0x%llx blkno 0x%llx dev 0x%x\n"
	      "  proc %p error %d flags %lb\n",
	    bp->b_vp, (int64_t)bp->b_lblkno, (int64_t)bp->b_blkno, bp->b_dev,
	    bp->b_proc, bp->b_error, bp->b_flags, B_BITS);

	(*pr)("  bufsize 0x%lx bcount 0x%lx resid 0x%lx\n"
	      "  data %p saveaddr %p dep %p iodone %p\n",
	    bp->b_bufsize, bp->b_bcount, (long)bp->b_resid,
	    bp->b_data, bp->b_saveaddr,
	    LIST_FIRST(&bp->b_dep), bp->b_iodone);

	(*pr)("  dirty {off 0x%x end 0x%x} valid {off 0x%x end 0x%x}\n",
	    bp->b_dirtyoff, bp->b_dirtyend, bp->b_validoff, bp->b_validend);

#ifdef FFS_SOFTUPDATES
	if (full)
		softdep_print(bp, full, pr);
#endif
}

const char *vtypes[] = { VTYPE_NAMES };
const char *vtags[] = { VTAG_NAMES };

void
vfs_vnode_print(void *v, int full,
    int (*pr)(const char *, ...) __attribute__((__format__(__kprintf__,1,2))))
{
	struct vnode *vp = v;

	(*pr)("tag %s(%d) type %s(%d) mount %p typedata %p\n",
	      (u_int)vp->v_tag >= nitems(vtags)? "<unk>":vtags[vp->v_tag],
	      vp->v_tag,
	      (u_int)vp->v_type >= nitems(vtypes)? "<unk>":vtypes[vp->v_type],
	      vp->v_type, vp->v_mount, vp->v_mountedhere);

	(*pr)("data %p usecount %d writecount %d holdcnt %d numoutput %d\n",
	      vp->v_data, vp->v_usecount, vp->v_writecount,
	      vp->v_holdcnt, vp->v_numoutput);

	/* uvm_object_printit(&vp->v_uobj, full, pr); */

	if (full) {
		struct buf *bp;

		(*pr)("clean bufs:\n");
		LIST_FOREACH(bp, &vp->v_cleanblkhd, b_vnbufs) {
			(*pr)(" bp %p\n", bp);
			vfs_buf_print(bp, full, pr);
		}

		(*pr)("dirty bufs:\n");
		LIST_FOREACH(bp, &vp->v_dirtyblkhd, b_vnbufs) {
			(*pr)(" bp %p\n", bp);
			vfs_buf_print(bp, full, pr);
		}
	}
}

void
vfs_mount_print(struct mount *mp, int full,
    int (*pr)(const char *, ...) __attribute__((__format__(__kprintf__,1,2))))
{
	struct vfsconf *vfc = mp->mnt_vfc;
	struct vnode *vp;
	int cnt = 0;

	(*pr)("flags %b\nvnodecovered %p syncer %p data %p\n",
	    mp->mnt_flag, MNT_BITS,
	    mp->mnt_vnodecovered, mp->mnt_syncer, mp->mnt_data);

	(*pr)("vfsconf: ops %p name \"%s\" num %d ref %d flags 0x%x\n",
            vfc->vfc_vfsops, vfc->vfc_name, vfc->vfc_typenum,
	    vfc->vfc_refcount, vfc->vfc_flags);

	(*pr)("statvfs cache: bsize %x iosize %x\nblocks %llu free %llu avail %lld\n",
	    mp->mnt_stat.f_bsize, mp->mnt_stat.f_iosize, mp->mnt_stat.f_blocks,
	    mp->mnt_stat.f_bfree, mp->mnt_stat.f_bavail);

	(*pr)("  files %llu ffiles %llu favail %lld\n", mp->mnt_stat.f_files,
	    mp->mnt_stat.f_ffree, mp->mnt_stat.f_favail);

	(*pr)("  f_fsidx {0x%x, 0x%x} owner %u ctime 0x%llx\n",
	    mp->mnt_stat.f_fsid.val[0], mp->mnt_stat.f_fsid.val[1],
	    mp->mnt_stat.f_owner, mp->mnt_stat.f_ctime);

 	(*pr)("  syncwrites %llu asyncwrites = %llu\n",
	    mp->mnt_stat.f_syncwrites, mp->mnt_stat.f_asyncwrites);

 	(*pr)("  syncreads %llu asyncreads = %llu\n",
	    mp->mnt_stat.f_syncreads, mp->mnt_stat.f_asyncreads);

	(*pr)("  fstype \"%s\" mnton \"%s\" mntfrom \"%s\" mntspec \"%s\"\n",
	    mp->mnt_stat.f_fstypename, mp->mnt_stat.f_mntonname,
	    mp->mnt_stat.f_mntfromname, mp->mnt_stat.f_mntfromspec);

	(*pr)("locked vnodes:");
	/* XXX would take mountlist lock, except ddb has no context */
	LIST_FOREACH(vp, &mp->mnt_vnodelist, v_mntvnodes)
		if (VOP_ISLOCKED(vp)) {
			if (!LIST_NEXT(vp, v_mntvnodes))
				(*pr)(" %p", vp);
			else if (!(cnt++ % (72 / (sizeof(void *) * 2 + 4))))
				(*pr)("\n\t%p", vp);
			else
				(*pr)(", %p", vp);
		}
	(*pr)("\n");

	if (full) {
		(*pr)("all vnodes:\n\t");
		/* XXX would take mountlist lock, except ddb has no context */
		LIST_FOREACH(vp, &mp->mnt_vnodelist, v_mntvnodes)
			if (!LIST_NEXT(vp, v_mntvnodes))
				(*pr)(" %p", vp);
			else if (!(cnt++ % (72 / (sizeof(void *) * 2 + 4))))
				(*pr)(" %p,\n\t", vp);
			else
				(*pr)(" %p,", vp);
		(*pr)("\n");
	}
}
#endif /* DDB */

void
copy_statfs_info(struct statfs *sbp, const struct mount *mp)
{
	const struct statfs *mbp;

	strncpy(sbp->f_fstypename, mp->mnt_vfc->vfc_name, MFSNAMELEN);

	if (sbp == (mbp = &mp->mnt_stat))
		return;

	sbp->f_fsid = mbp->f_fsid;
	sbp->f_owner = mbp->f_owner;
	sbp->f_flags = mbp->f_flags;
	sbp->f_syncwrites = mbp->f_syncwrites;
	sbp->f_asyncwrites = mbp->f_asyncwrites;
	sbp->f_syncreads = mbp->f_syncreads;
	sbp->f_asyncreads = mbp->f_asyncreads;
	sbp->f_namemax = mbp->f_namemax;
	memcpy(sbp->f_mntonname, mp->mnt_stat.f_mntonname, MNAMELEN);
	memcpy(sbp->f_mntfromname, mp->mnt_stat.f_mntfromname, MNAMELEN);
	memcpy(sbp->f_mntfromspec, mp->mnt_stat.f_mntfromspec, MNAMELEN);
	memcpy(&sbp->mount_info, &mp->mnt_stat.mount_info,
	    sizeof(union mount_info));
}
@


1.259
log
@Tweak lock inits to make the system runnable with witness(4)
on amd64 and i386.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.258 2017/04/04 18:17:02 deraadt Exp $	*/
d158 1
d160 1
d1359 1
d1362 1
a1362 1
 * Called by ufs_mount() to set up the lists of export addresses.
d1460 1
d1465 1
d1478 3
d1486 1
d1516 3
@


1.258
log
@struct vfsconf is tightly packed, but let's M_ZERO it in case that ever
changes to avoid exposing userland memory.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.257 2017/01/15 23:18:05 bluhm Exp $	*/
d175 1
a175 1
		rw_init(&mp->mnt_lock, "vfslock");
@


1.257
log
@When traversing the mount list, the current mount point is locked
with vfs_busy().  If the FOREACH_SAFE macro is used, the next pointer
is not locked and could be freed by another process.  Unless
necessary, do not use _SAFE as it is unsafe.  In vfs_unmountall()
the current pointer is actullay freed.  Add a comment that this
race has to be fixed later.
OK krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.256 2017/01/10 19:56:34 bluhm Exp $	*/
d1314 1
a1314 1
		tmpvfsp = malloc(sizeof(*tmpvfsp), M_TEMP, M_WAITOK);
@


1.256
log
@Replace manual for() loops with FOREACH() macro.
OK millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.255 2017/01/10 19:48:32 bluhm Exp $	*/
d1254 1
a1254 1
	struct mount *mp, *nmp;
d1259 1
a1259 1
	TAILQ_FOREACH_SAFE(mp, &mountlist, mnt_list, nmp) {
d1577 1
a1577 1
		if ((vfs_busy(mp, VB_WRITE|VB_NOWAIT)) != 0)
d1579 1
@


1.255
log
@Remove the unused olddp parameter from function dounmount().
OK mpi@@ millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.254 2016/09/28 22:22:52 kettenis Exp $	*/
d383 1
a383 2
		for (vp = TAILQ_FIRST(listhd); vp != NULLVP;
		    vp = TAILQ_NEXT(vp, v_freelist)) {
d838 1
a838 1
	for (vp = LIST_FIRST(&mp->mnt_vnodelist); vp != NULL; vp = nvp) {
a840 1
		nvp = LIST_NEXT(vp, v_mntvnodes);
d1874 1
a1874 2
	for (bp = LIST_FIRST(&vp->v_dirtyblkhd); bp != NULL; bp = nbp) {
		nbp = LIST_NEXT(bp, b_vnbufs);
@


1.254
log
@Cast enum to u_int when doing a bounds check to avoid a clang warning that
the comparison is always true.

ok jca@@, tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.253 2016/09/16 03:21:16 dlg Exp $	*/
d1581 1
a1581 1
		if ((error = dounmount(mp, MNT_FORCE, curproc, NULL)) != 0) {
@


1.253
log
@move the namecache_rb_tree from RB macros to RBT functions.

i had to shuffle the includes a bit. all the knowledge of the RB
tree is now inside vfs_cache.c, and all accesses are via cache_*
functions.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.252 2016/09/16 02:54:51 dlg Exp $	*/
d2165 3
a2167 2
	      vp->v_tag >= nitems(vtags)? "<unk>":vtags[vp->v_tag], vp->v_tag,
	      vp->v_type >= nitems(vtypes)? "<unk>":vtypes[vp->v_type],
@


1.252
log
@move buf_rb_bufs from RB macros to RBT functions

i had to shuffle the order of some header bits cos RBT_PROTOTYPE
needs to see what RBT_HEAD produces.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.251 2016/09/15 02:00:16 dlg Exp $	*/
d379 1
a379 1
		RB_INIT(&vp->v_nc_tree);
@


1.251
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.250 2016/08/25 00:01:13 dlg Exp $	*/
d125 2
a126 2
static int rb_buf_compare(struct buf *b1, struct buf *b2);
RB_GENERATE(buf_rb_bufs, buf, b_rbbufs, rb_buf_compare);
d128 2
a129 2
static int
rb_buf_compare(struct buf *b1, struct buf *b2)
d378 1
a378 1
		RB_INIT(&vp->v_bufs_tree);
@


1.250
log
@pool_setipl

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.249 2016/07/22 09:54:09 kettenis Exp $	*/
d146 4
a149 6
	pool_init(&vnode_pool, sizeof(struct vnode), 0, 0, PR_WAITOK,
	    "vnodes", NULL);
	pool_setipl(&vnode_pool, IPL_NONE);
	pool_init(&uvm_vnode_pool, sizeof(struct uvm_vnode), 0, 0, PR_WAITOK,
	    "uvmvnodes", NULL);
	pool_setipl(&uvm_vnode_pool, IPL_NONE);
@


1.249
log
@Prevent NULL-pointer call for filesystems that don't provide vfs_sysctl
in their vfsops.

Issue reported by Tim Newsham.

ok claudio@@, natano@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.248 2016/06/19 11:54:33 natano Exp $	*/
d148 1
d151 1
@


1.248
log
@Remove the lockmgr() API. It is only used by filesystems, where it is a
trivial change to use rrw locks instead. All it needs is LK_* defines
for the RW_* flags.

tested by naddy and sthen on package building infrastructure
input and ok jmc mpi tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.247 2016/05/26 16:03:29 natano Exp $	*/
d1293 1
a1293 1
		if (vfsp == NULL)
@


1.247
log
@The doforce variable isn't modified anywhere. Also, the only filesystem
left using it is fuse. It has been removed from all other filesystems.

ok millert deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.246 2016/04/26 18:23:07 natano Exp $	*/
d960 1
a960 1
	VOP_LOCK(vp, LK_DRAIN, p);
@


1.246
log
@copy_statfs_info() is not only used by ufs, but by other filesystems too,
so make sure that all members of mp->mnt_stat.mount_info are copied.
ok stefan
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.245 2016/04/26 09:51:22 beck Exp $	*/
a86 1
int doforce = 1;		/* 1 => permit forcible unmounting */
@


1.245
log
@fix off by one in vfs_vnode_print - found by miod
ok deraadt@@, krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.244 2016/04/07 09:58:11 natano Exp $	*/
d2279 2
a2280 2
	memcpy(&sbp->mount_info.ufs_args, &mp->mnt_stat.mount_info.ufs_args,
	    sizeof(struct ufs_args));
@


1.244
log
@Share clone bitmap between aliased vnodes. This prevents duplicate clone
instance numbers being handed out for the same minor device.
ok mikeb
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.243 2016/04/05 19:26:15 natano Exp $	*/
d2166 2
a2167 2
	      vp->v_tag > nitems(vtags)? "<unk>":vtags[vp->v_tag], vp->v_tag,
	      vp->v_type > nitems(vtypes)? "<unk>":vtypes[vp->v_type],
@


1.243
log
@Increase size of the clone bitmap (revised diff after revert). I have
tested this with fuse _and_ drm on amd64 and macppc. Also tested with
cloning bpf (not in the tree) on macppc.

ok mikeb
"looks correct to me" millert

The original commit message is as follows:

Increase size of the clone bitmap. A limit of only 64 device clones
turned out to be too low for the upcoming work on cloning bpf. The new
limit is 1024 device clones. As part of the size increase, the bitmap
has been changed to be allocated separately to avoid bloating all device
nodes, as suggested by guenther, millert and deraadt.

ok millert mikeb
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.242 2016/04/01 11:51:55 mikeb Exp $	*/
d562 5
a566 2
			nvp->v_specbitmap = malloc(CLONE_MAPSZ, M_VNODE,
			    M_WAITOK | M_ZERO);
d1077 5
a1108 5
		}
		if (vp->v_type == VCHR &&
		    (cdevsw[major(vp->v_rdev)].d_flags & D_CLONE) &&
		    (minor(vp->v_rdev) >> CLONE_SHIFT == 0)) {
			free(vp->v_specbitmap, M_VNODE, CLONE_MAPSZ);
@


1.242
log
@Revert the clone bitmap enlargement change
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.240 2016/03/19 12:04:15 natano Exp $	*/
d558 7
a564 1
		memset(nvp->v_specbitmap, 0, sizeof(nvp->v_specbitmap));
d1101 5
@


1.241
log
@Increase size of the clone bitmap. A limit of only 64 device clones
turned out to be too low for the upcoming work on cloning bpf. The new
limit is 1024 device clones. As part of the size increase, the bitmap
has been changed to be allocated separately to avoid bloating all device
nodes, as suggested by guenther, millert and deraadt.

ok millert mikeb
@
text
@d558 1
a558 7
		nvp->v_specbitmap = NULL;
		if (nvp_rdev == VCHR &&
		    (cdevsw[major(nvp_rdev)].d_flags & D_CLONE) &&
		    minor(nvp_rdev) >> CLONE_SHIFT == 0) {
			nvp->v_specbitmap = malloc(CLONE_MAP_SZ, M_VNODE,
			    M_WAITOK | M_ZERO);
		}
a1094 5
		}
		if (vp->v_rdev == VCHR &&
		    (cdevsw[major(vp->v_rdev)].d_flags & D_CLONE) &&
		    minor(vp->v_rdev) >> CLONE_SHIFT == 0) {
			free(vp->v_specbitmap, M_VNODE, CLONE_MAP_SZ);
@


1.240
log
@Remove the unused flags argument from VOP_UNLOCK().

torture tested on amd64, i386 and macppc
ok beck mpi stefan
"the change looks right" deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.239 2016/03/14 23:08:06 krw Exp $	*/
d558 7
a564 1
		memset(nvp->v_specbitmap, 0, sizeof(nvp->v_specbitmap));
d1101 5
@


1.239
log
@Change a bunch of (<blah> *)0 to NULL.

ok beck@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.238 2015/12/05 10:11:53 tedu Exp $	*/
d579 1
a579 1
	VOP_UNLOCK(vp, 0, p);
d710 1
a710 1
		VOP_UNLOCK(vp, 0, p);
d977 1
a977 1
		VOP_UNLOCK(vp, 0, p);
@


1.238
log
@remove stale lint annotations
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.237 2015/11/16 18:23:50 deraadt Exp $	*/
d893 1
a893 1
			insmntque(vp, (struct mount *)0);
d1062 1
a1062 1
		insmntque(vp, (struct mount *)0);
d1253 1
a1253 1
				vprint((char *)0, vp);
d1602 1
a1602 1
		sys_sync(&proc0, (void *)0, (register_t *)0);
d1634 1
a1634 1
	sys_sync(p, (void *)0, (register_t *)0);
@


1.238.2.1
log
@backport 1.249 null pointer check:
Prevent NULL-pointer call for filesystems that don't provide vfs_sysctl
in their vfsops.

Issue reported by Tim Newsham.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.238 2015/12/05 10:11:53 tedu Exp $	*/
d1280 1
a1280 1
		if (vfsp == NULL || vfsp->vfc_vfsops->vfs_sysctl == NULL)
@


1.237
log
@In getdevvp() set the VISTTY flag on a vnode to indicate the underlying
device is a D_TTY device.  (Like spec_open, but this sets the flag to
satisfy pre-VOP_OPEN situations)
ok millert semarie tedu guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.236 2015/10/13 09:11:48 guenther Exp $	*/
a1421 1
/* ARGSUSED */
@


1.236
log
@Initialize va_filerev in vattr_null() to avoid leaking stack garbage;
problem pointed out by Martin Natano (natano (at) natano.net)

Also, stop chaining assignments (foo = bar = baz) in vattr_null().
The exact meaning of those depends on the order of the sizes-and-
signednesses of the lvalues, making them fragile: a statement here
mixed *six* types, but managed to get them in a safe order.  Delete
a 20+ year old XXX comment that was almost certainly bemoaning a bug
from when they were in an unsafe order.

ok deraadt@@ miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.235 2015/10/08 08:41:58 mpi Exp $	*/
d52 1
d504 2
@


1.235
log
@Use the radix API directly and get rid of the function pointers.  There
is no point in keeping an unused level of abstraction.

ok mikeb@@, claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.234 2015/10/07 11:57:44 mpi Exp $	*/
d299 11
a309 1
	/* XXX These next two used to be one line, but for a GCC bug. */
d311 10
d322 1
a322 7
	vap->va_mode = vap->va_nlink = vap->va_uid = vap->va_gid =
		vap->va_fsid = vap->va_fileid =
		vap->va_blocksize = vap->va_rdev =
		vap->va_atime.tv_sec = vap->va_atime.tv_nsec =
		vap->va_mtime.tv_sec = vap->va_mtime.tv_nsec =
		vap->va_ctime.tv_sec = vap->va_ctime.tv_nsec =
		vap->va_flags = vap->va_gen = VNOVAL;
@


1.234
log
@rn_inithead() offset argument is now specified in byte, missed in previous.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.233 2015/09/04 08:43:39 mpi Exp $	*/
d1392 1
a1392 2
	rn = (*rnh->rnh_addaddr)((caddr_t)saddr, (caddr_t)smask, rnh,
		np->netc_rnodes, 0);
d1411 1
a1411 1
	(*rnh->rnh_deladdr)(rn->rn_key, rn->rn_mask, rnh, NULL);
d1425 1
a1425 1
		(*rnh->rnh_walktree)(rnh, vfs_free_netcred, rnh);
d1470 2
a1471 5
			if (rnh != NULL) {
				np = (struct netcred *)
					(*rnh->rnh_matchaddr)((caddr_t)saddr,
					    rnh);
			}
@


1.233
log
@Make every subsystem using a radix tree call rn_init() and pass the
length of the key as argument.

This way every consumer of the radix tree has a chance to explicitly
initialize the shared data structures and no longer rely on another
subsystem to do the initialization.

As a bonus ``dom_maxrtkey'' is no longer used an die.

ART kernels should now be fully usable because pf(4) and IPSEC properly
initialized the radix tree.

ok chris@@, reyk@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.232 2015/07/16 18:17:27 claudio Exp $	*/
d1381 1
a1381 1
			    offsetof(struct sockaddr_in, sin_addr) * 8)) {
@


1.232
log
@Fix rn_match and there for the expoerted lookup functions in radix.c
to never return the internal RNF_ROOT nodes. This removes the checks
in the callee to verify that not an RNF_ROOT node was returned.
OK mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.231 2015/05/12 09:30:35 mikeb Exp $	*/
d157 2
@


1.232.4.1
log
@backport 1.249 null pointer check:
Prevent NULL-pointer call for filesystems that don't provide vfs_sysctl
in their vfsops.

Issue reported by Tim Newsham.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.232 2015/07/16 18:17:27 claudio Exp $	*/
d1261 1
a1261 1
		if (vfsp == NULL || vfsp->vfc_vfsops->vfs_sysctl == NULL)
@


1.231
log
@Drop and reacquire the kernel lock in the vfs_shutdown and "cold"
portions of msleep and tsleep to give interrupts a chance to run
on other CPUs.

Tweak and OK kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.230 2015/03/14 03:38:51 jsg Exp $	*/
a1472 2
				if (np && np->netc_rnodes->rn_flags & RNF_ROOT)
					np = NULL;
@


1.230
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.229 2015/03/02 20:46:50 guenther Exp $	*/
d1617 3
d1654 6
d1661 4
@


1.229
log
@Return EINVAL if the creds supplied for NFS export have a cr_ngroups less
than zero or greater than NGROUPS_MAX

Fixes panic seen by henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.228 2015/01/09 05:01:56 tedu Exp $	*/
a2089 1
#include <ddb/db_output.h>
@


1.228
log
@rename desiredvnodes to initialvnodes. less of a lie. ok beck deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.227 2014/12/19 05:59:21 tedu Exp $	*/
d1346 3
d1371 3
a1397 2
	/* fill in the kernel's ucred from userspace's xucred */
	crfromxucred(&np->netc_anon, &argp->ex_anon);
@


1.227
log
@start retiring the nointr allocator. specify PR_WAITOK as a flag as a
marker for which pools are not interrupt safe. ok dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.226 2014/12/17 19:42:15 tedu Exp $	*/
d145 1
a145 1
	maxvnodes = 2 * desiredvnodes;
@


1.226
log
@remove lock.h from uvm_extern.h. another holdover from the simpletonlock
era. fix uvm including c files to include lock.h or atomic.h as necessary.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.225 2014/12/16 18:30:04 tedu Exp $	*/
d146 4
a149 4
	pool_init(&vnode_pool, sizeof(struct vnode), 0, 0, 0, "vnodes",
	    &pool_allocator_nointr);
	pool_init(&uvm_vnode_pool, sizeof(struct uvm_vnode), 0, 0, 0, "uvmvnodes",
	    &pool_allocator_nointr);
@


1.225
log
@primary change: move uvm_vnode out of vnode, keeping only a pointer.
objective: vnode.h doesn't include uvm_extern.h anymore.
followup changes: include uvm_extern.h or lock.h where necessary.
ok and help from deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.224 2014/12/10 02:44:47 tedu Exp $	*/
d53 1
@


1.224
log
@convert bcopy to memcpy. ok millert
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.223 2014/11/21 07:14:17 tedu Exp $	*/
d68 3
d122 1
d147 2
d359 2
@


1.223
log
@simple lock is long dead
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.222 2014/11/19 18:04:54 tedu Exp $	*/
d1276 1
a1276 1
		bcopy(vfsp, tmpvfsp, sizeof(*tmpvfsp));
d2225 4
a2228 4
	bcopy(mp->mnt_stat.f_mntonname, sbp->f_mntonname, MNAMELEN);
	bcopy(mp->mnt_stat.f_mntfromname, sbp->f_mntfromname, MNAMELEN);
	bcopy(mp->mnt_stat.f_mntfromspec, sbp->f_mntfromspec, MNAMELEN);
	bcopy(&mp->mnt_stat.mount_info.ufs_args, &sbp->mount_info.ufs_args,
@


1.222
log
@delete the KERN_VNODE sysctl. it fails to provide any isolation from the
kernel struct vnode defintion, and the only consumer (pstat) still needs
kvm to read much of the required information. no great loss to always use
kvm until there's a better replacement interface.
ok deraadt millert uebayasi
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.221 2014/11/14 23:26:48 tedu Exp $	*/
a408 1
	simple_lock_init(&vp->v_uvm.u_obj.vmobjlock);
@


1.221
log
@prefer sizeof(*ptr) to sizeof(struct) for malloc and free
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.220 2014/11/03 03:08:00 deraadt Exp $	*/
a1291 64
}

int kinfo_vdebug = 1;
#define KINFO_VNODESLOP	10
/*
 * Dump vnode list (via sysctl).
 * Copyout address of vnode followed by vnode.
 */
/* ARGSUSED */
int
sysctl_vnode(char *where, size_t *sizep, struct proc *p)
{
	struct mount *mp, *nmp;
	struct vnode *vp, *nvp;
	char *bp = where, *savebp;
	char *ewhere;
	int error;

	if (where == NULL) {
		*sizep = (numvnodes + KINFO_VNODESLOP) * sizeof(struct e_vnode);
		return (0);
	}
	ewhere = where + *sizep;

	TAILQ_FOREACH_SAFE(mp, &mountlist, mnt_list, nmp) {
		if (vfs_busy(mp, VB_READ|VB_NOWAIT))
			continue;
		savebp = bp;
again:
		LIST_FOREACH_SAFE(vp, &mp->mnt_vnodelist, v_mntvnodes, nvp) {
			/*
			 * Check that the vp is still associated with
			 * this filesystem.  RACE: could have been
			 * recycled onto the same filesystem.
			 */
			if (vp->v_mount != mp) {
				if (kinfo_vdebug)
					printf("kinfo: vp changed\n");
				bp = savebp;
				goto again;
			}
			if (bp + sizeof(struct e_vnode) > ewhere) {
				*sizep = bp - where;
				vfs_unbusy(mp);
				return (ENOMEM);
			}
			if ((error = copyout(&vp,
			    &((struct e_vnode *)bp)->vptr,
			    sizeof(struct vnode *))) ||
			   (error = copyout(vp,
			    &((struct e_vnode *)bp)->vnode,
			    sizeof(struct vnode)))) {
				vfs_unbusy(mp);
				return (error);
			}
			bp += sizeof(struct e_vnode);
		}

		vfs_unbusy(mp);
	}

	*sizep = bp - where;

	return (0);
@


1.220
log
@pass size argument to free()
ok doug tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.219 2014/09/13 16:06:37 doug Exp $	*/
d219 1
a219 1
	mp = malloc(sizeof(struct mount), M_MOUNT, M_WAITOK|M_ZERO);
@


1.219
log
@Replace all queue *_END macro calls except CIRCLEQ_END with NULL.

CIRCLEQ_* is deprecated and not called in the tree.  The other queue types
have *_END macros which were added for symmetry with CIRCLEQ_END.  They are
defined as NULL.  There's no reason to keep the other *_END macro calls.

ok millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.218 2014/07/13 15:00:40 tedu Exp $	*/
d1393 1
a1393 1
	int i;
d1408 2
a1409 2
	i = sizeof(struct netcred) + argp->ex_addrlen + argp->ex_masklen;
	np = (struct netcred *)malloc(i, M_NETADDR, M_WAITOK|M_ZERO);
d1452 1
a1452 1
	free(np, M_NETADDR, 0);
@


1.218
log
@pass the size to free in some of the obvious cases
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.217 2014/07/12 18:43:32 tedu Exp $	*/
d1890 1
a1890 2
	for (bp = LIST_FIRST(&vp->v_dirtyblkhd);
	    bp != LIST_END(&vp->v_dirtyblkhd); bp = nbp) {
@


1.217
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.216 2014/07/10 12:21:08 mpi Exp $	*/
d1069 1
a1069 1
		free(vp->v_specinfo, M_VNODE, 0);
d1284 1
a1284 1
		free(tmpvfsp, M_TEMP, 0);
@


1.216
log
@Stop using a shutdown hook for softraid(4) and explicitly shutdown
the disciplines right after vfs_shutdown().

This change is required in order to be able to set `cold' to 1 before
traversing the device (mainbus) tree for DVACT_POWERDOWN when halting
a machine.  Yes, this is ugly because sr_shutdown() needs to sleep.  But
at least it is obvious and hopefully somebody will be ofended and fix
it.

In order to properly flush the cache of the disks under softraid0,
sr_shutdown() now propagates DVACT_POWERDOWN for this particular subtree
of devices which are not under mainbus.  As a side effect sd(4) shutdown
hook should no longer be necessary.

Tested by stsp@@ and Jean-Philippe Ouellet.

ok deraadt@@, stsp@@, jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.215 2014/07/08 17:19:25 deraadt Exp $	*/
d1069 1
a1069 1
		free(vp->v_specinfo, M_VNODE);
d1284 1
a1284 1
		free(tmpvfsp, M_TEMP);
d1452 1
a1452 1
	free(np, M_NETADDR);
d1463 1
a1463 1
	free(rn, M_NETADDR);
d1477 1
a1477 1
		free(rnh, M_RTABLE);
@


1.215
log
@decouple struct uvmexp into a new file, so that uvm_extern.h and sysctl.h
don't need to be married.
ok guenther miod beck jsing kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.214 2014/06/04 07:58:14 claudio Exp $	*/
d68 4
d1652 4
@


1.214
log
@While it may be smart to use the radix tree for exports it is not OK to
use the domain specific tree initialisation method for this since that one
is multipath enabled and assumes that the radix node is part of a struct
rtentry. This code uses a different struct and so the multipath modifies
wrong fields and breaks stuff in mysterious ways.
Since we only support AF_INET here anyway simplify the code and only have
one radix_node_head pointer instead of AF_MAX ones.
Fixes NFS server issues reported by rpe@@, OK rpe@@, guenther@@, sthen@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.213 2014/04/10 13:48:24 tedu Exp $	*/
d47 1
a66 3

#include <uvm/uvm_extern.h>
#include <sys/sysctl.h>
@


1.213
log
@pull the bufcache freelist code out into separate functions to allow new
algorithms to be tested. in the process, drop support for unused B_AGE and
b_synctime options.
previous versions ok beck deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.212 2014/03/24 00:19:48 guenther Exp $	*/
a58 1
#include <sys/domain.h>
d65 2
a1393 1
	struct domain *dom;
d1423 12
a1434 1
	if (i < 0 || i > AF_MAX) {
a1437 16
	if ((rnh = nep->ne_rtable[i]) == 0) {
		/*
		 * Seems silly to initialize every AF when most are not
		 * used, do so on demand here
		 */
		for (dom = domains; dom; dom = dom->dom_next)
			if (dom->dom_family == i && dom->dom_rtattach) {
				dom->dom_rtattach((void **)&nep->ne_rtable[i],
					dom->dom_rtoffset);
				break;
			}
		if ((rnh = nep->ne_rtable[i]) == 0) {
			error = ENOBUFS;
			goto out;
		}
	}
a1470 1
	int i;
d1473 5
a1477 6
	for (i = 0; i <= AF_MAX; i++)
		if ((rnh = nep->ne_rtable[i]) != NULL) {
			(*rnh->rnh_walktree)(rnh, vfs_free_netcred, rnh);
			free(rnh, M_RTABLE);
			nep->ne_rtable[i] = 0;
		}
d1511 8
a1518 1
			rnh = nep->ne_rtable[saddr->sa_family];
@


1.212
log
@Split the API: struct ucred remains the kernel internal structure while
struct xucred becomes the structure for syscalls (mount(2) and nfssvc(2)).

ok deraadt@@ beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.211 2014/01/21 01:48:45 tedu Exp $	*/
d2150 1
a2150 1
	(*pr)("  bufsize 0x%lx bcount 0x%lx resid 0x%lx sync 0x%llx\n"
d2153 1
a2153 1
	    (long long)bp->b_synctime, bp->b_data, bp->b_saveaddr,
@


1.211
log
@bzero -> memset
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.210 2013/12/01 16:40:56 krw Exp $	*/
a1399 3
		np->netc_exflags = argp->ex_flags;
		np->netc_anon = argp->ex_anon;
		np->netc_anon.cr_ref = 1;
d1401 1
a1401 1
		return (0);
d1449 1
d1451 2
a1452 2
	np->netc_anon = argp->ex_anon;
	np->netc_anon.cr_ref = 1;
@


1.210
log
@Change 'mountlist' from CIRCLEQ to TAILQ. Be paranoid and
use TAILQ_*_SAFE more than might be needed.

Bulk ports build by sthen@@ showed nobody sticking their fingers
so deep into the kernel.

Feedback and suggestions from millert@@. ok jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.209 2013/11/27 15:50:52 jsing Exp $	*/
d528 1
a528 1
		bzero(nvp->v_specbitmap, sizeof(nvp->v_specbitmap));
@


1.209
log
@Defer the v_type initialisation until after the vnode has been purged from
the namecache. Changing the v_type between cache_enter() and cache_purge()
results in bad things happening.

ok beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.208 2013/10/02 21:29:21 sf Exp $	*/
d142 1
a142 1
	CIRCLEQ_INIT(&mountlist);
d241 1
a241 1
	CIRCLEQ_FOREACH(mp, &mountlist, mnt_list) {
d270 1
a270 1
	if (!CIRCLEQ_EMPTY(&mountlist)) {
d1218 2
a1219 4
	for (mp = CIRCLEQ_FIRST(&mountlist); mp != CIRCLEQ_END(&mountlist);
	    mp = nmp) {
		if (vfs_busy(mp, VB_READ|VB_NOWAIT)) {
			nmp = CIRCLEQ_NEXT(mp, mnt_list);
a1220 1
		}
a1224 1
		nmp = CIRCLEQ_NEXT(mp, mnt_list);
d1313 2
a1314 4
	for (mp = CIRCLEQ_FIRST(&mountlist); mp != CIRCLEQ_END(&mountlist);
	    mp = nmp) {
		if (vfs_busy(mp, VB_READ|VB_NOWAIT)) {
			nmp = CIRCLEQ_NEXT(mp, mnt_list);
a1315 1
		}
d1318 1
a1318 2
		for (vp = LIST_FIRST(&mp->mnt_vnodelist); vp != NULL;
		    vp = nvp) {
a1329 1
			nvp = LIST_NEXT(vp, v_mntvnodes);
a1346 1
		nmp = CIRCLEQ_NEXT(mp, mnt_list);
d1605 1
a1605 3
	for (mp = CIRCLEQ_LAST(&mountlist); mp != CIRCLEQ_END(&mountlist);
	    mp = nmp) {
		nmp = CIRCLEQ_PREV(mp, mnt_list);
@


1.208
log
@format string fix: b_flags is long
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.207 2013/10/01 20:15:56 sf Exp $	*/
d398 1
a399 1
	cache_purge(vp);
@


1.207
log
@Format string fixes: Cast time_t to long long

and mnt_stat.f_ctime is long long, too
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.206 2013/08/08 23:25:06 syl Exp $	*/
d2160 1
a2160 1
	      "  proc %p error %d flags %b\n",
@


1.206
log
@Uncomment kprintf format attributes for sys/kern

tested on vax (gcc3) ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.205 2013/07/30 17:07:56 beck Exp $	*/
d2164 1
a2164 1
	(*pr)("  bufsize 0x%lx bcount 0x%lx resid 0x%lx sync 0x%x\n"
d2166 3
a2168 2
	    bp->b_bufsize, bp->b_bcount, (long)bp->b_resid, bp->b_synctime,
	    bp->b_data, bp->b_saveaddr, LIST_FIRST(&bp->b_dep), bp->b_iodone);
d2239 1
a2239 1
	(*pr)("  f_fsidx {0x%x, 0x%x} owner %u ctime 0x%x\n",
@


1.205
log
@The previous change was made while chasing nfs performance issues
on Theo's servers - however this was in the context of the buffer flipper
changes and this is now suspect in a continues performance issue with NFS
so back it out for now
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.203 2013/04/15 15:32:19 jsing Exp $	*/
d2155 1
a2155 1
    int (*pr)(const char *, ...) /* __attribute__((__format__(__kprintf__,1,2))) */)
d2183 1
a2183 1
    int (*pr)(const char *, ...) /* __attribute__((__format__(__kprintf__,1,2))) */)
d2217 1
a2217 1
    int (*pr)(const char *, ...) /* __attribute__((__format__(__kprintf__,1,2))) */)
@


1.204
log
@Manipulating buffers after sleeping is dangerous. Instead of attempting
to cheat and VOP_BWRITE a buffer, restart the vinvalbuf if we have to wait
for a busy buffer to complete
ok tedu@@ guenther@@
@
text
@a1822 1
loop:
d1837 1
a1859 1
				splx(s);
d1861 1
d1864 12
a1877 1
			bremfree(bp);
@


1.203
log
@Add an f_mntfromspec member to struct statfs, which specifies the name of
the special provided when the mount was requested. This may be the same as
the special that was actually used for the mount (e.g. in the case of a
device node) or it may be different (e.g. in the case of a DUID).

Whilst here, change f_ctime to a 64 bit type and remove the pointless
f_spare members.

Compatibility goo courtesy of guenther@@

ok krw@@ millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.202 2013/02/17 17:39:29 miod Exp $	*/
d1823 1
a1837 1
loop:
d1860 1
a1861 1
					splx(s);
d1864 1
a1864 1
				break;
a1866 11
			/*
			 * XXX Since there are no node locks for NFS, I believe
			 * there is a slight chance that a delayed write will
			 * occur while sleeping just above, so check for it.
			 */
			if ((bp->b_flags & B_DELWRI) && (flags & V_SAVE)) {
				buf_acquire(bp);
				splx(s);
				(void) VOP_BWRITE(bp);
				goto loop;
			}
@


1.202
log
@Comment out recently added __attribute__((__format__(__kprintf__))) annotations
in MI code; gcc 2.95 does not accept such annotation for function pointer
declarations, only function prototypes.
To be uncommented once gcc 2.95 bites the dust.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.201 2013/02/09 20:56:35 miod Exp $	*/
d227 2
a228 1
	(void)copystr(devname, mp->mnt_stat.f_mntfromname, MNAMELEN - 1, 0);
d2248 1
a2248 1
	(*pr)("  fstype \"%s\" mnton \"%s\" mntfrom \"%s\"\n",
d2250 1
a2250 1
	    mp->mnt_stat.f_mntfromname);
d2300 1
@


1.201
log
@Add explicit __attribute__ ((__format__(__kprintf__)))) to the functions and
function pointer arguments which are {used as,} wrappers around the kernel
printf function.
No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.200 2012/11/17 23:08:22 beck Exp $	*/
d2154 1
a2154 1
    int (*pr)(const char *, ...) __attribute__((__format__(__kprintf__,1,2))))
d2182 1
a2182 1
    int (*pr)(const char *, ...) __attribute__((__format__(__kprintf__,1,2))))
d2216 1
a2216 1
    int (*pr)(const char *, ...) __attribute__((__format__(__kprintf__,1,2))))
@


1.200
log
@
 Don't map a buffer (and potentially sleep) when invalidating it in vinvalbuf.
This fixes a problem where we could sleep for kva and then our pointers
would not be valid on the next pass through the loop. We do this
by adding buf_acquire_nomap() - which can be used to busy up the buffer
without changing its mapped or unmapped state. We do not need to have
the buffer mapped to invalidate it, so it is sufficient to acquire it
for that. In the case where we write the buffer, we do map the buffer, and
potentially sleep.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.199 2012/10/01 00:08:43 guenther Exp $	*/
d2153 2
a2154 1
vfs_buf_print(void *b, int full, int (*pr)(const char *, ...))
d2181 2
a2182 1
vfs_vnode_print(void *v, int full, int (*pr)(const char *, ...))
d2191 1
a2191 1
	(*pr)("data %p usecount %d writecount %ld holdcnt %ld numoutput %d\n",
d2215 2
a2216 1
vfs_mount_print(struct mount *mp, int full, int (*pr)(const char *, ...))
@


1.199
log
@Make groupmember() check the effective gid too, so that the checks are
consistent when the effective gid isn't also a supplementary group.

ok beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.198 2012/09/19 00:53:13 guenther Exp $	*/
a1865 1
			buf_acquire(bp);
d1872 1
d1877 1
@


1.198
log
@vhold() and vdrop() are prototyped in vnode.h, so don't repeat them here

ok beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.197 2012/07/16 15:31:17 deraadt Exp $	*/
d1581 1
a1581 1
	if (cred->cr_gid == gid || groupmember(gid, cred)) {
@


1.197
log
@oops, need sys/acct.h too
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.196 2012/07/16 15:20:39 deraadt Exp $	*/
a97 2
void	vhold(struct vnode *);
void	vdrop(struct vnode *);
@


1.196
log
@Put acct_shutdown() proto in a better place
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.195 2011/07/04 20:35:35 deraadt Exp $	*/
d53 1
@


1.195
log
@move the specfs code to a place people can see it; ok guenther thib krw
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.194 2011/07/02 15:52:25 thib Exp $	*/
a1643 2
	extern void acct_shutdown(void);

@


1.194
log
@rename VFSDEBUG to VFLCKDEBUG;

prompted by tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.193 2010/12/21 20:14:43 thib Exp $	*/
d63 1
a66 2

#include <miscfs/specfs/specdev.h>
@


1.193
log
@Bring back the "End the VOP experiment." diff, naddy's issues where
unrelated, and his alpha is much happier now.

OK deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.192 2010/12/06 18:44:49 jasper Exp $	*/
d973 1
a973 1
#ifdef VFSDEBUG
d1821 1
a1821 1
#ifdef VFSDEBUG
@


1.192
log
@- drop NENTS(), which was yet another copy of nitems().
no binary change


ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.191 2010/09/10 16:34:08 thib Exp $	*/
a303 1
extern int (**dead_vnodeop_p)(void *);
d310 1
a310 1
getnewvnode(enum vtagtype tag, struct mount *mp, int (**vops)(void *),
d466 1
a466 1
	error = getnewvnode(VT_NON, NULL, spec_vnodeop_p, &nvp);
d863 1
a863 1
			vp->v_op = spec_vnodeop_p;
d969 1
a969 1
	vp->v_op = dead_vnodeop_p;
@


1.191
log
@Backout the VOP diff until the issues naddy was seeing on alpha (gcc3)
have been resolved.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.190 2010/09/06 23:44:10 thib Exp $	*/
a2187 1
#define	NENTS(n)	(sizeof n / sizeof(n[0]))
d2189 2
a2190 2
	      vp->v_tag > NENTS(vtags)? "<unk>":vtags[vp->v_tag], vp->v_tag,
	      vp->v_type > NENTS(vtypes)? "<unk>":vtypes[vp->v_type],
@


1.190
log
@End the VOP experiment. Instead of the ridicolusly complicated operation
vector setup that has questionable features (that have, as far as I can
tell never been used in practice, atleast not in OpenBSD), remove all
the gunk and favor a simple struct full of function pointers that get
set directly by each of the filesystems.

Removes gobs of ugly code and makes things simpler by a magnitude.

The only downside of this is that we loose the vnoperate feature so
the spec/fifo operations of the filesystems need to be kept in sync
with specfs and fifofs, this is no big deal as the API it self is pretty
static.

Many thanks to armani@@ who pulled an earlier version of this diff to
current after c2k10 and Gabriel Kihlman on tech@@ for testing.

Liked by many. "come on, find your balls" deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.189 2010/08/12 15:00:17 oga Exp $	*/
d304 1
d311 1
a311 1
getnewvnode(enum vtagtype tag, struct mount *mp, struct vops *vops,
d467 1
a467 1
	error = getnewvnode(VT_NON, NULL, &spec_vops, &nvp);
d864 1
a864 1
			vp->v_op = &spec_vops;
d970 1
a970 1
	vp->v_op = &dead_vops;
@


1.189
log
@Nuke extra (typoed) extern declaration and a spare newline from the last
commit.

"fix it -- free commit" beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.188 2010/08/11 14:35:34 beck Exp $	*/
a303 1
extern int (**dead_vnodeop_p)(void *);
d310 1
a310 1
getnewvnode(enum vtagtype tag, struct mount *mp, int (**vops)(void *),
d466 1
a466 1
	error = getnewvnode(VT_NON, NULL, spec_vnodeop_p, &nvp);
d863 1
a863 1
			vp->v_op = spec_vnodeop_p;
d969 1
a969 1
	vp->v_op = dead_vnodeop_p;
@


1.188
log
@Make the number of vnodes to correspond to the number of buffers in
buffer cache - we grow them dynamically, but do not attempt to shrink
them if the buffer cache shrinks after growing.

Tested by very many for a long time.

ok oga@@ todd@@ phessler@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.187 2010/06/29 04:09:32 tedu Exp $	*/
a110 1
extern struct bcachestats bcasts;
a111 1

@


1.187
log
@makefstype was only used in ported from freebsd filesystems.  fix them
and remove the function.  ok thib
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.186 2010/06/28 18:50:36 claudio Exp $	*/
d111 1
d114 1
d141 1
a141 1
	maxvnodes = desiredvnodes;
d323 7
d345 1
a345 1
	if (numvnodes > 2 * maxvnodes)
d769 1
a769 1
		panic("vdrop: zero holdcnt"); 
d855 1
a855 1
		
@


1.186
log
@Add the rtable id as an argument to rn_walktree(). Functions like
rt_if_remove_rtdelete() need to know the table id to be able to correctly
remove nodes.
Problem found by Andrea Parazzini and analyzed by Martin Pelikn.
OK henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.185 2010/05/06 06:53:09 mpf Exp $	*/
a277 17
}

/*
 * Make a 'unique' number from a mount type name.
 * Note that this is no longer used for ffs which
 * now has an on-disk filesystem id.
 */
long
makefstype(char *type)
{
	long rv;

	for (rv = 0; *type; type++) {
		rv <<= 2;
		rv ^= *type;
	}
	return rv;
@


1.185
log
@Fix favail format string.
From mickey.
OK thib, otto.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.184 2009/12/17 16:44:12 oga Exp $	*/
d106 1
a106 1
int vfs_free_netcred(struct radix_node *, void *);
d1485 1
a1485 1
vfs_free_netcred(struct radix_node *rn, void *w)
@


1.184
log
@if anyone vref()s a VNON vnode, panic. This should not happen.

Written while trying to debug the nfs_inactive panics. Turns out it
never got hit, but it's a useful check to have.

ok beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.183 2009/08/17 13:11:58 jasper Exp $	*/
d2246 1
a2246 1
	(*pr)("  files %llu ffiles %llu favail $lld\n", mp->mnt_stat.f_files,
@


1.183
log
@dd 'show all bufs' to show all the buffers in the system

ok beck@@ thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.182 2009/08/13 13:49:20 thib Exp $	*/
d633 2
@


1.182
log
@add a show all vnodes command, use dlg's nice pool_walk() to accomplish
this.

ok beck@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.180 2009/08/02 16:28:40 beck Exp $	*/
d2165 1
a2165 1
vfs_buf_print(struct buf *bp, int full, int (*pr)(const char *, ...))
d2167 1
@


1.181
log
@Namecache revamp.

This eliminates the large single namecache hash table, and implements
the name cache as a global lru of entires, and a redblack tree in each
vnode. It makes cache_purge actually purge the namecache entries associated
with a vnode when a vnode is recycled (very important for later on actually being
able to resize the vnode pool)

This commit does #if 0 out a bunch of procmap code that was
already broken before this change, but needs to be redone completely.

Tested by many, including in thib's nfs test setup.

ok oga@@,art@@,thib@@,miod@@
@
text
@d2191 1
a2191 1
vfs_vnode_print(struct vnode *vp, int full, int (*pr)(const char *, ...))
d2193 1
a2310 1

@


1.180
log
@
Dynamic buffer cache support - a re-commit of what was backed out
after c2k9

allows buffer cache to be extended and grow/shrink dynamically

tested by many, ok oga@@, "why not just commit it" deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.179 2009/06/25 15:49:26 thib Exp $	*/
d363 2
@


1.179
log
@backout the buf_acquire() does the bremfree() since all callers
where doing bremfree() befure calling buf_acquire().

This is causing us headache pinning down a bug that showed up
when deraadt@@ too cvs to current, and will have to be done
anyway as a preperation for backouts.

OK deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.177 2009/06/06 18:06:22 art Exp $	*/
d62 1
d119 13
d362 1
@


1.178
log
@Back out all the buffer cache changes I committed during c2k9. This reverts three
commits:

1) The sysctl allowing bufcachepercent to be changed at boot time.
2) The change moving the buffer cache hash chains to a red-black tree
3) The dynamic buffer cache (Which depended on the earlier too).

ok on the backout from marco and todd
@
text
@d1690 1
d1861 1
d1899 1
@


1.177
log
@All caller of buf_acquire were doing bremfree before the call.
Just put it in the buf_acquire function.
oga@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.176 2009/06/03 04:30:57 beck Exp $	*/
a61 1
#include <sys/tree.h>
a117 13
static int rb_buf_compare(struct buf *b1, struct buf *b2);
RB_GENERATE(buf_rb_bufs, buf, b_rbbufs, rb_buf_compare);

static int
rb_buf_compare(struct buf *b1, struct buf *b2)
{
	if (b1->b_lblkno < b2->b_lblkno)
		return(-1);
	if (b1->b_lblkno > b2->b_lblkno)
		return(1);
	return(0);
}

a347 1
		RB_INIT(&vp->v_bufs_tree);
@


1.176
log
@Change bufhash from the old grotty hash table to red-black trees hanging
off the vnode.
ok art@@, oga@@, miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.175 2008/11/10 11:53:16 pedro Exp $	*/
a1704 1
				bremfree(bp);
a1874 1
			bremfree(bp);
a1911 1
		bremfree(bp);
@


1.175
log
@Fix typo in comment, okay jmc@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.174 2008/11/01 20:34:09 deraadt Exp $	*/
d62 1
d119 13
d362 1
@


1.174
log
@change vrele() to return an int.  if it returns 0, it can gaurantee that
it did not sleep.  this is used to avoid checkdirs() to avoid having
to restart the allproc walk every time through
idea from tedu, ok thib pedro
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.173 2008/07/05 12:48:03 thib Exp $	*/
d1663 1
a1663 1
 * assumtions: called w/ scheduler disabled and physical io enabled
@


1.173
log
@re-introduce vdrop() to signal a lost intrest in a vnode;

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.172 2008/06/14 10:55:21 mk Exp $	*/
d693 1
d695 1
a695 1
void
d712 1
a712 1
		return;
d726 1
a726 1
		return;
d733 1
@


1.172
log
@A bunch of pool_get() + bzero() -> pool_get(..., .. | PR_ZERO)
conversions that should shave a few bytes off the kernel.

ok henning, krw, jsing, oga, miod, and thib (``even though i usually prefer
FOO|BAR''; thanks for looking.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.171 2008/06/13 03:45:39 beck Exp $	*/
d97 2
d734 1
a734 5
void vhold(struct vnode *vp);

/*
 * Page or buffer structure gets a reference.
 */
d750 22
a1281 1

d1974 1
a1974 7
	bp->b_vp = (struct vnode *) 0;

#ifdef DIAGNOSTIC
	if (vp->v_holdcnt == 0)
		panic("brelvp: holdcnt");
#endif
	vp->v_holdcnt--;
d1976 1
a1976 9
	/*
	 * If it is on the holdlist and the hold count drops to
	 * zero, move it to the free list.
	 */
	if ((vp->v_bioflag & VBIOONFREELIST) &&
	    vp->v_holdcnt == 0 && vp->v_usecount == 0) {
		TAILQ_REMOVE(&vnode_hold_list, vp, v_freelist);
		TAILQ_INSERT_TAIL(&vnode_free_list, vp, v_freelist);
	}
@


1.171
log
@back out stupid vnode change that was unintentionally included
with biomem and art has no idea how it got there.
ok art@@ thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.170 2008/06/12 06:58:39 deraadt Exp $	*/
d345 1
a345 2
		vp = pool_get(&vnode_pool, PR_WAITOK);
		bzero((char *)vp, sizeof *vp);
@


1.170
log
@Bring biomem diff back into the tree after the nfs_bio.c fix went in.
ok thib beck art
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.168 2008/06/10 20:14:36 beck Exp $	*/
d123 1
a123 1
	maxvnodes = bufpages;
@


1.169
log
@back out biomem diff since it is not right yet.  Doing very large
file copies to nfsv2 causes the system to eventually peg the console.
On the console ^T indicates that the load is increasing rapidly, ddb
indicates many calls to getbuf, there is some very slow nfs traffic
making none (or extremely slow) progress.  Eventually some machines
seize up entirely.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.167 2008/06/09 23:38:37 millert Exp $	*/
d123 1
a123 1
	maxvnodes = desiredvnodes;
d1259 5
a1264 1

d1671 1
a1671 1
				bp->b_flags |= B_BUSY;
d1842 1
a1842 1
			bp->b_flags |= B_BUSY;
d1880 1
a1880 1
		bp->b_flags |= B_BUSY;
@


1.168
log
@
Buffer cache revamp

1) remove multiple size queues, introduced as a stopgap.
2) decouple pages containing data from their mappings
3) only keep buffers mapped when they actually have to be mapped
  (right now, this is when buffers are B_BUSY)
4) New functions to make a buffer busy, and release the busy flag
   (buf_acquire and buf_release)
5) Move high/low water marks and statistics counters into a structure
6) Add a sysctl to retrieve buffer cache statistics

Tested in several variants and beat upon by bob and art for a year. run
accidentally on henning's nfs server for a few months...

ok deraadt@@, krw@@, art@@ - who promises to be around to deal with any fallout
@
text
@d123 1
a123 1
	maxvnodes = bufpages;
d1259 1
a1259 4
	case VFS_BCACHESTAT:	/* buffer cache statistics */
		ret = sysctl_rdstruct(oldp, oldlenp, newp, &bcstats,
		    sizeof(struct bcachestats));
		return(ret);
a1260 1
	}
d1667 1
a1667 1
				buf_acquire(bp);
d1838 1
a1838 1
			buf_acquire(bp);
d1876 1
a1876 1
		buf_acquire(bp);
@


1.167
log
@Update access(2) to have modern semantics with respect to X_OK and
the superuser.  access(2) will now only indicate success for X_OK on
non-directories if there is at least one execute bit set on the file.
OK deraadt@@ thib@@ otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.166 2008/05/07 14:08:37 thib Exp $	*/
d123 1
a123 1
	maxvnodes = desiredvnodes;
d1259 5
a1264 1

d1671 1
a1671 1
				bp->b_flags |= B_BUSY;
d1842 1
a1842 1
			bp->b_flags |= B_BUSY;
d1880 1
a1880 1
		bp->b_flags |= B_BUSY;
@


1.166
log
@remove the vfc_mountroot member from vfsconf and
do appropriate cleanup;

OK deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.165 2008/05/07 05:14:21 claudio Exp $	*/
d1523 2
a1524 2
vaccess(mode_t file_mode, uid_t uid, gid_t gid, mode_t acc_mode,
    struct ucred *cred)
d1528 6
a1533 2
	/* User id 0 always gets access. */
	if (cred->cr_uid == 0)
d1535 1
@


1.165
log
@Implement routing priorities. Every route inserted has a priority assigned
and the one route with the lowest number wins. This will be used by the
routing daemons to resolve the synchronisations issue in case of conflicts.
The nasty bits of this are in the multipath code. If no priority is specified
the kernel will choose an appropriate priority.

Looked at by a few people at n2k8 code is much older
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.164 2008/05/06 17:19:40 thib Exp $	*/
a1251 1
		tmpvfsp->vfc_mountroot = NULL;
@


1.164
log
@retire vfs_mountroot();

setroot() is now (and has been) responsible for setting
the mountroot function pointer "to the right thing", or
failing todo that, to ffs_mountroot;

based on a discussion/diff from deraadt@@.
OK deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.163 2008/03/23 12:32:44 miod Exp $	*/
d1427 1
a1427 1
		np->netc_rnodes);
@


1.163
log
@Wrong printf construct.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.162 2008/03/16 19:42:57 otto Exp $	*/
a216 24

/*
 * Find an appropriate filesystem to use for the root. If a filesystem
 * has not been preselected, walk through the list of known filesystems
 * trying those that have mountroot routines, and try them until one
 * works or we have tried them all.
 */
int
vfs_mountroot(void)
{
	struct vfsconf *vfsp;
	int error;

	if (mountroot != NULL)
		return ((*mountroot)());
	for (vfsp = vfsconf; vfsp; vfsp = vfsp->vfc_next) {
		if (vfsp->vfc_mountroot == NULL)
			continue;
		if ((error = (*vfsp->vfc_mountroot)()) == 0)
			return (0);
		printf("%s_mountroot failed: %d\n", vfsp->vfc_name, error);
 	}
	return (ENODEV);
}
@


1.162
log
@Widen some struct statfs fields to support large filesystem stata
and add some to be able to support statvfs(2). Do the compat dance
to provide backward compatibility.  ok thib@@ miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.161 2007/12/13 18:22:36 blambert Exp $	*/
d2275 1
a2275 1
		(*pr)("\n", vp);
@


1.161
log
@replace calls to ltsleep with tsleep

remove PNORELOCK flag, as PNORELOCK is used for msleep

ok art@@ thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.160 2007/11/16 13:47:27 deraadt Exp $	*/
d2231 1
a2231 1
	(*pr)("statvfs cache: bsize %x iosize %x\nblocks %u free %u avail %u\n",
d2235 2
a2236 2
	(*pr)("  files %u ffiles %u\n", mp->mnt_stat.f_files,
	    mp->mnt_stat.f_ffree);
d2242 1
a2242 1
 	(*pr)("  syncwrites %lu asyncwrites = %lu\n",
d2245 3
d2279 25
@


1.160
log
@er, the newline is wrong.  dissapointing.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.159 2007/11/15 16:50:28 deraadt Exp $	*/
d603 1
a603 1
		ltsleep(vp, PINOD | PNORELOCK, "vget", 0, NULL);
d1016 1
a1016 1
		ltsleep(vp, PINOD | PNORELOCK, "vgone", 0, NULL);
@


1.159
log
@newline before syncing disks is way prettier
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.158 2007/10/29 14:12:19 chl Exp $	*/
d1641 1
a1641 1
	printf("\nsyncing disks... ");
@


1.158
log
@MALLOC/FREE -> malloc/free
replace an hard coded value with M_WAITOK

ok krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.157 2007/09/15 19:22:18 bluhm Exp $	*/
d1641 1
a1641 1
	printf("syncing disks... ");
@


1.157
log
@Allow to pull out an usb stick with ffs filesystem while mounted
and a file is written onto the stick.  Without these fixes the
machine panics or hangs.
The usb fix calls the callback when the stick is pulled out to free
the associated buffers.  Otherwise we have busy buffers for ever
and the automatic unmount will panic.
The change in the scsi layer prevents passing down further dirty
buffers to usb after the stick has been deactivated.
In vfs the automatic unmount has moved from the function vgonel()
to vop_generic_revoke().  Both are called when the sd device's vnode
is removed.  In vgonel() the VXLOCK is already held which can cause
a deadlock.  So call dounmount() earlier.

ok krw@@, I like this marco@@, tested by ian@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.156 2007/09/07 15:00:20 art Exp $	*/
d539 2
a540 2
		MALLOC(nvp->v_specinfo, struct specinfo *,
			sizeof(struct specinfo), M_VNODE, M_WAITOK);
d1062 1
a1062 1
		FREE(vp->v_specinfo, M_VNODE);
@


1.156
log
@Use M_ZERO in a few more places to shave bytes from the kernel.

eyeballed and ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.155 2007/08/07 04:32:45 beck Exp $	*/
a1008 2
	struct mount *mp;
	int flags;
a1061 14

		/*
		 * If we have a mount point associated with the vnode, we must
		 * flush it out now, as to not leave a dangling zombie mount
		 * point laying around in VFS.
		 */
		mp = vp->v_specmountpoint;
		if (mp != NULL) {
			if (!vfs_busy(mp, VB_WRITE|VB_WAIT)) {
				flags = MNT_FORCE | MNT_DOOMED;
				dounmount(mp, flags, p, NULL);
			}
		}

@


1.155
log
@
   A few changes to deal with multi-user performance issues seen. this
brings us back roughly to 4.1 level performance, although this is still
far from optimal as we have seen in a number of cases. This change

	1) puts a lower bound on buffer cache queues to prevent starvation
	2) fixes the code which looks for a buffer to recycle
	3) reduces the number of vnodes back to 4.1 levels to avoid complex
	   performance issues better addressed after 4.2

ok art@@ deraadt@@, tested by many
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.154 2007/06/01 17:29:10 beck Exp $	*/
d202 1
a202 2
	mp = malloc(sizeof(struct mount), M_MOUNT, M_WAITOK);
	bzero(mp, sizeof(struct mount));
d1430 1
a1430 2
	np = (struct netcred *)malloc(i, M_NETADDR, M_WAITOK);
	bzero(np, i);
@


1.154
log
@decouple the allocated number of vnodes from the "desiredvnodes" variable
which is used to size a zillion other things that increasing excessively
has been shown to cause problems - so that we may incrementally look at
increasing those other things without making the kernel unusable.

This diff effectivly increases the number of vnodes back to the number
of buffers, as in the earlier dynamic buffer cache commits, without
increasing anything else (namecache, softdeps, etc. etc.)

ok pedro@@ tedu@@ art@@ thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.153 2007/05/31 17:00:51 tedu Exp $	*/
d123 1
a123 6
	/* 
	 * XXX note this is different from desiredvnodes, which is
	 * XXX the old static value computed from MAXUSERS which is
	 * XXX used for sizing may other subsystems (namecache, softdeps, etc)
	 */
	maxvnodes = bufpages;
@


1.153
log
@remove some silly casts, no real change
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.152 2007/05/31 05:12:41 pedro Exp $	*/
d108 1
d122 7
d367 1
a367 1
	if (numvnodes > 2 * desiredvnodes)
d371 1
a371 1
	if ((numvnodes < desiredvnodes) ||
@


1.152
log
@NFSv2 cannot cope with a big number of vnodes, so revert to NPROC-based
calculation until the problem is fixed, okay beck@@ art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.151 2007/05/30 04:27:42 beck Exp $	*/
d199 3
a201 3
	mp = malloc((u_long)sizeof(struct mount), M_MOUNT, M_WAITOK);
	bzero((char *)mp, (u_long)sizeof(struct mount));
	(void) vfs_busy(mp, VB_READ|VB_NOWAIT);
d211 1
a211 1
	(void) copystr(devname, mp->mnt_stat.f_mntfromname, MNAMELEN - 1, 0);
d255 1
a255 1
	return ((struct mount *)0);
@


1.151
log
@back out vfs change - todd fries has seen afs issues, and I'm suspicious
this can cause other problems.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.149 2007/05/28 21:05:21 thib Exp $	*/
a113 1
int desiredvnodes;
a120 2

	desiredvnodes = bufpages;
@


1.150
log
@
	Step one of some vnode improvements - change getnewvnode to
actually allocate "desiredvnodes" - add a vdrop to un-hold a vnode held
with vhold, and change the name cache to make use of vhold/vdrop, while
keeping track of which vnodes are referred to by which cache entries to
correctly hold/drop vnodes when the cache uses them.
ok thib@@, tedu@@, art@@
@
text
@d342 1
d347 4
a350 2
	 * Allocate a new vnode if we have less than the desired
	 * number allocated, otherwise, recycle one. Generally we only
d352 13
a364 7
	 * them, so we look first on the vnode_free_list. If it is
	 * empty, we next consider vnodes with referencing buffers on
	 * the vnode_hold_list. We are reticent to recycle vnodes from
	 * the vnode_hold_list because we will lose the identity of
	 * all its referencing buffers.
	 */
	simple_lock(&vnode_free_list_slock);
d366 3
a368 1
	if (numvnodes < desiredvnodes) {
a371 2
		LIST_INIT(&vp->v_cache_src);
		TAILQ_INIT(&vp->v_cache_dst);
d374 1
a374 1
		for (vp = TAILQ_FIRST(listhd = &vnode_free_list); vp != NULLVP;
a378 12
		/* 
		 * There is nothing on the free list, so we have to try to
		 * recycle one off the hold list
		 */
		if (vp == NULL) {
			for (vp = TAILQ_FIRST(listhd = &vnode_hold_list);
			    vp != NULLVP;
			    vp = TAILQ_NEXT(vp, v_freelist)) {
				if ((VOP_ISLOCKED(vp) == 0) && (vp->v_holdcnt == 0))
					break;
			}
		}
d380 3
a382 3
		 * We have made a pass through both the free and hold list
		 * and not encountered an unlocked entry. So this is close
		 * enough to being empty.
a395 5

		if (vp->v_holdcnt) {
			vprint("held vnode", vp);
			panic("unheld vnode being held!");
		}
d761 1
a761 1
 * declare interest in a vnode.
a777 21
void vdrop(struct vnode *vp);

/*
 * lose interest in a vnode
 */
void
vdrop(struct vnode *vp)
{
	vp->v_holdcnt--;

	/*
	 * If it is on the holdlist and the hold count drops to
	 * zero, move it to the free list.
	 */
	if ((vp->v_bioflag & VBIOONFREELIST) &&
	    vp->v_holdcnt == 0 && vp->v_usecount == 0) {
		TAILQ_REMOVE(&vnode_hold_list, vp, v_freelist);
		TAILQ_INSERT_TAIL(&vnode_free_list, vp, v_freelist);
	}
}

d1996 11
a2006 1
	vdrop(vp);
@


1.149
log
@de-inline vref();

ok pedro@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.148 2007/05/26 20:26:51 pedro Exp $	*/
a341 1
	static int toggle;
d346 2
a347 4
	 * We must choose whether to allocate a new vnode or recycle an
	 * existing one. The criterion for allocating a new one is that
	 * the total number of vnodes is less than the number desired or
	 * there are no vnodes on either free list. Generally we only
d349 7
a355 13
	 * them, so we look first on the vnode_free_list. If it is empty,
	 * we next consider vnodes with referencing buffers on the
	 * vnode_hold_list. The toggle ensures that half the time we
	 * will use a buffer from the vnode_hold_list, and half the time
	 * we will allocate a new one unless the list has grown to twice
	 * the desired size. We are reticent to recycle vnodes from the
	 * vnode_hold_list because we will lose the identity of all its
	 * referencing buffers.
	 */
	toggle ^= 1;
	if (numvnodes > 2 * desiredvnodes)
		toggle = 0;

d357 1
a357 3
	if ((numvnodes < desiredvnodes) ||
	    ((TAILQ_FIRST(listhd = &vnode_free_list) == NULL) &&
	    ((TAILQ_FIRST(listhd = &vnode_hold_list) == NULL) || toggle))) {
d361 2
d365 1
a365 1
		for (vp = TAILQ_FIRST(listhd); vp != NULLVP;
d370 12
d383 3
a385 3
		 * Unless this is a bad time of the month, at most
		 * the first NCPUS items on the free list are
		 * locked, so this is close enough to being empty.
d399 5
d769 1
a769 1
 * Page or buffer structure gets a reference.
d786 21
d2025 1
a2025 11
	vp->v_holdcnt--;

	/*
	 * If it is on the holdlist and the hold count drops to
	 * zero, move it to the free list.
	 */
	if ((vp->v_bioflag & VBIOONFREELIST) &&
	    vp->v_holdcnt == 0 && vp->v_usecount == 0) {
		TAILQ_REMOVE(&vnode_hold_list, vp, v_freelist);
		TAILQ_INSERT_TAIL(&vnode_free_list, vp, v_freelist);
	}
@


1.148
log
@Dynamic buffer cache. Initial diff from mickey@@, okay art@@ beck@@ toby@@
deraadt@@ dlg@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.147 2007/05/26 18:42:21 thib Exp $	*/
d633 1
a633 4
#ifdef DIAGNOSTIC
/*
 * Vnode reference.
 */
d637 1
d640 1
a642 1
#endif /* DIAGNOSTIC */
@


1.147
log
@Nuke a bunch of simpelocks and associated goo.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.146 2007/05/17 23:46:28 thib Exp $	*/
d123 1
a123 2
	/* every buffer needs its vnode! */
	desiredvnodes = nbuf;
d1696 1
a1696 1
		for (bp = &buf[nbuf]; --bp >= buf; ) {
@


1.146
log
@Collapse struct v_selectinfo in struct vnode, remove the
simplelock and reuse the name for the selinfo member.
Clean-up accordingly.

ok tedu@@,art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.145 2007/05/09 01:09:16 deraadt Exp $	*/
a94 4
static struct simplelock mntid_slock;
struct simplelock mntvnode_slock;
struct simplelock vnode_free_list_slock;
struct simplelock spechash_slock;
a126 3
	simple_lock_init(&mntvnode_slock);
	simple_lock_init(&mntid_slock);
	simple_lock_init(&spechash_slock);
a128 1
	simple_lock_init(&vnode_free_list_slock);
a273 1
	simple_lock(&mntid_slock);
a287 1
	simple_unlock(&mntid_slock);
a365 1
	simple_lock(&vnode_free_list_slock);
a370 1
		simple_unlock(&vnode_free_list_slock);
a386 1
			simple_unlock(&vnode_free_list_slock);
a402 1
		simple_unlock(&vnode_free_list_slock);
a435 2
	simple_lock(&mntvnode_slock);

a445 2

	simple_unlock(&mntvnode_slock);
a519 1
	simple_lock(&spechash_slock);
a527 1
			simple_unlock(&spechash_slock);
a531 1
			simple_unlock(&spechash_slock);
a548 1
		simple_unlock(&spechash_slock);
a568 1
	simple_unlock(&spechash_slock);
a611 1
		simple_lock(&vnode_free_list_slock);
a615 1
		simple_unlock(&vnode_free_list_slock);
a774 1
		simple_lock(&vnode_free_list_slock);
a776 1
		simple_unlock(&vnode_free_list_slock);
a799 1
	simple_lock(&mntvnode_slock);
a804 1
		simple_unlock(&mntvnode_slock);
a807 2
		simple_lock(&mntvnode_slock);

a810 1
	simple_unlock(&mntvnode_slock);
a1039 1
		simple_lock(&spechash_slock);
a1067 1
		simple_unlock(&spechash_slock);
a1098 1
		simple_lock(&vnode_free_list_slock);
a1108 1
		simple_unlock(&vnode_free_list_slock);
a1120 1
	simple_lock(&spechash_slock);
a1127 1
	simple_unlock(&spechash_slock);
a1157 1
	simple_lock(&spechash_slock);
a1165 1
			simple_unlock(&spechash_slock);
a1170 1
	simple_unlock(&spechash_slock);
a1346 1
				simple_unlock(&mntvnode_slock);
a1353 1
				simple_unlock(&mntvnode_slock);
a1367 1
			simple_lock(&mntvnode_slock);
a1369 1
		simple_unlock(&mntvnode_slock);
a1390 1
		simple_lock(&spechash_slock);
a1399 1
		simple_unlock(&spechash_slock);
a2006 1
		simple_lock(&vnode_free_list_slock);
a2008 1
		simple_unlock(&vnode_free_list_slock);
@


1.145
log
@kinfo_vgetfailed has not been used for > 8 years
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.144 2007/04/13 17:09:22 thib Exp $	*/
a1000 1
	simple_lock(&vp->v_selectinfo.vsi_lock);
a1001 1
	simple_unlock(&vp->v_selectinfo.vsi_lock);
@


1.144
log
@Move the declaration of VN_KNOTE() into vnode.h instead of having
multiple defines all over;

ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.143 2007/04/13 10:44:07 bluhm Exp $	*/
a1352 1
int kinfo_vgetfailed;
@


1.143
log
@Remove comments talking about vnode interlock.  No binary change.
ok thib
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.142 2007/04/11 16:08:50 thib Exp $	*/
a115 3

#define VN_KNOTE(vp, b) \
	KNOTE((struct klist *)&vp->v_selectinfo.vsi_selinfo.si_note, (b))
@


1.142
log
@Remove the simplelock argument from vrecycle();

ok pedro@@, sturm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.141 2007/03/21 17:29:31 thib Exp $	*/
a927 1
 * The vnode interlock is held on entry.
a1019 1
 * Release the passed interlock if the vnode will be recycled.
d1043 1
a1043 1
 * vgone, with the vp interlock held.
@


1.141
log
@Remove the v_interlock simplelock from the vnode structure.
Zap all calls to simple_lock/unlock() on it (those calls are
#defined away though). Remove the LK_INTERLOCK from the calls
to vn_lock() and cleanup the filesystems wich implement VOP_LOCK().
(by remvoing the v_interlock from there calls to lockmgr()).

ok pedro@@, art@@, tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.140 2007/03/12 19:25:58 mickey Exp $	*/
d1024 1
a1024 1
vrecycle(struct vnode *vp, struct simplelock *inter_lkp, struct proc *p)
a1026 2
		if (inter_lkp)
			simple_unlock(inter_lkp);
@


1.140
log
@better desiredvnodes not based on maxusers; pedro@@ deraadt@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.139 2007/02/20 17:42:47 deraadt Exp $	*/
a387 1
		simple_lock_init(&vp->v_interlock);
d392 2
a393 6
			if (simple_lock_try(&vp->v_interlock)) {
				if (VOP_ISLOCKED(vp) == 0)
					break;
				else
					simple_unlock(&vp->v_interlock);
			}
a421 2
		else
			simple_unlock(&vp->v_interlock);
a542 1
		simple_lock(&vp->v_interlock);
a543 1
			simple_unlock(&vp->v_interlock);
d554 1
a554 1
		if (vget(vp, LK_EXCLUSIVE | LK_INTERLOCK, p)) {
a595 1
	simple_lock(&vp->v_interlock);
a623 4
	if ((flags & LK_INTERLOCK) == 0) {
		simple_lock(&vp->v_interlock);
		flags |= LK_INTERLOCK;
	}
a626 1
			simple_unlock(&vp->v_interlock);
d630 2
a631 2
 		vp->v_flag |= VXWANT;
		ltsleep(vp, PINOD | PNORELOCK, "vget", 0, &vp->v_interlock);
d633 1
a633 1
 	}
a653 2

			simple_unlock(&vp->v_interlock);
a657 2
	simple_unlock(&vp->v_interlock);

a668 1
	simple_lock(&vp->v_interlock);
a671 1
	simple_unlock(&vp->v_interlock);
a718 1
	simple_lock(&vp->v_interlock);
a727 1
		simple_unlock(&vp->v_interlock);
a737 1
	simple_unlock(&vp->v_interlock);
a740 2
	simple_lock(&vp->v_interlock);

a742 2

	simple_unlock(&vp->v_interlock);
a757 1
	simple_lock(&vp->v_interlock);
a765 1
		simple_unlock(&vp->v_interlock);
d776 1
a776 1
	if (vn_lock(vp, LK_EXCLUSIVE|LK_INTERLOCK, p)) {
a784 2
	simple_lock(&vp->v_interlock);

a786 2

	simple_unlock(&vp->v_interlock);
a800 1
  	simple_lock(&vp->v_interlock);
a808 1
	simple_unlock(&vp->v_interlock);
a835 1
		simple_lock(&vp->v_interlock);		
a861 1
		simple_unlock(&vp->v_interlock);
a865 1
		simple_unlock(&vp->v_interlock);
a874 1
		simple_unlock(&vp->v_interlock);
a906 1
	simple_unlock(&vp->v_interlock);
d958 1
a958 1
	VOP_LOCK(vp, LK_DRAIN | LK_INTERLOCK, p);
a991 2
		simple_lock(&vp->v_interlock);

a997 2

		simple_unlock(&vp->v_interlock);
a1025 1
	simple_lock(&vp->v_interlock);
a1031 1
	simple_unlock(&vp->v_interlock);
a1042 2

	simple_lock (&vp->v_interlock);
d1063 1
a1063 1
		ltsleep(vp, PINOD | PNORELOCK, "vgone", 0, &vp->v_interlock);
a2051 1
	simple_lock(&vp->v_interlock);
a2068 1
	simple_unlock(&vp->v_interlock);
@


1.139
log
@for vfsconf sysctl, do not leak kernel sensors out to userland
ok art thib
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.138 2007/02/17 23:57:16 mickey Exp $	*/
d121 1
d130 2
@


1.138
log
@fix ddb buf printing for daddr_t growth to 64bit;
from juan hernandez gonzalez; tested by bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.135 2006/11/20 12:52:54 tom Exp $	*/
d1354 2
a1355 1
	struct vfsconf *vfsp;
d1388 12
a1399 2
		return (sysctl_rdstruct(oldp, oldlenp, newp, vfsp,
		    sizeof(struct vfsconf)));
@


1.137
log
@Consistently spell FALLTHROUGH to appease lint.
ok kettenis@@ cloder@@ tom@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.136 2007/02/13 17:04:14 mickey Exp $	*/
d2278 1
a2278 1
	(*pr)("  vp %p lblkno 0x%x blkno 0x%x dev 0x%x\n"
d2280 1
a2280 1
	    bp->b_vp, bp->b_lblkno, bp->b_blkno, bp->b_dev,
@


1.136
log
@fix ddb buf print
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.135 2006/11/20 12:52:54 tom Exp $	*/
d2181 1
a2181 1
				/* fall through */
@


1.135
log
@vprint() should be defined if DIAGNOSTIC || DEBUG.  Noticed by (and
original diff from) Jake < antipsychic (at) hotmail.com >.  Discussed
with Mickey and Miod.

ok miod@@ pedro@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.134 2006/10/30 00:34:01 thib Exp $	*/
d2283 1
a2283 1
	(*pr)("  bufsize 0x%lx bcount 0x%lx resid 0x%zx sync 0x%x\n"
d2285 1
a2285 1
	    bp->b_bufsize, bp->b_bcount, bp->b_resid, bp->b_synctime,
@


1.134
log
@use vp->v_type to index into vtypes rather then vp->v_tag,
fixing odd output in the 'show vnode' ddb code.

ok mickey@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.133 2006/07/11 21:17:58 mickey Exp $	*/
d1270 1
a1270 1
#ifdef DIAGNOSTIC
d1315 1
a1315 1
#endif /* DIAGNOSTIC */
@


1.133
log
@add mount/vnode/buf and softdep printing commands; tested on a few archs and will make pedro happy too (;
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.132 2006/07/09 23:20:50 pedro Exp $	*/
d2307 2
a2308 2
	      vp->v_type > NENTS(vtypes)? "<unk>":vtypes[vp->v_tag],vp->v_type,
	      vp->v_mount, vp->v_mountedhere);
@


1.132
log
@Fix tab where space was meant
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.131 2006/07/08 20:01:13 thib Exp $	*/
d2268 125
@


1.131
log
@vinvalbuf() debugging aid, under VFSDEBUG.

ok pedro@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.130 2006/07/03 12:39:52 mickey Exp $	*/
d1922 1
a1922 1
#ifdef	VFSDEBUG
@


1.130
log
@also print vp in vprint (useful for debugging); pedro@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.129 2006/06/25 15:01:53 sturm Exp $	*/
d1921 5
@


1.129
log
@rename vfs_busy() flags VB_UMIGNORE/VB_UMWAIT to VB_NOWAIT/VB_WAIT

requested by and ok pedro
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.128 2006/06/14 20:01:50 sturm Exp $	*/
d1284 2
a1285 2
	printf("type %s, usecount %u, writecount %u, holdcount %u,",
		typename[vp->v_type], vp->v_usecount, vp->v_writecount,
@


1.128
log
@move vfs_busy() to rwlocks and properly hide the locking api from vfs

ok tedu, pedro
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.127 2006/06/02 20:25:09 pedro Exp $	*/
d165 1
a165 1
	if (flags & VB_UMWAIT)
d213 1
a213 1
	(void) vfs_busy(mp, VB_READ|VB_UMIGNORE);
d1165 1
a1165 1
			if (!vfs_busy(mp, VB_WRITE|VB_UMWAIT)) {
d1332 1
a1332 1
		if (vfs_busy(mp, VB_READ|VB_UMIGNORE)) {
d1419 1
a1419 1
		if (vfs_busy(mp, VB_READ|VB_UMIGNORE)) {
d1720 1
a1720 1
		if ((vfs_busy(mp, VB_WRITE|VB_UMIGNORE)) != 0)
@


1.127
log
@Add a clonable devices implementation. Hacked along with thib@@, input
from krw@@ and toby@@, subliminal prodding from dlg@@, okay deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.126 2006/05/28 04:03:28 pedro Exp $	*/
d148 2
a149 5
 * historical behavior:
 *  - LK_NOWAIT means that we should just ignore the mount point if it's
 *     being unmounted.
 *  - no flags means that we should sleep on the mountpoint and then
 *     fail.
d154 1
a154 1
	int lkflags;
d156 13
a168 10
	switch (flags) {
	case LK_NOWAIT:
		lkflags = LK_SHARED|LK_NOWAIT;
		break;
	case 0:
		lkflags = LK_SHARED;
		break;
	default:
		lkflags = flags;
	}
d170 2
a171 5
	/*
	 * Always sleepfail. We will only sleep for an exclusive lock
	 * and the exclusive lock will only be acquired when unmounting.
	 */
	lkflags |= LK_SLEEPFAIL;
a172 2
	if (lockmgr(&mp->mnt_lock, lkflags, NULL))
		return (ENOENT);
d182 1
a182 1
	lockmgr(&mp->mnt_lock, LK_RELEASE, NULL);
d188 4
a191 1
	return (lockstatus(&mp->mnt_lock));
d213 1
a213 2
	lockinit(&mp->mnt_lock, PVFS, "vfslock", 0, 0);
	(void)vfs_busy(mp, LK_NOWAIT);
d1165 1
a1165 1
			if (!vfs_busy(mp, LK_EXCLUSIVE)) {
d1332 1
a1332 1
		if (vfs_busy(mp, LK_NOWAIT)) {
d1419 1
a1419 1
		if (vfs_busy(mp, LK_NOWAIT)) {
d1720 1
a1720 1
		if ((vfs_busy(mp, LK_EXCLUSIVE|LK_NOWAIT)) != 0)
@


1.126
log
@Spacing in vfs_sysctl()
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.125 2006/05/07 14:12:15 sturm Exp $	*/
d581 1
@


1.125
log
@forgot to remove this sentence from the comment
ok pedro
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.124 2006/04/30 14:20:07 sturm Exp $	*/
d1361 1
d1366 1
d1369 1
d1373 1
d1377 1
d1381 1
d1385 1
d1388 1
d1392 1
@


1.124
log
@remove the simplelock argument from vfs_busy() which is currently not
used and will never be used this way in VFS

requested by and ok pedro, ok krw, biorn
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.123 2006/04/19 11:55:55 pedro Exp $	*/
d146 1
a146 1
 * unmounting. Interlock is not released on failure.
@


1.123
log
@Remove unused mount list simple_lock() goo
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.122 2006/01/09 12:43:16 pedro Exp $	*/
d155 1
a155 1
vfs_busy(struct mount *mp, int flags, struct simplelock *interlkp)
d176 1
a176 3
	if (interlkp)
		lkflags |= LK_INTERLOCK;
	if (lockmgr(&mp->mnt_lock, lkflags, interlkp))
d216 1
a216 1
	(void)vfs_busy(mp, LK_NOWAIT, NULL);
d1167 1
a1167 1
			if (!vfs_busy(mp, LK_EXCLUSIVE, NULL)) {
d1334 1
a1334 1
		if (vfs_busy(mp, LK_NOWAIT, NULL)) {
d1412 1
a1412 1
		if (vfs_busy(mp, LK_NOWAIT, NULL)) {
d1713 1
a1713 1
		if ((vfs_busy(mp, LK_EXCLUSIVE|LK_NOWAIT, NULL)) != 0)
@


1.122
log
@Put vprint() under DIAGNOSTIC, as to save space in generated ramdisks.
Inspiration from miod@@, okay deraadt@@. Tested on i386, macppc and amd64.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.121 2005/11/30 10:35:07 pedro Exp $	*/
a94 1
struct simplelock mountlist_slock;
a137 1
	simple_lock_init(&mountlist_slock);
a264 1
	simple_lock(&mountlist_slock);
a267 1
			simple_unlock(&mountlist_slock);
d271 1
a271 1
	simple_unlock(&mountlist_slock);
d1333 1
a1333 1
	simple_lock(&mountlist_slock);
d1336 1
a1336 1
		if (vfs_busy(mp, LK_NOWAIT, &mountlist_slock)) {
a1343 1
		simple_lock(&mountlist_slock);
a1346 1
	simple_unlock(&mountlist_slock);
a1411 1
	simple_lock(&mountlist_slock);
d1414 1
a1414 1
		if (vfs_busy(mp, LK_NOWAIT, &mountlist_slock)) {
a1454 1
		simple_lock(&mountlist_slock);
d1459 1
a1459 1
	simple_unlock(&mountlist_slock);
a1460 1
	*sizep = bp - where;
@


1.121
log
@No need for vfs_busy() and vfs_unbusy() to take a process pointer
anymore. Testing by jolan@@, thanks.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.120 2005/11/24 12:08:16 pedro Exp $	*/
d418 2
d424 1
d810 1
d812 1
d814 1
d1278 1
d1323 1
d2027 1
d2029 1
@


1.120
log
@Remove kernfs, okay deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.119 2005/11/19 02:18:01 pedro Exp $	*/
d157 1
a157 2
vfs_busy(struct mount *mp, int flags, struct simplelock *interlkp,
    struct proc *p)
a184 1

d189 1
a189 1
vfs_unbusy(struct mount *mp, struct proc *p)
a208 1
	struct proc *p = curproc;
d220 1
a220 1
	(void)vfs_busy(mp, LK_NOWAIT, 0, p);
d1167 1
a1167 1
			if (!vfs_busy(mp, LK_EXCLUSIVE, NULL, p)) {
a1324 1
	struct proc *p = curproc;
d1332 1
a1332 1
		if (vfs_busy(mp, LK_NOWAIT, &mountlist_slock, p)) {
d1342 1
a1342 1
		vfs_unbusy(mp, p);
d1413 1
a1413 1
		if (vfs_busy(mp, LK_NOWAIT, &mountlist_slock, p)) {
d1437 1
a1437 1
				vfs_unbusy(mp, p);
d1446 1
a1446 1
				vfs_unbusy(mp, p);
d1456 1
a1456 1
		vfs_unbusy(mp, p);
a1709 1
	struct proc *p = curproc;
d1716 1
a1716 1
		if ((vfs_busy(mp, LK_EXCLUSIVE|LK_NOWAIT, NULL, p)) != 0)
@


1.119
log
@Remove unnecessary lockmgr() archaism that was costing too much in terms
of panics and bugfixes. Access curproc directly, do not expect a process
pointer as an argument. Should fix many "process context required" bugs.
Incentive and okay millert@@, okay marc@@. Various testing, thanks.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.118 2005/11/18 13:25:40 pedro Exp $	*/
d495 1
a495 1
 * Used for kernfs and some console handling.
d506 1
a506 1
 * and by cdevvp (character device) for console and kernfs.
@


1.118
log
@Work around yet another race on non-locking file systems: when calling
VOP_INACTIVE() in vrele() and vput(), we may sleep. Since there's no
locking of any kind, someone can vget() the vnode and vrele() it while
we sleep, beating us in getting the vnode on the free list.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.117 2005/11/08 15:50:01 pedro Exp $	*/
d181 1
a181 1
	if (lockmgr(&mp->mnt_lock, lkflags, interlkp, p))
d193 1
a193 1
	lockmgr(&mp->mnt_lock, LK_RELEASE, NULL, p);
@


1.117
log
@Missed one use of 'register'
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.116 2005/11/07 23:15:00 pedro Exp $	*/
d772 1
a772 1
	if (vp->v_usecount == 0)
d819 1
a819 1
	if (vp->v_usecount == 0)
@


1.116
log
@Use ANSI function declarations and deregister, no binary change
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.115 2005/10/19 16:50:46 pedro Exp $	*/
d1582 1
a1582 1
	register struct radix_node_head *rnh = (struct radix_node_head *)w;
@


1.115
log
@Remove v_vnlock from struct vnode, okay krw@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.114 2005/05/26 00:33:45 pedro Exp $	*/
d72 1
d127 1
a127 1
vntblinit()
d209 1
a209 4
vfs_rootmountalloc(fstypename, devname, mpp)
	char *fstypename;
	char *devname;
	struct mount **mpp;
d211 1
a211 1
	struct proc *p = curproc;	/* XXX */
d245 1
a245 1
vfs_mountroot()
d266 1
a266 2
vfs_getvfs(fsid)
	fsid_t *fsid;
d268 1
a268 1
	register struct mount *mp;
d287 1
a287 2
vfs_getnewfsid(mp)
	struct mount *mp;
d318 1
a318 2
makefstype(type)
	char *type;
d333 1
a333 2
vattr_null(vap)
	register struct vattr *vap;
d360 2
a361 5
getnewvnode(tag, mp, vops, vpp)
	enum vtagtype tag;
	struct mount *mp;
	int (**vops)(void *);
	struct vnode **vpp;
d363 1
a363 1
	struct proc *p = curproc;			/* XXX */
d464 1
a464 3
insmntque(vp, mp)
	register struct vnode *vp;
	register struct mount *mp;
a481 1

d488 1
a488 3
bdevvp(dev, vpp)
	dev_t dev;
	struct vnode **vpp;
a489 1

d498 1
a498 3
cdevvp(dev, vpp)
	dev_t dev;
	struct vnode **vpp;
a499 1

d509 1
a509 4
getdevvp(dev, vpp, type)
	dev_t dev;
	struct vnode **vpp;
	enum vtype type;
d511 1
a511 1
	register struct vnode *vp;
d543 1
a543 4
checkalias(nvp, nvp_rdev, mp)
	register struct vnode *nvp;
	dev_t nvp_rdev;
	struct mount *mp;
d546 1
a546 1
	register struct vnode *vp;
d629 1
a629 4
vget(vp, flags, p)
	struct vnode *vp;
	int flags;
	struct proc *p;
d691 1
a691 2
vref(vp)
	struct vnode *vp;
d737 1
a737 2
vput(vp)
	register struct vnode *vp;
d739 1
a739 1
	struct proc *p = curproc;	/* XXX */
d783 1
a783 2
vrele(vp)
	register struct vnode *vp;
d785 1
a785 1
	struct proc *p = curproc;	/* XXX */
d831 1
a831 2
vhold(vp)
	register struct vnode *vp;
a832 1

d955 1
a955 4
vflush(mp, skipvp, flags)
	struct mount *mp;
	struct vnode *skipvp;
	int flags;
d974 1
a974 4
vclean(vp, flags, p)
	register struct vnode *vp;
	int flags;
	struct proc *p;
d1071 1
a1071 4
vrecycle(vp, inter_lkp, p)
	struct vnode *vp;
	struct simplelock *inter_lkp;
	struct proc *p;
a1072 1

a1083 1

d1089 1
a1089 2
vgone(vp)
	register struct vnode *vp;
d1101 1
a1101 3
vgonel(vp, p)
	struct vnode *vp;
	struct proc *p;
d1103 1
a1103 1
	register struct vnode *vq;
d1212 1
a1212 4
vfinddev(dev, type, vpp)
	dev_t dev;
	enum vtype type;
	struct vnode **vpp;
d1214 1
a1214 1
	register struct vnode *vp;
d1234 1
a1234 3
vdevgone(maj, minl, minh, type)
	int maj, minl, minh;
	enum vtype type;
d1248 1
a1248 2
vcount(vp)
	struct vnode *vp;
d1282 1
a1282 3
vprint(label, vp)
	char *label;
	register struct vnode *vp;
d1326 1
a1326 1
printlockedvnodes()
d1329 2
a1330 2
	register struct mount *mp, *nmp;
	register struct vnode *vp;
d1357 2
a1358 8
vfs_sysctl(name, namelen, oldp, oldlenp, newp, newlen, p)
	int *name;
	u_int namelen;
	void *oldp;
	size_t *oldlenp;
	void *newp;
	size_t newlen;
	struct proc *p;
d1400 1
a1400 4
sysctl_vnode(where, sizep, p)
	char *where;
	size_t *sizep;
	struct proc *p;
d1402 1
a1402 1
	register struct mount *mp, *nmp;
d1404 1
a1404 1
	register char *bp = where, *savebp;
d1473 1
a1473 2
vfs_mountedon(vp)
	register struct vnode *vp;
d1475 1
a1475 1
	register struct vnode *vq;
d1501 2
a1502 4
vfs_hang_addrlist(mp, nep, argp)
	struct mount *mp;
	struct netexport *nep;
	struct export_args *argp;
d1504 3
a1506 3
	register struct netcred *np;
	register struct radix_node_head *rnh;
	register int i;
d1580 1
a1580 3
vfs_free_netcred(rn, w)
	struct radix_node *rn;
	void *w;
d1593 1
a1593 2
vfs_free_addrlist(nep)
	struct netexport *nep;
d1595 2
a1596 2
	register int i;
	register struct radix_node_head *rnh;
d1607 1
a1607 4
vfs_export(mp, nep, argp)
	struct mount *mp;
	struct netexport *nep;
	struct export_args *argp;
d1624 1
a1624 4
vfs_export_lookup(mp, nep, nam)
	register struct mount *mp;
	struct netexport *nep;
	struct mbuf *nam;
d1626 2
a1627 2
	register struct netcred *np;
	register struct radix_node_head *rnh;
d1661 2
a1662 6
vaccess(file_mode, uid, gid, acc_mode, cred)
	mode_t file_mode;
	uid_t uid;
	gid_t gid;
	mode_t acc_mode;
	struct ucred *cred;
d1744 1
a1744 1
vfs_shutdown()
d1777 1
a1777 2
vfs_syncwait(verbose)
	int verbose;
d1779 1
a1779 1
	register struct buf *bp;
d1826 2
a1827 8
fs_posix_sysctl(name, namelen, oldp, oldlenp, newp, newlen, p)
	int *name;
	u_int namelen;
	void *oldp;
	size_t *oldlenp;
	void *newp;
	size_t newlen;
	struct proc *p;
d1848 2
a1849 8
fs_sysctl(name, namelen, oldp, oldlenp, newp, newlen, p)
	int *name;
	u_int namelen;
	void *oldp;
	size_t *oldlenp;
	void *newp;
	size_t newlen;
	struct proc *p;
d1874 1
a1874 4
vwaitforio(vp, slpflag, wmesg, timeo)
	struct vnode *vp;
	int slpflag, timeo;
	char *wmesg;
d1897 1
a1897 2
vwakeup(vp)
	struct vnode *vp;
d1916 2
a1917 6
vinvalbuf(vp, flags, cred, p, slpflag, slptimeo)
	register struct vnode *vp;
	int flags;
	struct ucred *cred;
	struct proc *p;
	int slpflag, slptimeo;
d1919 1
a1919 1
	register struct buf *bp;
d1990 1
a1990 3
vflushbuf(vp, sync)
	register struct vnode *vp;
	int sync;
d1992 1
a1992 1
	register struct buf *bp, *nbp;
d2036 1
a2036 3
bgetvp(vp, bp)
	register struct vnode *vp;
	register struct buf *bp;
d2061 1
a2061 2
brelvp(bp)
	register struct buf *bp;
d2112 1
a2112 3
buf_replacevnode(bp, newvp)
	struct buf *bp;
	struct vnode *newvp;
d2138 1
a2138 2
reassignbuf(bp)
	struct buf *bp;
d2186 1
a2186 2
vfs_register(vfs)
	struct vfsconf *vfs;
d2219 1
a2219 2
vfs_unregister(vfs)
	struct vfsconf *vfs;
d2255 1
a2255 3
vn_isdisk(vp, errp)
	struct vnode *vp;
	int *errp;
@


1.114
log
@RIP stackable filesystems, ok marius@@ tedu@@, discussed with deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.113 2005/05/24 05:34:54 pedro Exp $	*/
a458 1
	vp->v_vnlock = NULL;
a635 1
	vp->v_vnlock = NULL;
@


1.114.2.1
log
@MFC:
Fix by pedro@@

Work around yet another race on non-locking file systems: when calling
VOP_INACTIVE() in vrele() and vput(), we may sleep. Since there's no
locking of any kind, someone can vget() the vnode and vrele() it while
we sleep, beating us in getting the vnode on the free list.

ok deraadt@@ pedro@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.114 2005/05/26 00:33:45 pedro Exp $	*/
d803 1
a803 1
	if (vp->v_usecount == 0 && !(vp->v_bioflag & VBIOONFREELIST))
d851 1
a851 1
	if (vp->v_usecount == 0 && !(vp->v_bioflag & VBIOONFREELIST))
@


1.113
log
@when a device vnode associated with a mount point disappears, mark the
filesystem as doomed and unmount it
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.112 2005/05/22 21:12:42 pedro Exp $	*/
a411 2
				if ((vp->v_flag & VLAYER) == 0)
					break;
a459 1
	lockinit(&vp->v_lock, PVFS, "v_lock", 0, 0);
a637 1
	lockinit(&vp->v_lock, PVFS, "v_lock", 0, 0);
@


1.112
log
@put VLOCKSWORK stuff under a single option, VFSDEBUG
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.111 2005/05/01 12:28:18 pedro Exp $	*/
d1157 2
d1214 14
@


1.111
log
@check for VBIOONFREELIST and VBIOONSYNCLIST in vprint(), okay marius@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.110 2005/03/24 02:40:26 tedu Exp $	*/
d1101 1
a1101 1
#ifdef DIAGNOSTIC
@


1.110
log
@always good to check for invalid values.  ok marius pedro
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.109 2005/01/10 11:58:34 pedro Exp $	*/
d1347 5
a1351 1
		strlcat(buf, "| VBIOWAIT", sizeof buf);
@


1.109
log
@change vget() to only put a vnode back on the free lists if it actually
was there. should fix a (rare) corner case introduced by my last commit.
ok tedu@@, testing by joris, moritz@@, danh@@, otto@@ and krw@@. many thanks.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.108 2004/12/31 15:28:40 pedro Exp $	*/
d1574 2
a1575 1
	if (argp->ex_addrlen > MLEN)
@


1.109.2.1
log
@MFC:
Fix by pedro@@

Work around yet another race on non-locking file systems: when calling
VOP_INACTIVE() in vrele() and vput(), we may sleep. Since there's no
locking of any kind, someone can vget() the vnode and vrele() it while
we sleep, beating us in getting the vnode on the free list.

ok deraadt@@ pedro@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.109 2005/01/10 11:58:34 pedro Exp $	*/
d807 1
a807 1
	if (vp->v_usecount == 0 && !(vp->v_bioflag & VBIOONFREELIST))
d855 1
a855 1
	if (vp->v_usecount == 0 && !(vp->v_bioflag & VBIOONFREELIST))
@


1.108
log
@sprinkle some more list macros in here
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.107 2004/12/31 12:13:53 pedro Exp $	*/
d664 1
a664 2
	int error;
	int s;
d688 2
a689 2
	if (vp->v_usecount == 0 &&
	    (vp->v_bioflag & VBIOONFREELIST)) {
d700 1
d705 1
a705 1
			if (vp->v_usecount == 0)
d712 1
d714 1
@


1.107
log
@when releasing a vnode, make it inactive before sticking it to one of
the free lists. should fix some races on filesystems that don't have
locks, such as nfs. also, it allows for a more straightforward way of
releasing vnodes (nodes that are going to be recycled don't have to be
moved to the head of the list). tested by many, thanks.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.106 2004/12/28 15:14:37 deraadt Exp $	*/
d87 1
a87 1
	(bp)->b_vnbufs.le_next = NOLIST;				\
d2036 1
a2036 1
			nbp = bp->b_vnbufs.le_next;
d2160 1
a2160 1
	if (bp->b_vnbufs.le_next != NOLIST)
d2240 1
a2240 1
	if (bp->b_vnbufs.le_next != NOLIST)
d2242 1
@


1.106
log
@clean dirty accident by miod
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.105 2004/12/26 21:22:13 miod Exp $	*/
a798 2
	vputonfreelist(vp);

d802 7
d844 11
a854 1
	vputonfreelist(vp);
d856 1
a856 2
	if (vn_lock(vp, LK_EXCLUSIVE|LK_INTERLOCK, p) == 0)
		VOP_INACTIVE(vp, p);
@


1.105
log
@Use list and queue macros where applicable to make the code easier to read;
no change in compiler assembly output.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.104 2004/12/09 22:36:40 pedro Exp $	*/
d2014 1
a2014 1
		    (blist = LIST_FIRST(&vp->v_cleanblkhd)) &&
@


1.104
log
@minor spacing/styling nits
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.103 2004/08/04 20:36:27 art Exp $	*/
d892 1
a892 1
	for (vp = mp->mnt_vnodelist.lh_first; vp; vp = nvp) {
d895 1
a895 1
		nvp = vp->v_mntvnodes.le_next;
d1364 1
a1364 2
		for (vp = mp->mnt_vnodelist.lh_first; vp;
		    vp = vp->v_mntvnodes.le_next) {
d1456 1
a1456 1
		for (vp = mp->mnt_vnodelist.lh_first; vp != NULL;
d1470 1
a1470 1
			nvp = vp->v_mntvnodes.le_next;
d1995 1
a1995 1
		if (vp->v_dirtyblkhd.lh_first != NULL) {
d2001 1
a2001 1
			    vp->v_dirtyblkhd.lh_first != NULL)
d2009 1
a2009 1
		if ((blist = vp->v_cleanblkhd.lh_first) &&
d2012 3
a2014 2
				blist = blist->b_vnbufs.le_next;
		if (!blist && (blist = vp->v_dirtyblkhd.lh_first) &&
d2017 1
a2017 1
				blist = blist->b_vnbufs.le_next;
d2052 1
a2052 1
	    (vp->v_dirtyblkhd.lh_first || vp->v_cleanblkhd.lh_first))
d2068 3
a2070 2
	for (bp = vp->v_dirtyblkhd.lh_first; bp; bp = nbp) {
		nbp = bp->b_vnbufs.le_next;
d2093 1
a2093 1
	if (vp->v_dirtyblkhd.lh_first != NULL) {
@


1.103
log
@Uninline vputonfreelist.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.102 2004/08/04 03:05:25 pedro Exp $	*/
d90 2
a91 2
struct freelst vnode_hold_list;   /* list of vnodes referencing buffers */
struct freelst vnode_free_list;   /* vnode free list */
d93 1
a93 1
struct mntlist mountlist;			/* mounted filesystem list */
a154 1

a206 1

d245 1
a245 1
  */
d482 1
a485 1

d493 1
d1032 1
a1032 1
	 * clean out any VM data associated with the vnode.
a1093 2


a1420 1

@


1.102
log
@better comments
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.101 2004/08/02 03:26:50 pedro Exp $	*/
d109 1
a109 1
static __inline__ void vputonfreelist(struct vnode *);
d734 2
a735 3
static __inline__ void
vputonfreelist(vp)
	struct vnode *vp;
@


1.101
log
@- check for LK_NOWAIT on vget()
- use ltsleep() instead of the unlock + sleep combo

ok art@@, inspiration from free/net
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.100 2004/05/27 20:48:46 tedu Exp $	*/
d652 6
a657 5
 * reference count and lock it. The vnode lock bit is set the
 * vnode is being eliminated in vgone. The process is awakened
 * when the transition is completed, and an error returned to
 * indicate that the vnode is no longer usable (possibly having
 * been changed to a new file system type).
a912 1

a1960 1

d2181 1
a2181 1
 * Replaces the current vnode associated with the buffer, if any
d2184 1
a2184 1
 * If an output I/O is pending on the buffer, the old vnode is
@


1.100
log
@make acct(2) optional with ACCOUNTING
ok art@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.99 2004/05/27 08:25:53 tedu Exp $	*/
d666 1
d677 1
d679 5
d685 1
a685 2
		simple_unlock(&vp->v_interlock);
		tsleep(vp, PINOD, "vget", 0);
d688 1
d1152 1
a1152 2
		simple_unlock(&vp->v_interlock);
		tsleep(vp, PINOD, "vgone", 0);
d1155 1
@


1.99
log
@shutdown accounting before shutting down vfs.  should prevent some panics.
ok david@@ millert@@ (iirc)
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.98 2004/04/25 02:48:03 itojun Exp $	*/
d1793 1
d1796 3
a1802 2

	acct_shutdown();
@


1.98
log
@radix tree with multipath support.  from kame.  deraadt ok
user visible changes:
- you can add multiple routes with same key (route add A B then route add A C)
- you have to specify gateway address if there are multiple entries on the table
  (route delete A B, instead of route delete A)
kernel change:
- radix_node_head has an extra entry
- rnh_deladdr takes extra argument

TODO:
- actually take advantage of multipath (rtalloc -> rtalloc_mpath)
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.97 2004/01/09 03:01:03 tedu Exp $	*/
d1793 2
d1799 2
@


1.97
log
@back out vnode parents.  weird breakge found in ports tree
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.96 2004/01/06 04:22:59 tedu Exp $	*/
d1620 1
a1620 1
	(*rnh->rnh_deladdr)(rn->rn_key, rn->rn_mask, rnh);
@


1.96
log
@keep track of a vnode's parent dir.  ufs only, and unused atm, but
the fun stuff is coming.  testing by brad.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.95 2003/07/21 22:44:50 tedu Exp $	*/
a1054 4
	if (vp->v_parent) {
		vrele(vp->v_parent);
		vp->v_parent = NULL;
	}
@


1.95
log
@remove caddr_t casts.  it's just silly to cast something when the function
takes a void *.  convert uiomove to take a void * as well.  ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.94 2003/06/02 23:28:07 millert Exp $	*/
d1055 4
@


1.94
log
@Remove the advertising clause in the UCB license which Berkeley
rescinded 22 July 1999.  Proofed by myself and Theo.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.93 2003/05/13 09:31:06 naddy Exp $	*/
d679 1
a679 1
		tsleep((caddr_t)vp, PINOD, "vget", 0);
d1085 1
a1085 1
		wakeup((caddr_t)vp);
d1146 1
a1146 1
		tsleep((caddr_t)vp, PINOD, "vgone", 0);
d1476 1
a1476 1
			if ((error = copyout((caddr_t)&vp,
d1479 1
a1479 1
			   (error = copyout((caddr_t)vp,
d1561 1
a1561 1
	bzero((caddr_t)np, i);
d1563 1
a1563 1
	error = copyin(argp->ex_addr, (caddr_t)saddr, argp->ex_addrlen);
d1570 1
a1570 1
		error = copyin(argp->ex_mask, (caddr_t)smask, argp->ex_masklen);
d1621 1
a1621 1
	free((caddr_t)rn, M_NETADDR);
d1638 1
a1638 1
			free((caddr_t)rnh, M_RTABLE);
d1939 1
a1939 1
		error = tsleep((caddr_t)&vp->v_numoutput,
d1965 1
a1965 1
			wakeup((caddr_t)&vp->v_numoutput);
d2020 2
a2021 3
				error = tsleep((caddr_t)bp,
					slpflag | (PRIBIO + 1), "vinvalbuf",
					slptimeo);
@


1.93
log
@Back out previous change that causes "vnode table full" for large-scale
file operations.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.92 2003/05/13 02:30:01 tedu Exp $	*/
d21 1
a21 5
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
@


1.92
log
@do reclaim LAYER vnodes, no good reason not to
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.91 2003/05/06 20:52:14 tedu Exp $	*/
d418 2
@


1.91
log
@attempt to put a process's cwd back in place after a forced umount.
won't always work, but it's the best we can do for now.  this covers
at least some of the failure cases the previous commit to vfs_lookup.c
checks for.
ok weingart@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.90 2003/05/01 21:13:05 tedu Exp $	*/
a417 2
				if ((vp->v_flag & VLAYER) == 0)
					break;
@


1.90
log
@several related changes:
vfs_subr.c:
	add a missing simple_lock_init for vnode interlock
	try to avoid reclaiming locked or layered vnodes
	initialize vnlock pointer to NULL
	remove old code to free vnlock, never used
	lockinit the new vnode lock
vfs_syscalls.c:
	support for VLAYER flag
vnode_if.sh:
	support for splitting VDESC flags
vnode_if.src:
	split VDESC flags
	WILLPUT is the combination of WILLRELE and WILLUNLOCK
	most uses for WILLRELE become WILLPUT
vnode.h:
	add v_lock to struct vnode
	add VLAYER flag
	update for new VDESC flags
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.89 2003/04/06 18:54:20 ho Exp $	*/
d1774 1
a1774 1
		if ((error = dounmount(mp, MNT_FORCE, curproc)) != 0) {
@


1.89
log
@strcat/strcpy/sprintf cleanup. krw@@, anil@@ ok. art@@ tested sparc64.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.88 2002/08/11 22:32:31 art Exp $	*/
d412 1
d417 8
a424 2
			if (simple_lock_try(&vp->v_interlock))
				break;
d467 2
d645 2
a1073 6
	if (vp->v_vnlock) {
		if ((vp->v_vnlock->lk_flags & LK_DRAINED) == 0)
			vprint("vclean: lock not drained", vp);
		FREE(vp->v_vnlock, M_VNODE);
		vp->v_vnlock = NULL;
	}
@


1.88
log
@Add two missing vfs_busy calls in the failure path of sysctl_vnode.
Found by aaron@@

NOTE - I think we need a mount-point iterator just like we have
NOTE - vfs_mount_foreach_vnode. (btw. why don't we use foreach_vnode in here?)
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.87 2002/07/12 14:02:22 art Exp $	*/
d1317 1
a1317 1
		strcat(buf, "|VROOT");
d1319 1
a1319 1
		strcat(buf, "|VTEXT");
d1321 1
a1321 1
		strcat(buf, "|VSYSTEM");
d1323 1
a1323 1
		strcat(buf, "|VXLOCK");
d1325 1
a1325 1
		strcat(buf, "|VXWANT");
d1327 1
a1327 1
		strcat(buf, "| VBIOWAIT");
d1329 1
a1329 1
		strcat(buf, "|VALIASED");
@


1.87
log
@Change the locking on the mountpoint slightly. Instead of using mnt_lock
to get shared locks for lookup and get the exclusive lock only with
LK_DRAIN on unmount and do the real exclusive locking with flags in
mnt_flags, we now use shared locks for lookup and an exclusive lock for
unmount.

This is accomplished by slightly changing the semantics of vfs_busy.
Old vfs_busy behavior:
 - with LK_NOWAIT set in flags, a shared lock was obtained if the
   mountpoint wasn't being unmounted, otherwise we just returned an error.
 - with no flags, a shared lock was obtained if the mountpoint was being
   unmounted, otherwise we slept until the unmount was done and returned
   an error.
LK_NOWAIT was used for sync(2) and some statistics code where it isn't really
critical that we get the correct results.
0 was used in fchdir and lookup where it's critical that we get the right
directory vnode for the filesystem root.

After this change vfs_busy keeps the same behavior for no flags and LK_NOWAIT.
But if some other flags are passed into it, they are passed directly
into lockmgr (actually LK_SLEEPFAIL is always added to those flags because
if we sleep for the lock, that means someone was holding the exclusive lock
and the exclusive lock is only held when the filesystem is being unmounted.

More changes:
 dounmount must now be called with the exclusive lock held. (before this
 the caller was supposed to hold the vfs_busy lock, but that wasn't always
 true).
 Zap some (now) unused mount flags.
And the highlight of this change:
 Add some vfs_busy calls to match some vfs_unbusy calls, especially in
 sys_mount. (lockmgr doesn't detect the case where we release a lock noone
 holds (it will do that soon)).

If you've seen hangs on reboot with mfs this should solve it (I repeat this
for the fourth time now, but this time I spent two months fixing and
redesigning this and reading the code so this time I must have gotten
this right).
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.86 2002/06/16 16:54:25 miod Exp $	*/
d1472 1
d1480 2
a1481 1
			    sizeof(struct vnode))))
d1483 1
@


1.86
log
@When processing the KERN_VNODE sysctl, the kernel builds a packed structure,
while pstat(8) expects a C structure abiding the regular structure packing
rules. This caused pstat -v to break on powerpc.

Unbreak the confusion by defining the structure in a common header file,
and having the kernel use it.

ok millert@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.85 2002/06/08 18:36:45 art Exp $	*/
a148 1

d152 6
d161 2
a162 5
vfs_busy(mp, flags, interlkp, p)
	struct mount *mp;
	int flags;
	struct simplelock *interlkp;
	struct proc *p;
d166 9
a174 12
	if (mp->mnt_flag & MNT_UNMOUNT) {
		if (flags & LK_NOWAIT)
			return (ENOENT);
		mp->mnt_flag |= MNT_MWAIT;
		/*
		 * Since all busy locks are shared except the exclusive
		 * lock granted when unmounting, the only place that a
		 * wakeup needs to be done is at the release of the
		 * exclusive lock at the end of dounmount.
		 */
		ltsleep(mp, PVFS, "vfs_bsy", 0, interlkp);
		return (ENOENT);
d176 7
a182 1
	lkflags = LK_SHARED;
d186 1
a186 1
		panic("vfs_busy: unexpected lock failure");
d195 1
a195 3
vfs_unbusy(mp, p)
	struct mount *mp;
	struct proc *p;
d1753 1
a1753 1
vfs_unmountall()
d1755 1
a1755 1
	register struct mount *mp, *nmp;
d1757 1
d1764 2
@


1.85
log
@Use ltsleep in vfs_busy.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.84 2002/05/16 00:03:05 art Exp $	*/
a1435 2
#define VPTRSZ	sizeof (struct vnode *)
#define VNODESZ	sizeof (struct vnode)
d1437 1
a1437 1
		*sizep = (numvnodes + KINFO_VNODESLOP) * (VPTRSZ + VNODESZ);
d1466 1
a1466 1
			if (bp + VPTRSZ + VNODESZ > ewhere) {
d1471 6
a1476 2
			if ((error = copyout((caddr_t)&vp, bp, VPTRSZ)) ||
			   (error = copyout((caddr_t)vp, bp + VPTRSZ, VNODESZ)))
d1478 1
a1478 1
			bp += VPTRSZ + VNODESZ;
@


1.84
log
@sprinkle some splassert(IPL_BIO) in some functions that are commented as "should be called at splbio()"
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.83 2002/03/14 01:27:06 millert Exp $	*/
a167 2
		if (interlkp)
			simple_unlock(interlkp);
d174 1
a174 3
 		sleep((caddr_t)mp, PVFS);
		if (interlkp)
			simple_lock(interlkp);
@


1.83
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.82 2002/02/04 19:38:20 miod Exp $	*/
d1927 2
d1950 2
d2096 2
d2124 2
d2177 2
d2205 2
@


1.82
log
@Cleanup mountroot-related definitions.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.81 2002/01/23 00:39:48 art Exp $	*/
d104 1
a104 1
void	vclean __P((struct vnode *, int, struct proc *));
d106 2
a107 2
void insmntque __P((struct vnode *, struct mount *));
int getdevvp __P((dev_t, struct vnode **, enum vtype));
d109 5
a113 5
int vfs_hang_addrlist __P((struct mount *, struct netexport *,
				  struct export_args *));
int vfs_free_netcred __P((struct radix_node *, void *));
void vfs_free_addrlist __P((struct netexport *));
static __inline__ void vputonfreelist __P((struct vnode *));
d118 1
a118 1
void printlockedvnodes __P((void));
d366 1
a366 1
extern int (**dead_vnodeop_p) __P((void *));
d376 1
a376 1
	int (**vops) __P((void *));
d833 1
a833 1
void vhold __P((struct vnode *vp));
@


1.81
log
@Pool deals fairly well with physical memory shortage, but it doesn't deal
well (not at all) with shortages of the vm_map where the pages are mapped
(usually kmem_map).

Try to deal with it:
 - group all information the backend allocator for a pool in a separate
   struct. The pool will only have a pointer to that struct.
 - change the pool_init API to reflect that.
 - link all pools allocating from the same allocator on a linked list.
 - Since an allocator is responsible to wait for physical memory it will
   only fail (waitok) when it runs out of its backing vm_map, carefully
   drain pools using the same allocator so that va space is freed.
   (see comments in code for caveats and details).
 - change pool_reclaim to return if it actually succeeded to free some
   memory, use that information to make draining easier and more efficient.
 - get rid of PR_URGENT, noone uses it.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.80 2001/12/19 08:58:06 art Exp $	*/
a256 1
	extern int (*mountroot)(void);
@


1.80
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.79 2001/12/10 18:47:16 art Exp $	*/
d134 1
a134 1
		0, pool_page_alloc_nointr, pool_page_free_nointr, M_VNODE);
@


1.79
log
@No need to initialize the uobj on every getnewvnode. Just do
it when allocating. Add some improved diagnostics.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.78 2001/12/10 04:45:31 art Exp $	*/
a379 2
	extern struct uvm_pagerops uvm_vnodeops;
	struct uvm_object *uobj;
d413 1
a413 8
		bzero(vp, sizeof *vp);
		/*
		 * initialize uvm_object within vnode.
		 */
		uobj = &vp->v_uobj;
		uobj->pgops = &uvm_vnodeops;
		uobj->uo_npages = 0;
		TAILQ_INIT(&uobj->memq);
d416 2
a417 1
		TAILQ_FOREACH(vp, listhd, v_freelist) {
d448 1
a448 2
		if (vp->v_data || vp->v_uobj.uo_npages ||
		    TAILQ_FIRST(&vp->v_uobj.memq)) {
d452 1
d455 1
d468 1
a468 4
	simple_lock_init(&vp->v_uobj.vmobjlock);

	vp->v_size = VSIZENOTSET;

a671 4
		if (flags & LK_NOWAIT) {
			simple_unlock(&vp->v_interlock);
			return (EBUSY);
		}
a789 5
	if (vp->v_flag & VTEXT) {
		uvmexp.vtextpages -= vp->v_uobj.uo_npages;
		uvmexp.vnodepages += vp->v_uobj.uo_npages;
	}
	vp->v_flag &= ~VTEXT;
a829 5
	if (vp->v_flag & VTEXT) {
		uvmexp.vtextpages -= vp->v_uobj.uo_npages;
		uvmexp.vnodepages += vp->v_uobj.uo_npages;
	}
	vp->v_flag &= ~VTEXT;
d834 2
a837 1
 * Must be called at splbio();
d840 2
a841 1
vhold(struct vnode *vp)
a860 28
 * Release a vhold reference.
 * Must be called at splbio();
 */
void
vholdrele(struct vnode *vp)
{
	simple_lock(&vp->v_interlock);
#ifdef DIAGNOSTIC
	if (vp->v_holdcnt == 0)
		panic("vholdrele: holdcnt");
#endif
	vp->v_holdcnt--;

	/*
	 * If it is on the holdlist and the hold count drops to
	 * zero, move it to the free list.
	 */
	if ((vp->v_bioflag & VBIOONFREELIST) &&
	    vp->v_holdcnt == 0 && vp->v_usecount == 0) {
		simple_lock(&vnode_free_list_slock);
		TAILQ_REMOVE(&vnode_hold_list, vp, v_freelist);
		TAILQ_INSERT_TAIL(&vnode_free_list, vp, v_freelist);
		simple_unlock(&vnode_free_list_slock);
	}
	simple_unlock(&vp->v_interlock);
}

/*
a1011 6
	if (vp->v_flag & VTEXT) {
		uvmexp.vtextpages -= vp->v_uobj.uo_npages;
		uvmexp.vnodepages += vp->v_uobj.uo_npages;
	}
	vp->v_flag &= ~VTEXT;

d1022 5
a1026 1
	 * Clean out any cached data associated with the vnode.
d1971 1
a1971 2
	struct uvm_object *uobj = &vp->v_uobj;
	struct buf *bp;
d1973 1
a1973 13
	int s, error, rv;
	int flushflags = PGO_ALLPAGES|PGO_FREE|PGO_SYNCIO|
	    (flags & V_SAVE ? PGO_CLEANIT : 0);

	/* XXXUBC this doesn't look at flags or slp* */
	if (vp->v_type == VREG) {
		simple_lock(&uobj->vmobjlock);
		rv = (uobj->pgops->pgo_flush)(uobj, 0, 0, flushflags);
		simple_unlock(&uobj->vmobjlock);
		if (!rv) {
			return EIO;
		}
	}
d2043 1
a2043 1
	struct vnode *vp;
d2046 1
a2046 2
	struct uvm_object *uobj = &vp->v_uobj;
	struct buf *bp, *nbp;
a2048 8
	if (vp->v_type == VREG) {
		int flags = PGO_CLEANIT|PGO_ALLPAGES| (sync ? PGO_SYNCIO : 0);

		simple_lock(&uobj->vmobjlock);
		(uobj->pgops->pgo_flush)(uobj, 0, 0, flags);
		simple_unlock(&uobj->vmobjlock);
	}

d2114 2
a2115 1
brelvp(struct buf *bp)
d2119 1
a2119 1
	if ((vp = bp->b_vp) == NULL)
a2120 1

d2126 1
a2126 2
	if (TAILQ_EMPTY(&vp->v_uobj.memq) &&
	    (vp->v_bioflag & VBIOONSYNCLIST) &&
d2131 8
a2138 1
	bp->b_vp = NULL;
d2140 12
a2151 1
	vholdrele(vp);
d2208 1
a2208 2
		if (TAILQ_EMPTY(&vp->v_uobj.memq) &&
		    (vp->v_bioflag & VBIOONSYNCLIST) &&
@


1.79.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.81 2002/01/23 00:39:48 art Exp $	*/
d134 1
a134 1
	    &pool_allocator_nointr);
@


1.79.2.2
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.79.2.1 2002/01/31 22:55:41 niklas Exp $	*/
d2026 1
a2026 1
	int s, error;
d2031 1
a2031 1
	if (TAILQ_FIRST(&uobj->memq)) {
d2033 1
a2033 1
		error = (uobj->pgops->pgo_put)(uobj, 0, 0, flushflags);
d2035 2
a2036 2
		if (error) {
			return error;
d2115 1
a2115 1
	if (TAILQ_FIRST(&uobj->memq)) {
d2119 2
a2120 1
		(uobj->pgops->pgo_put)(uobj, 0, 0, flags);
@


1.79.2.3
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.79.2.2 2002/02/02 03:28:25 art Exp $	*/
d104 1
a104 1
void	vclean(struct vnode *, int, struct proc *);
d106 2
a107 2
void insmntque(struct vnode *, struct mount *);
int getdevvp(dev_t, struct vnode **, enum vtype);
d109 5
a113 5
int vfs_hang_addrlist(struct mount *, struct netexport *,
				  struct export_args *);
int vfs_free_netcred(struct radix_node *, void *);
void vfs_free_addrlist(struct netexport *);
static __inline__ void vputonfreelist(struct vnode *);
d118 1
a118 1
void printlockedvnodes(void);
d168 2
d176 3
a178 1
		ltsleep(mp, PVFS, "vfs_bsy", 0, interlkp);
d257 1
d367 1
a367 1
extern int (**dead_vnodeop_p)(void *);
d377 1
a377 1
	int (**vops)(void *);
a1979 2
	splassert(IPL_BIO);

a2000 2
	splassert(IPL_BIO);

a2165 2
	splassert(IPL_BIO);

a2190 2
	splassert(IPL_BIO);

a2225 2
	splassert(IPL_BIO);

a2251 2

	splassert(IPL_BIO);
@


1.79.2.4
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.79.2.3 2002/06/11 03:29:40 art Exp $	*/
d149 1
a152 6
 *
 * historical behavior:
 *  - LK_NOWAIT means that we should just ignore the mount point if it's
 *     being unmounted.
 *  - no flags means that we should sleep on the mountpoint and then
 *     fail.
d156 5
a160 2
vfs_busy(struct mount *mp, int flags, struct simplelock *interlkp,
    struct proc *p)
d164 12
a175 9
	switch (flags) {
	case LK_NOWAIT:
		lkflags = LK_SHARED|LK_NOWAIT;
		break;
	case 0:
		lkflags = LK_SHARED;
		break;
	default:
		lkflags = flags;
d177 1
a177 7

	/*
	 * Always sleepfail. We will only sleep for an exclusive lock
	 * and the exclusive lock will only be acquired when unmounting.
	 */
	lkflags |= LK_SLEEPFAIL;

d181 1
a181 1
		return (ENOENT);
d190 3
a192 1
vfs_unbusy(struct mount *mp, struct proc *p)
d1488 2
d1491 1
a1491 1
		*sizep = (numvnodes + KINFO_VNODESLOP) * sizeof(struct e_vnode);
d1520 1
a1520 1
			if (bp + sizeof(struct e_vnode) > ewhere) {
a1522 1
				vfs_unbusy(mp, p);
d1525 2
a1526 7
			if ((error = copyout((caddr_t)&vp,
			    &((struct e_vnode *)bp)->vptr,
			    sizeof(struct vnode *))) ||
			   (error = copyout((caddr_t)vp,
			    &((struct e_vnode *)bp)->vnode,
			    sizeof(struct vnode)))) {
				vfs_unbusy(mp, p);
d1528 1
a1528 2
			}
			bp += sizeof(struct e_vnode);
d1800 1
a1800 1
vfs_unmountall(void)
d1802 1
a1802 1
	struct mount *mp, *nmp;
a1803 1
	struct proc *p = curproc;
a1809 2
		if ((vfs_busy(mp, LK_EXCLUSIVE|LK_NOWAIT, NULL, p)) != 0)
			continue;
@


1.79.2.5
log
@Huge sync to NetBSD plus lots of bugfixes.
 - uvm is as in netbsd-current minus uvm_map forward merge.
 - various locking bugfixes in nfs.
 - make sure that all specops and fifoops are correct in all vnodeop vectors.
 - make the filesystem code more like filsystem code and less like vm code.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.79.2.4 2002/10/29 00:36:44 art Exp $	*/
d803 2
a804 2
		uvmexp.execpages -= vp->v_uobj.uo_npages;
		uvmexp.filepages += vp->v_uobj.uo_npages;
d848 2
a849 2
		uvmexp.execpages -= vp->v_uobj.uo_npages;
		uvmexp.filepages += vp->v_uobj.uo_npages;
d1061 2
a1062 2
		uvmexp.execpages -= vp->v_uobj.uo_npages;
		uvmexp.filepages += vp->v_uobj.uo_npages;
d2033 1
d2041 7
a2047 4
	simple_lock(&vp->v_interlock);
	error = VOP_PUTPAGES(vp, 0, 0, flushflags);
	if (error) {
		return error;
d2121 1
a2122 1
	int flags = PGO_CLEANIT | PGO_ALLPAGES | (sync ? PGO_SYNCIO : 0);
d2125 6
a2130 2
	simple_lock(&vp->v_interlock);
	VOP_PUTPAGES(vp, 0, 0, flags);
@


1.79.2.6
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d414 3
a416 1
		simple_lock_init(&vp->v_interlock);
d423 3
a425 10
		for (vp = TAILQ_FIRST(listhd); vp != NULLVP;
		    vp = TAILQ_NEXT(vp, v_freelist)) {
			if (simple_lock_try(&vp->v_interlock)) {
				if ((vp->v_flag & VLAYER) == 0)
					break;
				if (VOP_ISLOCKED(vp) == 0)
					break;
				else
					simple_unlock(&vp->v_interlock);
			}
a466 2
	vp->v_vnlock = NULL;
	lockinit(&vp->v_lock, PVFS, "v_lock", 0, 0);
a645 2
	vp->v_vnlock = NULL;
	lockinit(&vp->v_lock, PVFS, "v_lock", 0, 0);
d1115 6
d1369 1
a1369 1
		strlcat(buf, "|VROOT", sizeof buf);
d1371 1
a1371 1
		strlcat(buf, "|VTEXT", sizeof buf);
d1373 1
a1373 1
		strlcat(buf, "|VSYSTEM", sizeof buf);
d1375 1
a1375 1
		strlcat(buf, "|VXLOCK", sizeof buf);
d1377 1
a1377 1
		strlcat(buf, "|VXWANT", sizeof buf);
d1379 1
a1379 1
		strlcat(buf, "| VBIOWAIT", sizeof buf);
d1381 1
a1381 1
		strlcat(buf, "|VALIASED", sizeof buf);
d1821 1
a1821 1
		if ((error = dounmount(mp, MNT_FORCE, curproc, NULL)) != 0) {
@


1.79.2.7
log
@add VEXECMAP.  also make sure to modify filepages count only in the not
execpages case in uvm_pageremove().
this actually appears to solve the swap freak out problems.  sitting on it for
a long time, never checked if it worked.  sigh.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.79.2.6 2003/05/19 22:31:57 tedu Exp $	*/
d811 1
a811 1
	if (vp->v_flag & VEXECMAP) {
d815 1
a815 1
	vp->v_flag &= ~(VTEXT|VEXECMAP);
d817 1
d856 1
a856 1
	if (vp->v_flag & VEXECMAP) {
d860 1
a860 1
	vp->v_flag &= ~(VTEXT|VEXECMAP);
d1069 1
a1069 1
	if (vp->v_flag & VEXECMAP) {
d1073 1
a1073 1
	vp->v_flag &= ~(VTEXT|VEXECMAP);
a1374 2
	if (vp->v_flag & VEXECMAP)
		strlcat(buf, "|VEXECMAP", sizeof buf);
@


1.78
log
@Big cleanup inspired by NetBSD with some parts of the code from NetBSD.
 - get rid of VOP_BALLOCN and VOP_SIZE
 - move the generic getpages and putpages into miscfs/genfs
 - create a genfs_node which must be added to the top of the private portion
   of each vnode for filsystems that want to use genfs_{get,put}pages
 - rename genfs_mmap to vop_generic_mmap
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.77 2001/12/10 02:19:34 art Exp $	*/
d416 7
d425 1
a425 2
		for (vp = TAILQ_FIRST(listhd); vp != NULLVP;
		    vp = TAILQ_NEXT(vp, v_freelist)) {
d456 2
a457 1
		if (vp->v_data) {
a460 1
		s = splbio();
a462 1
		splx(s);
a476 7
	/*
	 * initialize uvm_object within vnode.
	 */

	uobj = &vp->v_uobj;
	uobj->pgops = &uvm_vnodeops;
	TAILQ_INIT(&uobj->memq);
@


1.77
log
@Merge in struct uvm_vnode into struct vnode.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.76 2001/12/05 00:24:36 art Exp $	*/
a462 1
	lockinit(&vp->v_glock, PVFS, "glock", 0, 0);
@


1.76
log
@Break out the part that lowers v_holdcnt in brelvp into an own function
and make it and vhold into public interfaces.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.75 2001/11/29 02:54:10 art Exp $	*/
d471 1
a471 1
	simple_lock_init(&vp->v_uvm.u_obj.vmobjlock);
d477 1
a477 1
	uobj = &vp->v_uvm.u_obj;
d480 1
a480 1
	vp->v_uvm.u_size = VSIZENOTSET;
d808 2
a809 2
		uvmexp.vtextpages -= vp->v_uvm.u_obj.uo_npages;
		uvmexp.vnodepages += vp->v_uvm.u_obj.uo_npages;
d853 2
a854 2
		uvmexp.vtextpages -= vp->v_uvm.u_obj.uo_npages;
		uvmexp.vnodepages += vp->v_uvm.u_obj.uo_npages;
d1066 2
a1067 2
		uvmexp.vtextpages -= vp->v_uvm.u_obj.uo_npages;
		uvmexp.vnodepages += vp->v_uvm.u_obj.uo_npages;
d2026 1
a2026 1
	struct uvm_object *uobj = &vp->v_uvm.u_obj;
d2114 1
a2114 1
	struct uvm_object *uobj = &vp->v_uvm.u_obj;
d2203 1
a2203 1
	if (TAILQ_EMPTY(&vp->v_uvm.u_obj.memq) &&
d2268 1
a2268 1
		if (TAILQ_EMPTY(&vp->v_uvm.u_obj.memq) &&
@


1.75
log
@Ooops. Revert part of the last commit that was completly wrong and wasn't supposed to be committed.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.74 2001/11/29 01:58:57 art Exp $	*/
a860 2
void vhold __P((struct vnode *vp));

d863 1
d866 1
a866 2
vhold(vp)
	register struct vnode *vp;
d886 28
d2191 1
a2191 2
brelvp(bp)
	struct buf *bp;
d2211 1
a2211 19
	simple_lock(&vp->v_interlock);
#ifdef DIAGNOSTIC
	if (vp->v_holdcnt == 0)
		panic("brelvp: holdcnt");
#endif
	vp->v_holdcnt--;

	/*
	 * If it is on the holdlist and the hold count drops to
	 * zero, move it to the free list.
	 */
	if ((vp->v_bioflag & VBIOONFREELIST) &&
	    vp->v_holdcnt == 0 && vp->v_usecount == 0) {
		simple_lock(&vnode_free_list_slock);
		TAILQ_REMOVE(&vnode_hold_list, vp, v_freelist);
		TAILQ_INSERT_TAIL(&vnode_free_list, vp, v_freelist);
		simple_unlock(&vnode_free_list_slock);
	}
	simple_unlock(&vp->v_interlock);
@


1.74
log
@Correctly handle b_vp with bgetvp and brelvp in {get,put}pages.
Prevents panics caused by vnodes being recycled under our feet.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.73 2001/11/27 05:27:12 art Exp $	*/
d2003 1
a2003 1
	int s, error;
d2010 1
a2010 1
		error = (uobj->pgops->pgo_flush)(uobj, 0, 0, flushflags);
d2012 2
a2013 2
		if (error) {
			return error;
@


1.73
log
@Merge in the unified buffer cache code as found in NetBSD 2001/03/10. The
code is written mostly by Chuck Silvers <chuq@@chuq.com>/<chs@@netbsd.org>.

Tested for the past few weeks by many developers, should be in a pretty stable
state, but will require optimizations and additional cleanups.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.72 2001/11/21 21:13:34 csapuntz Exp $	*/
d2003 1
a2003 1
	int s, error, rv;
d2010 1
a2010 1
		rv = (uobj->pgops->pgo_flush)(uobj, 0, 0, flushflags);
d2012 2
a2013 2
		if (!rv) {
			return EIO;
@


1.72
log
@Added vfs_isbusy. Useful for verifying that a mount point is locked
Added vfs_mount_foreach_vnode. Several places in the code seem to want to
traverse the mount list and they all seem to handle locking differently.
Centralize traversing the mount list in one place so that we only need
to get the locking right once.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.71 2001/11/15 06:40:39 art Exp $	*/
d380 2
d415 1
a415 1
		bzero((char *)vp, sizeof *vp);
d428 1
a428 1
		if (vp == NULLVP) {
d463 1
d472 10
d685 4
d807 5
d852 5
d1039 6
d1055 1
a1055 5
	 * clean out any VM data associated with the vnode.
	 */
	uvm_vnp_terminate(vp);
	/*
	 * Clean out any buffers associated with the vnode.
d2000 2
a2001 1
	register struct buf *bp;
d2003 13
a2015 1
	int s, error;
d2085 1
a2085 1
	register struct vnode *vp;
d2088 2
a2089 1
	register struct buf *bp, *nbp;
d2092 8
d2166 1
a2166 1
	register struct buf *bp;
d2170 1
a2170 1
	if ((vp = bp->b_vp) == (struct vnode *) 0)
d2172 1
d2178 2
a2179 1
	if ((vp->v_bioflag & VBIOONSYNCLIST) &&
d2184 1
a2184 1
	bp->b_vp = (struct vnode *) 0;
d2261 2
a2262 1
		if ((vp->v_bioflag & VBIOONSYNCLIST) &&
@


1.71
log
@Don't zero v_bioflag when recycling a vnode in getnewvnode.
Sometimes the vnode can be on the syncers list. While that is a bug, it's
just a minor annoyance. A vnode on a syncer worklist without VBIOONSYNCLIST
set is a disaster.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.70 2001/11/12 23:05:52 art Exp $	*/
d115 2
d201 6
d874 4
a877 8
vflush(mp, skipvp, flags)
	struct mount *mp;
	struct vnode *skipvp;
	int flags;
{
	struct proc *p = curproc;
	register struct vnode *vp, *nvp;
	int busy = 0;
d885 46
a930 5
		/*
		 * Skip over a selected vnode.
		 */
		if (vp == skipvp)
			continue;
d932 16
a947 23
		simple_lock(&vp->v_interlock);
		/*
		 * Skip over a vnodes marked VSYSTEM.
		 */
		if ((flags & SKIPSYSTEM) && (vp->v_flag & VSYSTEM)) {
			simple_unlock(&vp->v_interlock);
			continue;
		}
		/*
		 * If WRITECLOSE is set, only flush out regular file
		 * vnodes open for writing.
		 */
		if ((flags & WRITECLOSE) &&
		    (vp->v_writecount == 0 || vp->v_type != VREG)) {
			simple_unlock(&vp->v_interlock);
			continue;
		}
		/*
		 * With v_usecount == 0, all we need to do is clear
		 * out the vnode data structures and we are done.
		 */
		if (vp->v_usecount == 0) {
			simple_unlock(&mntvnode_slock);
d949 4
a952 19
			simple_lock(&mntvnode_slock);
			continue;
		}
		/*
		 * If FORCECLOSE is set, forcibly close the vnode.
		 * For block or character devices, revert to an
		 * anonymous device. For all other files, just kill them.
		 */
		if (flags & FORCECLOSE) {
			simple_unlock(&mntvnode_slock);
			if (vp->v_type != VBLK && vp->v_type != VCHR) {
				vgonel(vp, p);
			} else {
				vclean(vp, 0, p);
				vp->v_op = spec_vnodeop_p;
				insmntque(vp, (struct mount *)0);
			}
			simple_lock(&mntvnode_slock);
			continue;
d954 3
d958 2
a959 2
		if (busyprt)
			vprint("vflush: busy vnode", vp);
d961 19
a979 5
		simple_unlock(&vp->v_interlock);
		busy++;
	}
	simple_unlock(&mntvnode_slock);
	if (busy)
@


1.70
log
@Remove unnecessary check for NULL vnode in reassignbuf.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.69 2001/11/06 19:53:20 miod Exp $	*/
a449 1
		vp->v_bioflag = 0;
@


1.69
log
@Replace inclusion of <vm/foo.h> with the correct <uvm/bar.h> when necessary.
(Look ma, I might have broken the tree)
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.68 2001/10/02 17:21:02 csapuntz Exp $	*/
d2152 1
a2152 1
	register struct buf *bp;
a2157 4
	if (vp == NULL) {
		printf("reassignbuf: NULL");
		return;
	}
@


1.68
log
@

Bounds check index into routing table. Thanks to Ken Ashcraft of Stanford
for finding this bug.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.67 2001/09/19 22:52:41 csapuntz Exp $	*/
d67 1
a67 1
#include <vm/vm.h>
a70 2

#include <uvm/uvm_extern.h>
@


1.67
log
@Get rid of B_VFLUSH. Not relevant after the end of the AGE queue.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.66 2001/09/16 00:42:44 millert Exp $	*/
d1536 4
@


1.66
log
@Add some missing lengths checks when passing data from userland to
kernel.  From based on NetBSD patches.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.65 2001/08/02 08:16:45 assar Exp $	*/
d1977 2
a1978 1
			bp->b_flags |= B_BUSY | B_VFLUSH;
d2016 2
a2017 1
		bp->b_flags |= B_BUSY | B_VFLUSH;
@


1.65
log
@(vput): make panic strings actually say vput instead of vrele
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.64 2001/07/26 22:27:45 miod Exp $	*/
d1516 2
@


1.64
log
@Typo.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.63 2001/06/27 04:49:48 art Exp $	*/
d766 2
a767 2
		vprint("vrele: bad ref count", vp);
		panic("vrele: ref cnt");
d779 2
a780 2
		vprint("vrele: bad writecount", vp);
		panic("vrele: v_writecount != 0");
@


1.63
log
@remove old vm
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.62 2001/06/22 14:14:10 deraadt Exp $	*/
d624 1
a624 1
	 * craeted in ffs_mountroot.
@


1.62
log
@KNF
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.61 2001/06/05 21:47:07 provos Exp $	*/
a71 1
#if defined(UVM)
a72 2
#endif

a462 1
#ifdef UVM
a463 1
#endif
a983 1
#ifdef UVM
a987 1
#endif
a1748 8
		/* Release inodes held by texts before update. */
#if !defined(UVM)
		vnode_pager_umount(NULL);
#endif
#ifdef notdef
		vnshutdown();
#endif

@


1.61
log
@send note_revoke to knotes when vnode goes away, okay art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.60 2001/05/16 13:54:37 art Exp $	*/
d97 1
a97 1
}  
d189 1
a189 1
        return (0);
d220 1
a220 1
 
d256 1
a256 1
 
d268 1
a268 1
 
d406 1
a406 1
	     ((TAILQ_FIRST(listhd = &vnode_hold_list) == NULL) || toggle))) {
d414 1
a414 1
		     vp = TAILQ_NEXT(vp, v_freelist)) {
d484 1
a484 1
	
d602 1
a602 2
	
	/* 
d623 1
a623 1
	/* 
d630 1
a630 1
	 * 
d655 1
a655 1
        struct vnode *vp;
d723 1
a723 2
        struct vnode *vp;

d725 1
a725 1
        int s;
d732 1
a732 1
	
d741 1
a741 1
	if (vp->v_holdcnt > 0) 
d745 1
a745 1
	
d770 1
a770 1
	if (vp->v_usecount == 0) { 
d787 1
a787 1
#endif	
d811 1
a811 1
	if (vp->v_usecount == 0) { 
d894 1
a894 1
		
d1130 1
a1130 1
	 * If special device, remove it from special device alias list 
d1169 1
a1169 1
	 * move it to the head of the list. 
d1180 2
a1181 1
                simple_lock(&vnode_free_list_slock);
d1192 1
a1192 1
                simple_unlock(&vnode_free_list_slock);
d1327 2
a1328 2
	     mp = nmp) {
		if (vfs_busy(mp, LK_NOWAIT, &mountlist_slock, p)) { 
d1332 2
a1333 3
		for (vp = mp->mnt_vnodelist.lh_first;
		     vp != NULL;
		     vp = vp->v_mntvnodes.le_next) {
d1418 1
a1418 1
		
d1421 1
a1421 1
	     mp = nmp) {
d1428 2
a1429 3
		for (vp = mp->mnt_vnodelist.lh_first;
		     vp != NULL;
		     vp = nvp) {
d1644 1
a1644 1
							      rnh);
d1672 1
a1672 1
	
d1676 1
a1676 1
	
d1678 1
a1678 1
	
d1689 1
a1689 1
	
d1700 1
a1700 1
	
d1725 1
a1725 1
	     mp = nmp) {
d1792 1
a1792 1
 
d1824 1
a1824 1
	}   
d1883 2
a1884 2
/* 
 * Routines dealing with vnodes and buffers 
d1964 1
a1964 1
		if ((blist = vp->v_cleanblkhd.lh_first) && 
d2080 1
a2080 1
 * 
d2112 1
a2112 1
	 * zero, move it to the free list. 
d2142 1
a2142 1
		
d2223 2
a2224 3
	for (vfspp = &vfsconf, vfsp = vfsconf;
	     vfsp;
	     vfspp = &vfsp->vfc_next, vfsp = vfsp->vfc_next)
d2242 1
a2242 1
 
d2252 2
a2253 3
	for (vfspp = &vfsconf, vfsp = vfsconf;
	     vfsp;
	     vfspp = &vfsp->vfc_next, vfsp = vfsp->vfc_next) {
d2258 1
a2258 1
	if (!vfsp)                         /* Not found */
d2261 1
a2261 1
	if (vfsp->vfc_refcount)            /* In use */
d2264 1
a2264 1
	/* Remove from list and free  */
a2273 1

@


1.60
log
@indentation nit.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.59 2001/04/29 20:42:45 art Exp $	*/
d124 3
d1048 3
@


1.59
log
@cleanup, remove incorrect comment
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.58 2001/03/22 00:31:56 art Exp $	*/
a397 1

@


1.58
log
@Use pool for allocating vnodes.
Even though vnodes are never freed (could be) this gives us big memory and
kmem_map savings.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.57 2001/03/21 17:05:29 art Exp $	*/
a719 4

/*
 * Must be called at splbio
 */
d734 2
a735 2
		vprint ("vnode already on free list: ", vp);
		panic ("vnode already on free list");
a738 1

a744 1

@


1.58.2.1
log
@MFS (from millert):
Add some missing lengths checks when passing data from userland to
kernel.  From based on NetBSD patches.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.58 2001/03/22 00:31:56 art Exp $	*/
a1526 2
	if (argp->ex_addrlen > MLEN)
		return (EINVAL);
@


1.57
log
@uvm_vnp_terminate expect the vnode to be locked.
Why didn't LOCKDEBUG catch this?
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.56 2001/03/16 16:10:31 art Exp $	*/
d65 1
d124 2
d133 2
d407 1
a407 2
		vp = (struct vnode *)malloc((u_long)sizeof *vp,
		    M_VNODE, M_WAITOK);
@


1.56
log
@Oops. fix thinko in last.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.55 2001/03/16 16:05:28 art Exp $	*/
a981 6
#ifdef UVM
	/*
	 * clean out any VM data associated with the vnode.
	 */
	uvm_vnp_terminate(vp);
#endif
d991 6
@


1.55
log
@Use CIRCLEQ macros for mountlist.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.54 2001/03/16 15:51:04 art Exp $	*/
d1724 1
a1724 1
	for (mp = CIRCLEQ_FIRST(&mountlist); mp != CIRCLEQ_END(&mountlist);
@


1.54
log
@Initialize the mountlist_slock.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.53 2001/02/26 02:36:39 csapuntz Exp $	*/
d303 1
a303 1
	if (mountlist.cqh_first != (void *)&mountlist) {
d1324 1
a1324 1
	for (mp = CIRCLEQ_FIRST(&mountlist); mp != (void *)&mountlist;
d1419 2
a1420 1
	for (mp = mountlist.cqh_first; mp != (void *)&mountlist; mp = nmp) {
d1422 1
a1422 1
			nmp = mp->mnt_list.cqe_next;
d1457 1
a1457 1
		nmp = mp->mnt_list.cqe_next;
d1723 4
a1726 3
	for (allerror = 0,
	     mp = mountlist.cqh_last; mp != (void *)&mountlist; mp = nmp) {
		nmp = mp->mnt_list.cqe_prev;
@


1.53
log
@Move v_writecount test back to it original place
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.52 2001/02/26 00:18:33 csapuntz Exp $	*/
d137 1
@


1.52
log
@

Make ref counts 32-bit unsigned ints as opposed to a potpourri of longs and
ints.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.51 2001/02/24 19:07:08 csapuntz Exp $	*/
d771 3
a773 3
	if (vp->v_usecount == 0 || vp->v_writecount != 0) {
		vprint("vput: bad ref count", vp);
		panic("vput: ref cnt");
d782 7
a788 1
	
d812 1
a812 1
	if (vp->v_usecount == 0 || vp->v_writecount != 0) {
d823 6
@


1.51
log
@

Cleanup of vnode interface continues. Get rid of VHOLD/HOLDRELE.
Change VM/UVM to use buf_replacevnode to change the vnode associated
with a buffer.

Addition v_bioflag for flags written in interrupt handlers
(and read at splbio, though not strictly necessary)

Add vwaitforio and use it instead of a while loop of v_numoutput.

Fix race conditions when manipulation vnode free list
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.50 2001/02/23 14:42:37 csapuntz Exp $	*/
d708 1
a708 1
	if (vp->v_usecount <= 0)
d769 7
a781 6
#ifdef DIAGNOSTIC
	if (vp->v_usecount < 0 || vp->v_writecount != 0) {
		vprint("vput: bad ref count", vp);
		panic("vput: ref cnt");
	}
#endif
d805 6
a815 6
#ifdef DIAGNOSTIC
	if (vp->v_usecount < 0 || vp->v_writecount != 0) {
		vprint("vrele: bad ref count", vp);
		panic("vrele: ref cnt");
	}
#endif
d1269 1
a1269 1
	printf("type %s, usecount %d, writecount %d, holdcount %ld,",
d1878 1
a1878 1
vwaitforio(vp, slpflag, wchan, timeo)
d1881 1
a1881 1
	char *wchan;
d1888 1
a1888 1
		    slpflag | (PRIBIO + 1), wchan, timeo);
d1907 1
a1907 1
		if (--vp->v_numoutput < 0)
d1909 1
a1909 1
		if ((vp->v_bioflag & VBIOWAIT) && vp->v_numoutput <= 0) {
d2090 1
a2090 1
	if (vp->v_holdcnt <= 0)
a2092 1

@


1.50
log
@

Remove the clustering fields from the vnodes and place them in the
file system inode instead
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.49 2001/02/21 23:24:30 csapuntz Exp $	*/
d108 1
a371 1
#ifdef DIAGNOSTIC
a372 1
#endif
d395 1
d399 1
d417 1
d429 2
a430 1
		vp->v_flag &= ~VONFREELIST;
d448 1
d654 1
d671 3
a673 1
	if ((vp->v_flag & VONFREELIST) && (vp->v_usecount == 0)) {
d680 2
a681 1
		vp->v_flag &= ~VONFREELIST;
d715 4
d724 1
d727 1
a727 3
	/*
	 * insert at tail of LRU list
	 */
d729 1
a729 1
	if (vp->v_usecount != 0) {
d731 2
a732 3
	}

	if (vp->v_flag & VONFREELIST) {
a734 1
		return;
a737 1
	vp->v_flag |= VONFREELIST;
d739 1
a739 1
	simple_lock(&vnode_free_list_slock);
d750 1
a750 1
		TAILQ_INSERT_TAIL(lst, vp, v_freelist);		
d752 1
a752 1
	simple_unlock(&vnode_free_list_slock);
d781 1
d815 1
d822 2
a823 1
#ifdef DIAGNOSTIC
d837 1
a837 1
	if ((vp->v_flag & VONFREELIST) &&
a848 27
 * Page or buffer structure frees a reference.
 */
void
holdrele(vp)
	register struct vnode *vp;
{

	simple_lock(&vp->v_interlock);
	if (vp->v_holdcnt <= 0)
		panic("holdrele: holdcnt");
	vp->v_holdcnt--;
	/*
	 * If it is on the holdlist and the hold count drops to
	 * zero, move it to the free list. 
	 */
	if ((vp->v_flag & VONFREELIST) &&
	    vp->v_holdcnt == 0 && vp->v_usecount == 0) {
		simple_lock(&vnode_free_list_slock);
		TAILQ_REMOVE(&vnode_hold_list, vp, v_freelist);
		TAILQ_INSERT_TAIL(&vnode_free_list, vp, v_freelist);
		simple_unlock(&vnode_free_list_slock);
	}
	simple_unlock(&vp->v_interlock);
}
#endif /* DIAGNOSTIC */

/*
d1069 1
d1158 7
a1164 2
	if ((vp->v_flag & VONFREELIST) &&
	    vp->v_usecount == 0) {
d1166 2
d1169 7
a1175 5
			panic("vgonel: not clean");		
                if (TAILQ_FIRST(&vnode_free_list) != vp) {
                        TAILQ_REMOVE(&vnode_free_list, vp, v_freelist);
                        TAILQ_INSERT_HEAD(&vnode_free_list, vp, v_freelist);
                }
d1282 2
a1283 2
	if (vp->v_flag & VBWAIT)
		strcat(buf, "|VBWAIT");
d1872 25
d1902 2
a1903 2
vwakeup(bp)
	register struct buf *bp;
d1905 1
a1905 4
	register struct vnode *vp;

	bp->b_flags &= ~B_WRITEINPROG;
	if ((vp = bp->b_vp) != NULL) {
d1908 2
a1909 2
		if ((vp->v_flag & VBWAIT) && vp->v_numoutput <= 0) {
			vp->v_flag &= ~VBWAIT;
d1933 1
a1933 4
		while (vp->v_numoutput) {
			vp->v_flag |= VBWAIT;
			sleep((caddr_t)&vp->v_numoutput, PRIBIO + 1);
		}
d2028 1
a2028 5
	while (vp->v_numoutput) {
		vp->v_flag |= VBWAIT;
		tsleep((caddr_t)&vp->v_numoutput, PRIBIO + 1, "vflushbuf", 0);
	}
	splx(s);
d2030 1
d2034 1
d2050 1
a2050 1
	VHOLD(vp);
d2080 1
a2080 1
	if ((vp->v_flag & VONSYNCLIST) &&
d2082 1
a2082 1
		vp->v_flag &= ~VONSYNCLIST;
d2086 49
a2134 1
	HOLDRELE(vp);
d2138 3
a2140 3
 * Reassign a buffer from one vnode to another. Used to assign buffers
 * to the appropriate clean or dirty list and to add newly dirty vnodes
 * to the appropriate filesystem syncer list.
d2145 1
a2145 1
reassignbuf(bp, newvp)
a2146 1
	register struct vnode *newvp;
d2150 1
d2152 1
a2152 1
	if (newvp == NULL) {
d2166 5
a2170 5
		listheadp = &newvp->v_cleanblkhd;
		if ((newvp->v_flag & VONSYNCLIST) &&
		    LIST_FIRST(&newvp->v_dirtyblkhd) == NULL) {
			newvp->v_flag &= ~VONSYNCLIST;
			LIST_REMOVE(newvp, v_synclist);
d2173 3
a2175 3
		listheadp = &newvp->v_dirtyblkhd;
		if ((newvp->v_flag & VONSYNCLIST) == 0) {
			switch (newvp->v_type) {
d2180 1
a2180 1
				if (newvp->v_specmountpoint != NULL) {
d2188 1
a2188 1
			vn_syncer_add_to_worklist(newvp, delay);
@


1.49
log
@

Latest soft updates from FreeBSD/Kirk McKusick

Snapshot-related code has been commented out.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.48 2001/02/08 00:32:11 mickey Exp $	*/
d75 1
a429 1
		vp->v_lease = NULL;
a444 7
		vp->v_lastr = 0;
		vp->v_ralen = 0;
		vp->v_maxra = 0;
		vp->v_lastw = 0;
		vp->v_lasta = 0;
		vp->v_cstart = 0;
		vp->v_clen = 0;
@


1.48
log
@do not print stuff when not verbose
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.47 2000/09/27 09:37:16 art Exp $	*/
d2211 14
@


1.47
log
@Minimal optimization.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.46 2000/07/17 14:54:26 art Exp $	*/
d1810 2
a1811 1
					printf("softdep ");
@


1.47.2.1
log
@MFC:
Add some missing lengths checks when passing data from userland to the
kernel. From NetBSD via millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.66 2001/09/16 00:42:44 millert Exp $	*/
a1520 2
	if (argp->ex_addrlen > MLEN)
		return (EINVAL);
@


1.46
log
@Don't wait for B_READ buffers on shutdown.
From NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.45 2000/04/25 22:41:57 csapuntz Exp $	*/
d663 1
a663 1
	if ((flags & LK_INTERLOCK) == 0)
d665 2
d684 1
a684 1
		if ((error = vn_lock(vp, flags | LK_INTERLOCK, p)) != 0) {
@


1.45
log
@

Use CIRCLEQ_FOREACH
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.44 2000/04/21 16:33:12 mickey Exp $	*/
d1793 1
a1793 1
			if ((bp->b_flags & (B_BUSY|B_INVAL)) == B_BUSY)
@


1.44
log
@see if there is any meaning under curproc before using &proc0 in vfs_syncwait(); from art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.43 1999/12/05 07:54:44 art Exp $	*/
d268 1
a268 2
	for (mp = mountlist.cqh_first; mp != (void *)&mountlist;
	     mp = mp->mnt_list.cqe_next) {
@


1.43
log
@With soft updates, some buffers will be remarked as dirty after being written.
Handle this when syncing filesystems when unmounting.
From NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.41 1999/08/20 15:37:13 art Exp $	*/
d1784 1
d1786 2
a1787 1
	sys_sync(&proc0, (void *)0, (register_t *)0);
@


1.43.2.1
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.58 2001/03/22 00:31:56 art Exp $	*/
a64 1
#include <sys/pool.h>
a74 1

a106 1
void	vclean __P((struct vnode *, int, struct proc *));
a120 2
struct pool vnode_pool;

a127 2
	pool_init(&vnode_pool, sizeof(struct vnode), 0, 0, 0, "vnodes",
		0, pool_page_alloc_nointr, pool_page_free_nointr, M_VNODE);
a134 1
	simple_lock_init(&mountlist_slock);
d268 2
a269 1
	CIRCLEQ_FOREACH(mp, &mountlist, mnt_list) {
d301 1
a301 1
	if (!CIRCLEQ_EMPTY(&mountlist)) {
d371 1
d373 1
a395 1
	s = splbio();
a398 1
		splx(s);
d400 2
a401 1
		vp = pool_get(&vnode_pool, PR_WAITOK);
a415 1
			splx(s);
d427 1
a427 2
		vp->v_bioflag &= ~VBIOONFREELIST;
		splx(s);
d430 1
d446 7
a452 1
		vp->v_bioflag = 0;
a657 1
	int s;
d664 1
a664 1
	if ((flags & LK_INTERLOCK) == 0) {
a665 2
		flags |= LK_INTERLOCK;
	}
d672 1
a672 3
	if (vp->v_usecount == 0 &&
	    (vp->v_bioflag & VBIOONFREELIST)) {
		s = splbio();
d679 1
a679 2
		vp->v_bioflag &= ~VBIOONFREELIST;
		splx(s);
d683 1
a683 1
		if ((error = vn_lock(vp, flags, p)) != 0) {
d706 1
a706 1
	if (vp->v_usecount == 0)
a712 4

/*
 * Must be called at splbio
 */
a717 1
        int s;
d720 3
a722 1
	s = splbio();
d724 1
a724 1
	if (vp->v_usecount != 0)
d726 3
a728 2
	
	if (vp->v_bioflag & VBIOONFREELIST) {
d731 1
d735 1
d737 1
a737 1
	vp->v_bioflag |= VBIOONFREELIST;
d748 1
a748 1
		TAILQ_INSERT_TAIL(lst, vp, v_freelist);
d750 1
a750 1
	splx(s);
a766 7

#ifdef DIAGNOSTIC
	if (vp->v_usecount == 0) { 
		vprint("vrele: bad ref count", vp);
		panic("vrele: ref cnt");
	}
#endif
a772 1

d774 3
a776 3
	if (vp->v_writecount != 0) {
		vprint("vrele: bad writecount", vp);
		panic("vrele: v_writecount != 0");
d778 1
a778 1
#endif	
a800 6
#ifdef DIAGNOSTIC
	if (vp->v_usecount == 0) { 
		vprint("vrele: bad ref count", vp);
		panic("vrele: ref cnt");
	}
#endif
a805 1

d807 3
a809 3
	if (vp->v_writecount != 0) {
		vprint("vrele: bad writecount", vp);
		panic("vrele: v_writecount != 0");
d818 1
a818 2
void vhold __P((struct vnode *vp));

d832 1
a832 1
	if ((vp->v_bioflag & VBIOONFREELIST) &&
d844 27
d990 6
a1004 6
#ifdef UVM
	/*
	 * clean out any VM data associated with the vnode.
	 */
	uvm_vnp_terminate(vp);
#endif
a1090 1

d1179 2
a1180 7
	/*
	 * Move onto the free list, unless we were called from
	 * getnewvnode and we're not on any free list
	 */
	if (vp->v_usecount == 0 &&
	    (vp->v_bioflag & VBIOONFREELIST)) {
		int s;
a1181 2
		s = splbio();

d1183 5
a1187 7
			panic("vgonel: not clean");

		if (TAILQ_FIRST(&vnode_free_list) != vp) {
			TAILQ_REMOVE(&vnode_free_list, vp, v_freelist);
			TAILQ_INSERT_HEAD(&vnode_free_list, vp, v_freelist);
		}
		splx(s);
d1280 1
a1280 1
	printf("type %s, usecount %u, writecount %u, holdcount %u,",
d1294 2
a1295 2
	if (vp->v_bioflag & VBIOWAIT)
		strcat(buf, "| VBIOWAIT");
d1322 1
a1322 1
	for (mp = CIRCLEQ_FIRST(&mountlist); mp != CIRCLEQ_END(&mountlist);
d1417 1
a1417 2
	for (mp = CIRCLEQ_FIRST(&mountlist); mp != CIRCLEQ_END(&mountlist);
	     mp = nmp) {
d1419 1
a1419 1
			nmp = CIRCLEQ_NEXT(mp, mnt_list);
d1454 1
a1454 1
		nmp = CIRCLEQ_NEXT(mp, mnt_list);
d1720 3
a1722 4
	allerror = 0;
	for (mp = CIRCLEQ_LAST(&mountlist); mp != CIRCLEQ_END(&mountlist);
	     mp = nmp) {
		nmp = CIRCLEQ_PREV(mp, mnt_list);
a1783 1
	struct proc *p;
d1785 1
a1785 2
	p = curproc? curproc : &proc0;
	sys_sync(p, (void *)0, (register_t *)0);
d1792 1
a1792 1
			if ((bp->b_flags & (B_BUSY|B_INVAL|B_READ)) == B_BUSY)
d1807 1
a1807 2
					if (verbose)
						printf("softdep ");
a1880 25
 * Wait for all outstanding I/Os to complete
 *
 * Manipulates v_numoutput. Must be called at splbio()
 */
int
vwaitforio(vp, slpflag, wmesg, timeo)
	struct vnode *vp;
	int slpflag, timeo;
	char *wmesg;
{
	int error = 0;

	while (vp->v_numoutput) {
		vp->v_bioflag |= VBIOWAIT;
		error = tsleep((caddr_t)&vp->v_numoutput,
		    slpflag | (PRIBIO + 1), wmesg, timeo);
		if (error)
			break;
	}

	return (error);
}


/*
d1886 2
a1887 2
vwakeup(vp)
	struct vnode *vp;
d1889 5
a1893 2
	if (vp != NULL) {
		if (vp->v_numoutput-- == 0)
d1895 2
a1896 2
		if ((vp->v_bioflag & VBIOWAIT) && vp->v_numoutput == 0) {
			vp->v_bioflag &= ~VBIOWAIT;
d1920 4
a1923 1
		vwaitforio(vp, 0, "vinvalbuf", 0);
d2018 5
a2022 1
	vwaitforio(vp, 0, "vflushbuf", 0);
a2023 1
		splx(s);
a2026 1
	splx(s);
d2042 1
a2042 1
	vhold(vp);
d2072 1
a2072 1
	if ((vp->v_bioflag & VBIOONSYNCLIST) &&
d2074 1
a2074 1
		vp->v_bioflag &= ~VBIOONSYNCLIST;
d2078 1
a2078 20

	simple_lock(&vp->v_interlock);
#ifdef DIAGNOSTIC
	if (vp->v_holdcnt == 0)
		panic("brelvp: holdcnt");
#endif
	vp->v_holdcnt--;

	/*
	 * If it is on the holdlist and the hold count drops to
	 * zero, move it to the free list. 
	 */
	if ((vp->v_bioflag & VBIOONFREELIST) &&
	    vp->v_holdcnt == 0 && vp->v_usecount == 0) {
		simple_lock(&vnode_free_list_slock);
		TAILQ_REMOVE(&vnode_hold_list, vp, v_freelist);
		TAILQ_INSERT_TAIL(&vnode_free_list, vp, v_freelist);
		simple_unlock(&vnode_free_list_slock);
	}
	simple_unlock(&vp->v_interlock);
d2082 3
a2084 31
 * Replaces the current vnode associated with the buffer, if any
 * with a new vnode.
 *
 * If an output I/O is pending on the buffer, the old vnode is
 * I/O count is adjusted.
 *
 * Ignores vnode buffer queues. Must be called at splbio().
 */
void
buf_replacevnode(bp, newvp)
	struct buf *bp;
	struct vnode *newvp;
{
	struct vnode *oldvp = bp->b_vp;

	if (oldvp)
		brelvp(bp);
		
	if ((bp->b_flags & (B_READ | B_DONE)) == 0) {
		newvp->v_numoutput++;	/* put it on swapdev */
		vwakeup(oldvp);
	}

	bgetvp(newvp, bp);
	bufremvn(bp);
}

/*
 * Used to assign buffers to the appropriate clean or dirty list on
 * the vnode and to add newly dirty vnodes to the appropriate
 * filesystem syncer list.
d2089 1
a2089 1
reassignbuf(bp)
d2091 1
a2094 1
	struct vnode *vp = bp->b_vp;
d2096 1
a2096 1
	if (vp == NULL) {
d2110 5
a2114 5
		listheadp = &vp->v_cleanblkhd;
		if ((vp->v_bioflag & VBIOONSYNCLIST) &&
		    LIST_FIRST(&vp->v_dirtyblkhd) == NULL) {
			vp->v_bioflag &= ~VBIOONSYNCLIST;
			LIST_REMOVE(vp, v_synclist);
d2117 3
a2119 3
		listheadp = &vp->v_dirtyblkhd;
		if ((vp->v_bioflag & VBIOONSYNCLIST) == 0) {
			switch (vp->v_type) {
d2124 1
a2124 1
				if (vp->v_specmountpoint != NULL) {
d2132 1
a2132 1
			vn_syncer_add_to_worklist(vp, delay);
a2206 14
}

/*
 * Check if vnode represents a disk device
 */
int
vn_isdisk(vp, errp)
	struct vnode *vp;
	int *errp;
{
	if (vp->v_type != VBLK && vp->v_type != VCHR)
		return (0);

	return (1);
@


1.43.2.2
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.43.2.1 2001/05/14 22:32:46 niklas Exp $	*/
d72 1
d74 2
d97 1
a97 1
}
a123 3
#define VN_KNOTE(vp, b) \
	KNOTE((struct klist *)&vp->v_selectinfo.vsi_selinfo.si_note, (b))

d186 1
a186 1
	return (0);
d217 1
a217 1

d253 1
a253 1

d265 1
a265 1

d399 1
d404 1
a404 1
	    ((TAILQ_FIRST(listhd = &vnode_hold_list) == NULL) || toggle))) {
d412 1
a412 1
		    vp = TAILQ_NEXT(vp, v_freelist)) {
d464 1
d466 1
d482 1
a482 1

d600 2
a601 1
	/*
d622 1
a622 1
	/*
d629 1
a629 1
	 *
d654 1
a654 1
	struct vnode *vp;
d720 4
d726 2
a727 1
	struct vnode *vp;
d729 1
a729 1
	int s;
d736 1
a736 1

d738 2
a739 2
		vprint("vnode already on free list: ", vp);
		panic("vnode already on free list");
d743 1
d746 1
a746 1
	if (vp->v_holdcnt > 0)
d751 1
d776 1
a776 1
	if (vp->v_usecount == 0) {
d793 1
a793 1
#endif
d817 1
a817 1
	if (vp->v_usecount == 0) {
d900 1
a900 1

d995 1
d1000 1
a1051 3
	simple_lock(&vp->v_selectinfo.vsi_lock);
	VN_KNOTE(vp, NOTE_REVOKE);
	simple_unlock(&vp->v_selectinfo.vsi_lock);
d1133 1
a1133 1
	 * If special device, remove it from special device alias list
d1172 1
a1172 1
	 * move it to the head of the list.
d1183 1
a1183 2

		simple_lock(&vnode_free_list_slock);
d1194 1
a1194 1
		simple_unlock(&vnode_free_list_slock);
d1329 2
a1330 2
	    mp = nmp) {
		if (vfs_busy(mp, LK_NOWAIT, &mountlist_slock, p)) {
d1334 3
a1336 2
		for (vp = mp->mnt_vnodelist.lh_first; vp;
		    vp = vp->v_mntvnodes.le_next) {
d1421 1
a1421 1

d1424 1
a1424 1
	    mp = nmp) {
d1431 3
a1433 2
		for (vp = mp->mnt_vnodelist.lh_first; vp != NULL;
		    vp = nvp) {
d1648 1
a1648 1
					    rnh);
d1676 1
a1676 1

d1680 1
a1680 1

d1682 1
a1682 1

d1693 1
a1693 1

d1704 1
a1704 1

d1729 1
a1729 1
	    mp = nmp) {
d1760 8
d1796 1
a1796 1

d1828 1
a1828 1
	}
d1887 2
a1888 2
/*
 * Routines dealing with vnodes and buffers
d1968 1
a1968 1
		if ((blist = vp->v_cleanblkhd.lh_first) &&
d2084 1
a2084 1
 *
d2116 1
a2116 1
	 * zero, move it to the free list.
d2146 1
a2146 1

d2227 3
a2229 2
	for (vfspp = &vfsconf, vfsp = vfsconf; vfsp;
	    vfspp = &vfsp->vfc_next, vfsp = vfsp->vfc_next)
d2247 1
a2247 1

d2257 3
a2259 2
	for (vfspp = &vfsconf, vfsp = vfsconf; vfsp;
	    vfspp = &vfsp->vfc_next, vfsp = vfsp->vfc_next) {
d2264 1
a2264 1
	if (!vfsp)			/* Not found */
d2267 1
a2267 1
	if (vfsp->vfc_refcount)		/* In use */
d2270 1
a2270 1
	/* Remove from list and free */
d2280 1
@


1.43.2.3
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.43.2.2 2001/07/04 10:48:52 niklas Exp $	*/
d624 1
a624 1
	 * created in ffs_mountroot.
d766 2
a767 2
		vprint("vput: bad ref count", vp);
		panic("vput: ref cnt");
d779 2
a780 2
		vprint("vput: bad writecount", vp);
		panic("vput: v_writecount != 0");
a1515 2
	if (argp->ex_addrlen > MLEN)
		return (EINVAL);
a1533 4
	if (i < 0 || i > AF_MAX) {
		error = EINVAL;
		goto out;
	}
d1975 1
a1975 2
			bremfree(bp);
			bp->b_flags |= B_BUSY;
d2013 1
a2013 2
		bremfree(bp);
		bp->b_flags |= B_BUSY;
@


1.43.2.4
log
@merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d67 1
a67 1
#include <uvm/uvm_extern.h>
d71 2
@


1.43.2.5
log
@Merge in -current
@
text
@a114 2
int vflush_vnode(struct vnode *, void *);

a198 6
int
vfs_isbusy(struct mount *mp) 
{
	return (lockstatus(&mp->mnt_lock));
}

a371 2
	extern struct uvm_pagerops uvm_vnodeops;
	struct uvm_object *uobj;
d405 1
a405 1
		bzero(vp, sizeof *vp);
d418 1
a418 1
		if (vp == NULL) {
d450 1
a453 1
	lockinit(&vp->v_glock, PVFS, "glock", 0, 0);
a461 10

	/*
	 * initialize uvm_object within vnode.
	 */

	uobj = &vp->v_uvm.u_obj;
	uobj->pgops = &uvm_vnodeops;
	TAILQ_INIT(&uobj->memq);
	vp->v_uvm.u_size = VSIZENOTSET;

a664 4
		if (flags & LK_NOWAIT) {
			simple_unlock(&vp->v_interlock);
			return (EBUSY);
		}
a782 5
	if (vp->v_flag & VTEXT) {
		uvmexp.vtextpages -= vp->v_uvm.u_obj.uo_npages;
		uvmexp.vnodepages += vp->v_uvm.u_obj.uo_npages;
	}
	vp->v_flag &= ~VTEXT;
a822 5
	if (vp->v_flag & VTEXT) {
		uvmexp.vtextpages -= vp->v_uvm.u_obj.uo_npages;
		uvmexp.vnodepages += vp->v_uvm.u_obj.uo_npages;
	}
	vp->v_flag &= ~VTEXT;
d867 8
a874 4
vfs_mount_foreach_vnode(struct mount *mp, 
    int (*func)(struct vnode *, void *), void *arg) {
	struct vnode *vp, *nvp;
	int error = 0;
d882 5
a886 2
		simple_lock(&vp->v_interlock);		
		simple_unlock(&mntvnode_slock);
d888 23
a910 60
		error = func(vp, arg);

		simple_lock(&mntvnode_slock);

		if (error != 0)
			break;
	}
	simple_unlock(&mntvnode_slock);

	return (error);
}


struct vflush_args {
	struct vnode *skipvp;
	int busy;
	int flags;
};

int
vflush_vnode(struct vnode *vp, void *arg) {
	struct vflush_args *va = arg;
	struct proc *p = curproc;

	if (vp == va->skipvp) {
		simple_unlock(&vp->v_interlock);
		return (0);
	}

	if ((va->flags & SKIPSYSTEM) && (vp->v_flag & VSYSTEM)) {
		simple_unlock(&vp->v_interlock);
		return (0);
	}

	/*
	 * If WRITECLOSE is set, only flush out regular file
	 * vnodes open for writing.
	 */
	if ((va->flags & WRITECLOSE) &&
	    (vp->v_writecount == 0 || vp->v_type != VREG)) {
		simple_unlock(&vp->v_interlock);
		return (0);
	}

	/*
	 * With v_usecount == 0, all we need to do is clear
	 * out the vnode data structures and we are done.
	 */
	if (vp->v_usecount == 0) {
		vgonel(vp, p);
		return (0);
	}
		
	/*
	 * If FORCECLOSE is set, forcibly close the vnode.
	 * For block or character devices, revert to an
	 * anonymous device. For all other files, just kill them.
	 */
	if (va->flags & FORCECLOSE) {
		if (vp->v_type != VBLK && vp->v_type != VCHR) {
d912 19
a930 4
		} else {
			vclean(vp, 0, p);
			vp->v_op = spec_vnodeop_p;
			insmntque(vp, (struct mount *)0);
a931 3
		return (0);
	}

d933 2
a934 2
	if (busyprt)
		vprint("vflush: busy vnode", vp);
d936 5
a940 19
	simple_unlock(&vp->v_interlock);
	va->busy++;
	return (0);
}

int
vflush(mp, skipvp, flags)
	struct mount *mp;
	struct vnode *skipvp;
	int flags;
{
	struct vflush_args va;
	va.skipvp = skipvp;
	va.busy = 0;
	va.flags = flags;

	vfs_mount_foreach_vnode(mp, vflush_vnode, &va);

	if (va.busy)
a972 6
	if (vp->v_flag & VTEXT) {
		uvmexp.vtextpages -= vp->v_uvm.u_obj.uo_npages;
		uvmexp.vnodepages += vp->v_uvm.u_obj.uo_npages;
	}
	vp->v_flag &= ~VTEXT;

d983 5
a987 1
	 * Clean out any cached data associated with the vnode.
d1932 1
a1932 2
	struct uvm_object *uobj = &vp->v_uvm.u_obj;
	struct buf *bp;
d1934 1
a1934 13
	int s, error, rv;
	int flushflags = PGO_ALLPAGES|PGO_FREE|PGO_SYNCIO|
	    (flags & V_SAVE ? PGO_CLEANIT : 0);

	/* XXXUBC this doesn't look at flags or slp* */
	if (vp->v_type == VREG) {
		simple_lock(&uobj->vmobjlock);
		rv = (uobj->pgops->pgo_flush)(uobj, 0, 0, flushflags);
		simple_unlock(&uobj->vmobjlock);
		if (!rv) {
			return EIO;
		}
	}
d2004 1
a2004 1
	struct vnode *vp;
d2007 1
a2007 2
	struct uvm_object *uobj = &vp->v_uvm.u_obj;
	struct buf *bp, *nbp;
a2009 8
	if (vp->v_type == VREG) {
		int flags = PGO_CLEANIT|PGO_ALLPAGES| (sync ? PGO_SYNCIO : 0);

		simple_lock(&uobj->vmobjlock);
		(uobj->pgops->pgo_flush)(uobj, 0, 0, flags);
		simple_unlock(&uobj->vmobjlock);
	}

d2076 1
a2076 1
	struct buf *bp;
d2080 1
a2080 1
	if ((vp = bp->b_vp) == NULL)
a2081 1

d2087 1
a2087 2
	if (TAILQ_EMPTY(&vp->v_uvm.u_obj.memq) &&
	    (vp->v_bioflag & VBIOONSYNCLIST) &&
d2092 1
a2092 1
	bp->b_vp = NULL;
d2152 1
a2152 1
	struct buf *bp;
d2158 4
d2173 1
a2173 2
		if (TAILQ_EMPTY(&vp->v_uvm.u_obj.memq) &&
		    (vp->v_bioflag & VBIOONSYNCLIST) &&
@


1.43.2.6
log
@Merge in trunk
@
text
@d134 1
a134 1
	    &pool_allocator_nointr);
d257 1
d380 2
d415 1
a415 1
		bzero((char *)vp, sizeof *vp);
d463 1
d472 10
d685 4
d807 5
d852 5
d1039 6
d1055 1
a1055 5
	 * clean out any VM data associated with the vnode.
	 */
	uvm_vnp_terminate(vp);
	/*
	 * Clean out any buffers associated with the vnode.
d2000 2
a2001 1
	register struct buf *bp;
d2003 13
a2015 1
	int s, error;
d2085 1
a2085 1
	register struct vnode *vp;
d2088 2
a2089 1
	register struct buf *bp, *nbp;
d2092 8
d2166 1
a2166 1
	register struct buf *bp;
d2170 1
a2170 1
	if ((vp = bp->b_vp) == (struct vnode *) 0)
d2172 1
d2178 2
a2179 1
	if ((vp->v_bioflag & VBIOONSYNCLIST) &&
d2184 1
a2184 1
	bp->b_vp = (struct vnode *) 0;
d2261 2
a2262 1
		if ((vp->v_bioflag & VBIOONSYNCLIST) &&
@


1.43.2.7
log
@Merge in -current from about a week ago
@
text
@d104 1
a104 1
void	vclean(struct vnode *, int, struct proc *);
d106 2
a107 2
void insmntque(struct vnode *, struct mount *);
int getdevvp(dev_t, struct vnode **, enum vtype);
d109 5
a113 5
int vfs_hang_addrlist(struct mount *, struct netexport *,
				  struct export_args *);
int vfs_free_netcred(struct radix_node *, void *);
void vfs_free_addrlist(struct netexport *);
static __inline__ void vputonfreelist(struct vnode *);
d118 1
a118 1
void printlockedvnodes(void);
d366 1
a366 1
extern int (**dead_vnodeop_p)(void *);
d376 1
a376 1
	int (**vops)(void *);
d833 1
a833 1
void vhold(struct vnode *vp);
@


1.43.2.8
log
@Sync the SMP branch with 3.3
@
text
@d149 1
a152 6
 *
 * historical behavior:
 *  - LK_NOWAIT means that we should just ignore the mount point if it's
 *     being unmounted.
 *  - no flags means that we should sleep on the mountpoint and then
 *     fail.
d156 5
a160 2
vfs_busy(struct mount *mp, int flags, struct simplelock *interlkp,
    struct proc *p)
d164 16
a179 9
	switch (flags) {
	case LK_NOWAIT:
		lkflags = LK_SHARED|LK_NOWAIT;
		break;
	case 0:
		lkflags = LK_SHARED;
		break;
	default:
		lkflags = flags;
d181 1
a181 7

	/*
	 * Always sleepfail. We will only sleep for an exclusive lock
	 * and the exclusive lock will only be acquired when unmounting.
	 */
	lkflags |= LK_SLEEPFAIL;

d185 1
a185 1
		return (ENOENT);
d194 3
a196 1
vfs_unbusy(struct mount *mp, struct proc *p)
d1440 2
d1443 1
a1443 1
		*sizep = (numvnodes + KINFO_VNODESLOP) * sizeof(struct e_vnode);
d1472 1
a1472 1
			if (bp + sizeof(struct e_vnode) > ewhere) {
a1474 1
				vfs_unbusy(mp, p);
d1477 2
a1478 7
			if ((error = copyout((caddr_t)&vp,
			    &((struct e_vnode *)bp)->vptr,
			    sizeof(struct vnode *))) ||
			   (error = copyout((caddr_t)vp,
			    &((struct e_vnode *)bp)->vnode,
			    sizeof(struct vnode)))) {
				vfs_unbusy(mp, p);
d1480 1
a1480 2
			}
			bp += sizeof(struct e_vnode);
d1752 1
a1752 1
vfs_unmountall(void)
d1754 1
a1754 1
	struct mount *mp, *nmp;
a1755 1
	struct proc *p = curproc;
a1761 2
		if ((vfs_busy(mp, LK_EXCLUSIVE|LK_NOWAIT, NULL, p)) != 0)
			continue;
a1926 2
	splassert(IPL_BIO);

a1947 2
	splassert(IPL_BIO);

a2091 2
	splassert(IPL_BIO);

a2117 2
	splassert(IPL_BIO);

a2168 2
	splassert(IPL_BIO);

a2194 2

	splassert(IPL_BIO);
@


1.43.2.9
log
@Sync the SMP branch to -current. This includes moving to ELF.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.43.2.8 2003/03/28 00:41:27 niklas Exp $	*/
a411 1
		simple_lock_init(&vp->v_interlock);
d416 2
a417 8
			if (simple_lock_try(&vp->v_interlock)) {
				if ((vp->v_flag & VLAYER) == 0)
					break;
				if (VOP_ISLOCKED(vp) == 0)
					break;
				else
					simple_unlock(&vp->v_interlock);
			}
a459 2
	vp->v_vnlock = NULL;
	lockinit(&vp->v_lock, PVFS, "v_lock", 0, 0);
a635 2
	vp->v_vnlock = NULL;
	lockinit(&vp->v_lock, PVFS, "v_lock", 0, 0);
d1063 6
d1317 1
a1317 1
		strlcat(buf, "|VROOT", sizeof buf);
d1319 1
a1319 1
		strlcat(buf, "|VTEXT", sizeof buf);
d1321 1
a1321 1
		strlcat(buf, "|VSYSTEM", sizeof buf);
d1323 1
a1323 1
		strlcat(buf, "|VXLOCK", sizeof buf);
d1325 1
a1325 1
		strlcat(buf, "|VXWANT", sizeof buf);
d1327 1
a1327 1
		strlcat(buf, "| VBIOWAIT", sizeof buf);
d1329 1
a1329 1
		strlcat(buf, "|VALIASED", sizeof buf);
d1769 1
a1769 1
		if ((error = dounmount(mp, MNT_FORCE, curproc, NULL)) != 0) {
@


1.43.2.10
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.43.2.9 2003/05/13 19:21:28 ho Exp $	*/
d21 5
a25 1
 * 3. Neither the name of the University nor the names of its contributors
@


1.43.2.11
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d679 1
a679 1
		tsleep(vp, PINOD, "vget", 0);
d1085 1
a1085 1
		wakeup(vp);
d1146 1
a1146 1
		tsleep(vp, PINOD, "vgone", 0);
d1476 1
a1476 1
			if ((error = copyout(&vp,
d1479 1
a1479 1
			   (error = copyout(vp,
d1561 1
a1561 1
	bzero(np, i);
d1563 1
a1563 1
	error = copyin(argp->ex_addr, saddr, argp->ex_addrlen);
d1570 1
a1570 1
		error = copyin(argp->ex_mask, smask, argp->ex_masklen);
d1621 1
a1621 1
	free(rn, M_NETADDR);
d1638 1
a1638 1
			free(rnh, M_RTABLE);
d1939 1
a1939 1
		error = tsleep(&vp->v_numoutput,
d1965 1
a1965 1
			wakeup(&vp->v_numoutput);
d2020 3
a2022 2
				error = tsleep(bp, slpflag | (PRIBIO + 1),
				    "vinvalbuf", slptimeo);
@


1.43.2.12
log
@Merge with the trunk
@
text
@d1620 1
a1620 1
	(*rnh->rnh_deladdr)(rn->rn_key, rn->rn_mask, rnh, NULL);
a1792 6
#ifdef ACCOUNTING
	extern void acct_shutdown(void);

	acct_shutdown();
#endif

@


1.42
log
@Use VONSYNCLIST to see if we should remove a vnode from the sync list instead
of looking at v_dirtyblkhd.
@
text
@d1783 1
a1783 1
	int iter, nbusy;
d1788 1
d1791 1
a1791 1
		for (bp = &buf[nbuf]; --bp >= buf; )
d1794 23
a1816 5
			if (nbusy == 0)
				break;
			if (verbose)
				printf("%d ", nbusy);
			DELAY(40000 * iter);
@


1.41
log
@more paranoid check of the refcount in vfs_register
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.40 1999/08/08 00:34:38 niklas Exp $	*/
a2044 1
	struct buf *wasdirty;
a2050 1
	wasdirty = vp->v_dirtyblkhd.lh_first;
d2053 3
a2055 1
	if (wasdirty && LIST_FIRST(&vp->v_dirtyblkhd) == NULL)
d2057 1
a2074 1
	struct buf *wasdirty;
a2083 1
	wasdirty = newvp->v_dirtyblkhd.lh_first;
d2092 3
a2094 1
		if (wasdirty && LIST_FIRST(&newvp->v_dirtyblkhd) == NULL)
d2096 1
d2099 1
a2099 1
		if (LIST_FIRST(listheadp) == NULL) {
d2102 1
a2102 1
				delay = syncdelay / 3;
d2106 1
a2106 1
					delay = syncdelay / 2;
@


1.40
log
@From NetBSD; vdevgone, used for revoking access to device nodes when they
disappear (detach is coming).
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.39 1999/05/31 17:34:48 millert Exp $	*/
d2126 1
a2126 1
	if (vfs->vfc_refcount > 0)
@


1.39
log
@New struct statfs with mount options.  NOTE: this replaces statfs(2),
fstatfs(2), and getfsstat(2) so you will need to build a new kernel
before doing a "make build" or you will get "unimplemented syscall" errors.

The new struct statfs has the following featuires:
o Has a u_int32_t flags field--now softdep can have a real flag.

o Uses u_int32_t instead of longs (nicer on the alpha).  Note: the man
page used to lie about setting invalid/unused fields to -1.  SunOS does
that but our code never has.

o Gets rid of f_type completely.  It hasn't been used since NetBSD 0.9
and having it there but always 0 is confusing.  It is conceivable
that this may cause some old code to not compile but that is better
than silently breaking.

o Adds a mount_info union that contains the FSTYPE_args struct.  This
means that "mount" can now tell you all the options a filesystem was
mounted with.  This is especially nice for NFS.

Other changes:
o The linux statfs emulation didn't convert between BSD fs names
  and linux f_type numbers.  Now it does, since the BSD f_type
  number is useless to linux apps (and has been removed anyway)

o FreeBSD's struct statfs is different from our (both old and new)
and thus needs conversion.  Previously, the OpenBSD syscalls
were used without any real translation.

o mount(8) will now show extra info when invoked with no arguments.
However, to see *everything* you need to use the -v (verbose) flag.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.38 1999/05/06 15:59:40 mickey Exp $	*/
d1214 17
@


1.38
log
@factor out sync+wait code into vfa_syncwait() routine for
applications in system like power management and such.
art@@ finally said `commit it'
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.37 1999/04/30 08:21:52 art Exp $	*/
a224 1
	mp->mnt_stat.f_type = vfsp->vfc_typenum;
@


1.37
log
@in vput, simple_unlock the v_interlock before VOP_INACTIVE, not after
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.36 1999/03/11 19:47:25 deraadt Exp $	*/
a1729 3
	register struct buf *bp;
	int iter, nbusy;

d1751 18
a1768 1
	/* Sync again after unmount, just in case. */
d1770 1
a1770 1

d1777 8
a1784 9
		if (nbusy == 0)
			break;
		printf("%d ", nbusy);
		DELAY(40000 * iter);
	}
	if (nbusy)
		printf("giving up\n");
	else
		printf("done\n");
@


1.36
log
@backout
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.32 1999/02/26 05:17:43 art Exp $	*/
d782 2
a784 2

	simple_unlock(&vp->v_interlock);
@


1.35
log
@back out unapproved changes
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.33 1999/03/11 18:28:55 mickey Exp $	*/
d1730 3
d1754 2
a1755 17
	if (vfs_syncwait(1))
		printf("giving up\n");
	else
		printf("done\n");
}

/*
 * perform sync() operation and wait for buffers to flush.
 * assumtions: called w/ scheduler disabled and physical io enabled
 * for now called at spl0() XXX
 */
int
vfs_syncwait(verbose)
	int verbose;
{
	register struct buf *bp;
	int iter, nbusy;
a1756 2
	sys_sync(&proc0, (void *)0, (register_t *)0);
 
d1763 9
a1771 8
			if (nbusy == 0)
				break;
			if (verbose)
				printf("%d ", nbusy);
			DELAY(40000 * iter);
	}   

	return nbusy;
@


1.34
log
@indent
@
text
@d1777 5
a1781 5
		if (nbusy == 0)
			break;
		if (verbose)
			printf("%d ", nbusy);
		DELAY(40000 * iter);
@


1.33
log
@factor sync+wait operation out into a separate function.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.32 1999/02/26 05:17:43 art Exp $	*/
d1777 5
a1781 5
			if (nbusy == 0)
				break;
			if (verbose)
				printf("%d ", nbusy);
			DELAY(40000 * iter);
@


1.32
log
@adapt to uvm vnode pager
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.31 1999/02/19 17:15:45 art Exp $	*/
a1729 3
	register struct buf *bp;
	int iter, nbusy;

d1751 18
a1768 1
	/* Sync again after unmount, just in case. */
d1770 1
a1770 1

d1777 8
a1784 9
		if (nbusy == 0)
			break;
		printf("%d ", nbusy);
		DELAY(40000 * iter);
	}
	if (nbusy)
		printf("giving up\n");
	else
		printf("done\n");
@


1.31
log
@add vfs_register and vfs_unregister functions
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.30 1998/12/28 19:35:35 art Exp $	*/
d71 4
d464 3
d991 6
a996 2

	
d1740 1
d1742 1
@


1.30
log
@simple_lock fixes
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.29 1998/12/22 10:43:37 art Exp $	*/
d2073 71
@


1.29
log
@deconfuse vprint, print holdcount, not refcount when we are talking about holdcnt
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.28 1998/12/10 21:46:58 art Exp $	*/
d574 2
a575 1
		if (nvp_rdev != vp->v_rdev || nvp->v_type != vp->v_type)
d577 1
d1023 2
@


1.28
log
@vfs_unmountall: retry to unmount all remaining filesystems when one unmount failed
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.27 1998/12/05 16:50:40 csapuntz Exp $	*/
d1249 1
a1249 1
	printf("type %s, usecount %d, writecount %d, refcount %ld,",
@


1.27
log
@

Framework for generating automatic test code for locking discipline
in DIAGNOSTIC mode.

Added documentation to vfs_subr.c on locking needs of a couple calls.

Improvements to the vinvalbuf patch. We need to start over after we
let our pants down.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.26 1998/12/04 15:57:01 csapuntz Exp $	*/
d774 1
d1686 1
a1686 1
	int allerror, error;
d1688 1
d1698 2
a1699 1
	if (allerror)
d1701 6
@


1.26
log
@

VFS-Lite2 requires stricter locking around vnode buffer queues. vinvalbuf
had insufficient protection
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.25 1998/11/20 01:35:32 art Exp $	*/
d1043 3
d1808 2
d1861 1
d1899 1
a1899 2
				s = splbio();
				break;
a1904 1
	splx(s);
d1908 1
d1957 2
d1982 2
d2010 2
@


1.25
log
@vn_lock already unlocks the simple lock. don't do that again
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.24 1998/11/12 04:30:02 csapuntz Exp $	*/
d1856 1
a1872 1
			s = splbio();
d1878 2
a1879 2
				splx(s);
				if (error)
d1881 1
a1884 1
			splx(s);
d1891 1
d1893 1
d1900 1
@


1.24
log
@

Integrate latest soft updates patches for McKusick.

Integrate cleaner ffs mount code from FreeBSD. Most notably, this mount
code prevents you from mounting an unclean file system read-write.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.23 1998/10/13 16:42:01 csapuntz Exp $	*/
d805 1
a805 1
	if (vn_lock(vp, LK_EXCLUSIVE |LK_INTERLOCK, p) == 0)
a806 2

	simple_unlock(&vp->v_interlock);
@


1.23
log
@

In vrele, vget, reinstate to following order

- VNODE gets placed on free list
- VOP_INACTIVE is called

This was the original order. It was changed in an earlier patch due to
a race condition in non-locking FSes (like NFS) between getnewvnode
and inactive. However, the modified order had its own race conditions, so
it turned out not to be a good choice.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.22 1998/08/30 23:34:36 csapuntz Exp $	*/
d1841 16
a1856 5
	if ((flags & V_SAVE) && vp->v_dirtyblkhd.lh_first != NULL) {
		if ((error = VOP_FSYNC(vp, cred, MNT_WAIT, p)) != 0)
			return (error);
		if (vp->v_dirtyblkhd.lh_first != NULL)
			panic("vinvalbuf: dirty bufs");
@


1.22
log
@

Cleanup.

Error diagnostics in vputonfreelist to catch violations of assumptions.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.21 1998/08/06 19:34:26 csapuntz Exp $	*/
d771 2
a773 4

	if (vp->v_usecount == 0)
		vputonfreelist(vp);

d803 2
a806 3

	if (vp->v_usecount == 0)
		vputonfreelist(vp);
@


1.21
log
@

Rename vop_revoke, vn_bwrite, vop_noislocked, vop_nolock, vop_nounlock
to be vop_generic_revoke, vop_generic_bwrite, vop_generic_islocked,
vop_generic_lock and vop_generic_unlock.

Create vop_generic_abortop and propogate change to all file systems.

Fix PR/371.

Get rid of locking in NULLFS (should be mostly unnecessary now except for
forced unmounts).
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.20 1998/04/25 07:04:11 niklas Exp $	*/
d590 6
a595 1
	if (vp == NULL || vp->v_tag != VT_NON || vp->v_type != VBLK) {
d612 12
d715 12
a726 1
	
a742 1

d774 1
a774 1
	  vputonfreelist(vp);
d809 1
a809 1
	  vputonfreelist(vp);
@


1.20
log
@typo
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.19 1998/02/20 14:47:51 niklas Exp $	*/
a105 1
int vunref __P((struct vnode *));
d111 1
d422 1
d424 2
a425 2
		/* see comment on why 0xdeadb is set at end of vgone (below) */
		vp->v_flag |= VGONEHACK;
d647 1
a647 1
	if (vp->v_usecount == 0) {
d654 1
d659 4
a662 1
			vunref(vp);
d688 4
a691 3
int
vunref(vp)
	struct vnode *vp;
d693 2
a694 16
#ifdef DIAGNOSTIC
	if (vp == NULL)
		panic("vrele: null vp");
#endif
	simple_lock (&vp->v_interlock);
	vp->v_usecount--;
	if (vp->v_usecount > 0) {
		simple_unlock(&vp->v_interlock);
		return (vp->v_usecount);
	}
#ifdef DIAGNOSTIC
	if (vp->v_usecount < 0 || vp->v_writecount != 0) {
		vprint("vrele: bad ref count", vp);
		panic("vrele: ref cnt");
	}
#endif
d698 3
d702 9
a710 2
	if (vp->v_holdcnt > 0)
		TAILQ_INSERT_TAIL(&vnode_hold_list, vp, v_freelist);
d712 2
a713 1
		TAILQ_INSERT_TAIL(&vnode_free_list, vp, v_freelist);
a715 1
	return (0);
d727 1
a727 1
#ifdef DIGANOSTIC
d744 5
a748 9
	/*
	 * insert at tail of LRU list
	 */
	simple_lock(&vnode_free_list_slock);
	if (vp->v_holdcnt > 0)
		TAILQ_INSERT_TAIL(&vnode_hold_list, vp, v_freelist);
	else
		TAILQ_INSERT_TAIL(&vnode_free_list, vp, v_freelist);
	simple_unlock(&vnode_free_list_slock);
a749 1
	VOP_INACTIVE(vp, p);
d760 1
a760 1
	struct proc *p = curproc;
d762 17
a778 2
	if (vunref(vp) == 0 &&
	    vn_lock(vp, LK_EXCLUSIVE |LK_INTERLOCK, p) == 0)
d780 5
a798 7
	 *
	 * The VGONEHACK flag reflects a call from getnewvnode,
	 * which will remove the vnode from the free list, but
	 * will not increment the ref count until after it calls vgone
	 * If the ref count we're incremented first, vgone would
	 * (incorrectly) try to close the previous instance of the
	 * underlying object.
d801 1
a801 1
	if (!(vp->v_flag & VGONEHACK) &&
a826 2
	 *
	 * See above for VGONEHACK
d828 1
a828 1
	if (!(vp->v_flag & VGONEHACK) &&
d998 7
a1004 3
		if (vunref(vp) == 0 &&
		    vp->v_holdcnt > 0)
			panic("vclean: not clean");
a1135 2
	 *
	 * See above about the VGONEHACK
d1137 5
a1141 2
	if (vp->v_usecount == 0) {
		simple_lock(&vnode_free_list_slock);
d1144 5
a1148 6
		if (!(vp->v_flag & VGONEHACK) &&
		    TAILQ_FIRST(&vnode_free_list) != vp) {
			TAILQ_REMOVE(&vnode_free_list, vp, v_freelist);
			TAILQ_INSERT_HEAD(&vnode_free_list, vp, v_freelist);
		}
		simple_unlock(&vnode_free_list_slock);
a1149 1
	vp->v_type = VBAD;
@


1.19
log
@typo
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.18 1998/01/11 02:10:44 csapuntz Exp $	*/
d139 1
a139 1
 * Mark a mount point as busy. Used to synchornize access and to delay
@


1.18
log
@Fix a couple spinlock references. More code motion in vfs_subr.c
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.17 1998/01/10 23:41:19 csapuntz Exp $	*/
d140 1
a140 1
 * unmounting. Interlock is not released n failure.
@


1.17
log
@Broke up vfs_subr.c which was getting a bit huge. We now have seperate files
for the syncer daemon as well as default VOP_*.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.16 1997/11/24 22:42:38 niklas Exp $	*/
d101 1
a101 1
static struct simplelock spechash_slock;
a485 236


/*
 * Update outstanding I/O count and do wakeup if requested.
 */
void
vwakeup(bp)
	register struct buf *bp;
{
	register struct vnode *vp;

	bp->b_flags &= ~B_WRITEINPROG;
	if ((vp = bp->b_vp) != NULL) {
		if (--vp->v_numoutput < 0)
			panic("vwakeup: neg numoutput");
		if ((vp->v_flag & VBWAIT) && vp->v_numoutput <= 0) {
			vp->v_flag &= ~VBWAIT;
			wakeup((caddr_t)&vp->v_numoutput);
		}
	}
}

/*
 * Flush out and invalidate all buffers associated with a vnode.
 * Called with the underlying object locked.
 */
int
vinvalbuf(vp, flags, cred, p, slpflag, slptimeo)
	register struct vnode *vp;
	int flags;
	struct ucred *cred;
	struct proc *p;
	int slpflag, slptimeo;
{
	register struct buf *bp;
	struct buf *nbp, *blist;
	int s, error;

	if ((flags & V_SAVE) && vp->v_dirtyblkhd.lh_first != NULL) {
		if ((error = VOP_FSYNC(vp, cred, MNT_WAIT, p)) != 0)
			return (error);
		if (vp->v_dirtyblkhd.lh_first != NULL)
			panic("vinvalbuf: dirty bufs");
	}
	for (;;) {
		if ((blist = vp->v_cleanblkhd.lh_first) && 
		    (flags & V_SAVEMETA))
			while (blist && blist->b_lblkno < 0)
				blist = blist->b_vnbufs.le_next;
		if (!blist && (blist = vp->v_dirtyblkhd.lh_first) &&
		    (flags & V_SAVEMETA))
			while (blist && blist->b_lblkno < 0)
				blist = blist->b_vnbufs.le_next;
		if (!blist)
			break;

		for (bp = blist; bp; bp = nbp) {
			nbp = bp->b_vnbufs.le_next;
			if (flags & V_SAVEMETA && bp->b_lblkno < 0)
				continue;
			s = splbio();
			if (bp->b_flags & B_BUSY) {
				bp->b_flags |= B_WANTED;
				error = tsleep((caddr_t)bp,
					slpflag | (PRIBIO + 1), "vinvalbuf",
					slptimeo);
				splx(s);
				if (error)
					return (error);
				break;
			}
			bp->b_flags |= B_BUSY | B_VFLUSH;
			splx(s);
			/*
			 * XXX Since there are no node locks for NFS, I believe
			 * there is a slight chance that a delayed write will
			 * occur while sleeping just above, so check for it.
			 */
			if ((bp->b_flags & B_DELWRI) && (flags & V_SAVE)) {
				(void) VOP_BWRITE(bp);
				break;
			}
			bp->b_flags |= B_INVAL;
			brelse(bp);
		}
	}
	if (!(flags & V_SAVEMETA) &&
	    (vp->v_dirtyblkhd.lh_first || vp->v_cleanblkhd.lh_first))
		panic("vinvalbuf: flush failed");
	return (0);
}

void
vflushbuf(vp, sync)
	register struct vnode *vp;
	int sync;
{
	register struct buf *bp, *nbp;
	int s;

loop:
	s = splbio();
	for (bp = vp->v_dirtyblkhd.lh_first; bp; bp = nbp) {
		nbp = bp->b_vnbufs.le_next;
		if ((bp->b_flags & B_BUSY))
			continue;
		if ((bp->b_flags & B_DELWRI) == 0)
			panic("vflushbuf: not dirty");
		bp->b_flags |= B_BUSY | B_VFLUSH;
		splx(s);
		/*
		 * Wait for I/O associated with indirect blocks to complete,
		 * since there is no way to quickly wait for them below.
		 */
		if (bp->b_vp == vp || sync == 0)
			(void) bawrite(bp);
		else
			(void) bwrite(bp);
		goto loop;
	}
	if (sync == 0) {
		splx(s);
		return;
	}
	while (vp->v_numoutput) {
		vp->v_flag |= VBWAIT;
		tsleep((caddr_t)&vp->v_numoutput, PRIBIO + 1, "vflushbuf", 0);
	}
	splx(s);
	if (vp->v_dirtyblkhd.lh_first != NULL) {
		vprint("vflushbuf: dirty", vp);
		goto loop;
	}
}

/*
 * Associate a buffer with a vnode.
 */
void
bgetvp(vp, bp)
	register struct vnode *vp;
	register struct buf *bp;
{

	if (bp->b_vp)
		panic("bgetvp: not free");
	VHOLD(vp);
	bp->b_vp = vp;
	if (vp->v_type == VBLK || vp->v_type == VCHR)
		bp->b_dev = vp->v_rdev;
	else
		bp->b_dev = NODEV;
	/*
	 * Insert onto list for new vnode.
	 */
	bufinsvn(bp, &vp->v_cleanblkhd);
}

/*
 * Disassociate a buffer from a vnode.
 */
void
brelvp(bp)
	register struct buf *bp;
{
	struct vnode *vp;
	struct buf *wasdirty;

	if ((vp = bp->b_vp) == (struct vnode *) 0)
		panic("brelvp: NULL");
	/*
	 * Delete from old vnode list, if on one.
	 */
	wasdirty = vp->v_dirtyblkhd.lh_first;
	if (bp->b_vnbufs.le_next != NOLIST)
		bufremvn(bp);
	if (wasdirty && LIST_FIRST(&vp->v_dirtyblkhd) == NULL)
		LIST_REMOVE(vp, v_synclist);
	bp->b_vp = (struct vnode *) 0;
	HOLDRELE(vp);
}

/*
 * Reassign a buffer from one vnode to another. Used to assign buffers
 * to the appropriate clean or dirty list and to add newly dirty vnodes
 * to the appropriate filesystem syncer list.
 */
void
reassignbuf(bp, newvp)
	register struct buf *bp;
	register struct vnode *newvp;
{
	struct buflists *listheadp;
	struct buf *wasdirty;
	int delay;

	if (newvp == NULL) {
		printf("reassignbuf: NULL");
		return;
	}
	/*
	 * Delete from old vnode list, if on one.
	 */
	wasdirty = newvp->v_dirtyblkhd.lh_first;
	if (bp->b_vnbufs.le_next != NOLIST)
		bufremvn(bp);
	/*
	 * If dirty, put on list of dirty buffers;
	 * otherwise insert onto list of clean buffers.
	 */
	if ((bp->b_flags & B_DELWRI) == 0) {
		listheadp = &newvp->v_cleanblkhd;
		if (wasdirty && LIST_FIRST(&newvp->v_dirtyblkhd) == NULL)
			LIST_REMOVE(newvp, v_synclist);
	} else {
		listheadp = &newvp->v_dirtyblkhd;
		if (LIST_FIRST(listheadp) == NULL) {
			switch (newvp->v_type) {
			case VDIR:
				delay = syncdelay / 3;
				break;
			case VBLK:
				if (newvp->v_specmountpoint != NULL) {
					delay = syncdelay / 2;
					break;
				}
				/* fall through */
			default:
				delay = syncdelay;
			}
			vn_syncer_add_to_worklist(newvp, delay);
		}
	}
	bufinsvn(bp, listheadp);
}

d1766 237
@


1.16
log
@Fix non-DIAGNOSTIC (and non-COMPAT*) compilation
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.15 1997/11/07 23:01:37 csapuntz Exp $	*/
a102 11
/*
 * The workitem queue.
  */ 
#define SYNCER_MAXDELAY       32
int syncer_maxdelay = SYNCER_MAXDELAY;        /* maximum delay time */
time_t syncdelay = 30;                        /* time to delay syncing vnodes */
 
static int syncer_delayno = 0;
static long syncer_mask;
LIST_HEAD(synclist, vnode);
static struct synclist *syncer_workitem_pending;
a103 2
int vfs_lock __P((struct mount *));
void vfs_unlock __P((struct mount *));
d134 1
a134 4
	syncer_workitem_pending = hashinit(syncer_maxdelay, M_VNODE,
					   &syncer_mask);
	syncer_maxdelay = syncer_mask + 1;

d201 1
a201 1
 {
d485 3
a668 105
 * The workitem queue.
 * 
 * It is useful to delay writes of file data and filesystem metadata
 * for tens of seconds so that quickly created and deleted files need
 * not waste disk bandwidth being created and removed. To realize this,
 * we append vnodes to a "workitem" queue. When running with a soft
 * updates implementation, most pending metadata dependencies should
 * not wait for more than a few seconds. Thus, mounted on block devices
 * are delayed only about a half the time that file data is delayed.
 * Similarly, directory updates are more critical, so are only delayed
 * about a third the time that file data is delayed. Thus, there are
 * SYNCER_MAXDELAY queues that are processed round-robin at a rate of
 * one each second (driven off the filesystem syner process). The
 * syncer_delayno variable indicates the next queue that is to be processed.
 * Items that need to be processed soon are placed in this queue:
 *
 *	syncer_workitem_pending[syncer_delayno]
 *
 * A delay of fifteen seconds is done by placing the request fifteen
 * entries later in the queue:
 *
 *	syncer_workitem_pending[(syncer_delayno + 15) & syncer_mask]
 *
 */

/*
 * Add an item to the syncer work queue.
 */
void
vn_syncer_add_to_worklist(vp, delay)
	struct vnode *vp;
	int delay;
{
	int s, slot;

	s = splbio();
	if (delay > syncer_maxdelay - 2)
		delay = syncer_maxdelay - 2;
	slot = (syncer_delayno + delay) & syncer_mask;
	LIST_INSERT_HEAD(&syncer_workitem_pending[slot], vp, v_synclist);
	splx(s);
}

/*
 * System filesystem synchronizer daemon.
 */

extern int lbolt;

void 
sched_sync(p)
	struct proc *p;
{
	struct synclist *slp;
	struct vnode *vp;
	long starttime;
	int s;

	for (;;) {
		starttime = time.tv_sec;

		/*
		 * Push files whose dirty time has expired.
		 */
		s = splbio();
		slp = &syncer_workitem_pending[syncer_delayno];
		syncer_delayno += 1;
		if (syncer_delayno == syncer_maxdelay)
			syncer_delayno = 0;
		splx(s);
		while ((vp = LIST_FIRST(slp)) != NULL) {
			vn_lock(vp, LK_EXCLUSIVE | LK_RETRY, p);
			(void) VOP_FSYNC(vp, p->p_ucred, MNT_LAZY, p);
			VOP_UNLOCK(vp, 0, p);
			if (LIST_FIRST(slp) == vp) {
				if (LIST_FIRST(&vp->v_dirtyblkhd) == NULL)
					panic("sched_sync: fsync failed");
				/*
				 * Move ourselves to the back of the sync list.
				 */
				LIST_REMOVE(vp, v_synclist);
				vn_syncer_add_to_worklist(vp, syncdelay);
			}
		}

		/*
		 * Do soft update processing.
		 */
		if (bioops.io_sync)
			(*bioops.io_sync)(NULL);

		/*
		 * If it has taken us less than a second to process the
		 * current work, then wait. Otherwise start right over
		 * again. We can still lose time if any single round
		 * takes more than two seconds, but it does not really
		 * matter as we are just trying to generally pace the
		 * filesystem activity.
		 */
		if (time.tv_sec == starttime)
			tsleep(&lbolt, PPAUSE, "syncer", 0);
	}
}

/*
a901 109
/*
 * Stubs to use when there is no locking to be done on the underlying object.
 * A minimal shared lock is necessary to ensure that the underlying object
 * is not revoked while an operation is in progress. So, an active shared
 * count is maintained in an auxillary vnode lock structure.
 */
int
vop_nolock(v)
	void *v;
{
	struct vop_lock_args /* {
		struct vnode *a_vp;
		int a_flags;
		struct proc *a_p;
	} */ *ap = v;

#ifdef notyet
	/*
	 * This code cannot be used until all the non-locking filesystems
	 * (notably NFS) are converted to properly lock and release nodes.
	 * Also, certain vnode operations change the locking state within
	 * the operation (create, mknod, remove, link, rename, mkdir, rmdir,
	 * and symlink). Ideally these operations should not change the
	 * lock state, but should be changed to let the caller of the
	 * function unlock them. Otherwise all intermediate vnode layers
	 * (such as union, umapfs, etc) must catch these functions to do
	 * the necessary locking at their layer. Note that the inactive
	 * and lookup operations also change their lock state, but this 
	 * cannot be avoided, so these two operations will always need
	 * to be handled in intermediate layers.
	 */
	struct vnode *vp = ap->a_vp;
	int vnflags, flags = ap->a_flags;

	if (vp->v_vnlock == NULL) {
		if ((flags & LK_TYPE_MASK) == LK_DRAIN)
			return (0);
		MALLOC(vp->v_vnlock, struct lock *, sizeof(struct lock),
		    M_VNODE, M_WAITOK);
		lockinit(vp->v_vnlock, PVFS, "vnlock", 0, 0);
	}
	switch (flags & LK_TYPE_MASK) {
	case LK_DRAIN:
		vnflags = LK_DRAIN;
		break;
	case LK_EXCLUSIVE:
	case LK_SHARED:
		vnflags = LK_SHARED;
		break;
	case LK_UPGRADE:
	case LK_EXCLUPGRADE:
	case LK_DOWNGRADE:
		return (0);
	case LK_RELEASE:
	default:
		panic("vop_nolock: bad operation %d", flags & LK_TYPE_MASK);
	}
	if (flags & LK_INTERLOCK)
		vnflags |= LK_INTERLOCK;
	return(lockmgr(vp->v_vnlock, vnflags, &vp->v_interlock, ap->a_p));
#else /* for now */
        /*
         * Since we are not using the lock manager, we must clear
         * the interlock here.
         */
        if (ap->a_flags & LK_INTERLOCK)
                simple_unlock(&ap->a_vp->v_interlock);
        return (0);
#endif
}
 
/*
 * Decrement the active use count.
 */

int
vop_nounlock(v)
	void *v;
{
	struct vop_unlock_args /* {
		struct vnode *a_vp;
		int a_flags;
		struct proc *a_p;
	} */ *ap = v;

	struct vnode *vp = ap->a_vp;

	if (vp->v_vnlock == NULL)
		return (0);
	return (lockmgr(vp->v_vnlock, LK_RELEASE, NULL, ap->a_p));
}

/*
 * Return whether or not the node is in use.
 */
int
vop_noislocked(v)
	void *v;
{
	struct vop_islocked_args /* {
		struct vnode *a_vp;
	} */ *ap = v;

	struct vnode *vp = ap->a_vp;

	if (vp->v_vnlock == NULL)
		return (0);
	return (lockstatus(vp->v_vnlock));
}
a1250 62
/*
 * Eliminate all activity associated with  the requested vnode
 * and with all vnodes aliased to the requested vnode.
 */
int
vop_revoke(v)
	void *v;
{
	struct vop_revoke_args /* {
	        struct vnode *a_vp;
		int a_flags;
	} */ *ap = v;
	struct vnode *vp, *vq;
	struct proc *p = curproc;

#ifdef DIAGNOSTIC
	if ((ap->a_flags & REVOKEALL) == 0)
		panic("vop_revoke");
#endif

	vp = ap->a_vp;
	simple_lock(&vp->v_interlock);
 
	if (vp->v_flag & VALIASED) {
		/*
		 * If a vgone (or vclean) is already in progress,
		 * wait until it is done and return.
		 */
		if (vp->v_flag & VXLOCK) {
			vp->v_flag |= VXWANT;
			simple_unlock(&vp->v_interlock);
			tsleep((caddr_t)vp, PINOD, "vop_revokeall", 0);
			return(0);
		}
		/*
		 * Ensure that vp will not be vgone'd while we
		 * are eliminating its aliases.
		 */
		vp->v_flag |= VXLOCK;
		simple_unlock(&vp->v_interlock);
		while (vp->v_flag & VALIASED) {
			simple_lock(&spechash_slock);
			for (vq = *vp->v_hashchain; vq; vq = vq->v_specnext) {
				if (vq->v_rdev != vp->v_rdev ||
				    vq->v_type != vp->v_type || vp == vq)
					continue;
				simple_unlock(&spechash_slock);
				vgone(vq);
				break;
			}
		}
		/*
		 * Remove the lock so that vgone below will
		 * really eliminate the vnode after which time
		 * vgone will awaken any sleepers.
		 */
		simple_lock(&vp->v_interlock);
		vp->v_flag &= ~VXLOCK;
	}
	vgonel(vp, p);
	return (0);
}
a2000 156
/*
 * Routine to create and manage a filesystem syncer vnode.
 */
#define sync_close nullop
int   sync_fsync __P((void *));
int   sync_inactive __P((void *));
#define sync_reclaim nullop
#define sync_lock vop_nolock
#define sync_unlock vop_nounlock
int   sync_print __P((void *));
#define sync_islocked vop_noislocked
 
int (**sync_vnodeop_p) __P((void *));
struct vnodeopv_entry_desc sync_vnodeop_entries[] = {
      { &vop_default_desc, vn_default_error },
      { &vop_close_desc, sync_close },                /* close */
      { &vop_fsync_desc, sync_fsync },                /* fsync */
      { &vop_inactive_desc, sync_inactive },          /* inactive */
      { &vop_reclaim_desc, sync_reclaim },            /* reclaim */
      { &vop_lock_desc, sync_lock },                  /* lock */
      { &vop_unlock_desc, sync_unlock },              /* unlock */
      { &vop_print_desc, sync_print },                /* print */
      { &vop_islocked_desc, sync_islocked },          /* islocked */
      { (struct vnodeop_desc*)NULL, (int(*) __P((void *)))NULL }
};
struct vnodeopv_desc sync_vnodeop_opv_desc =
      { &sync_vnodeop_p, sync_vnodeop_entries };

/*
 * Create a new filesystem syncer vnode for the specified mount point.
 */
int
vfs_allocate_syncvnode(mp)
      struct mount *mp;
{
      struct vnode *vp;
      static long start, incr, next;
      int error;

      /* Allocate a new vnode */
      if ((error = getnewvnode(VT_VFS, mp, sync_vnodeop_p, &vp)) != 0) {
              mp->mnt_syncer = NULL;
              return (error);
      }
      vp->v_writecount = 1;
      vp->v_type = VNON;
      /*
       * Place the vnode onto the syncer worklist. We attempt to
       * scatter them about on the list so that they will go off
       * at evenly distributed times even if all the filesystems
       * are mounted at once.
       */
      next += incr;
      if (next == 0 || next > syncer_maxdelay) {
              start /= 2;
              incr /= 2;
              if (start == 0) {
                      start = syncer_maxdelay / 2;
                      incr = syncer_maxdelay;
              }
              next = start;
      }
      vn_syncer_add_to_worklist(vp, next);
      mp->mnt_syncer = vp;
      return (0);
}

/*
 * Do a lazy sync of the filesystem.
 */
int
sync_fsync(v)
	void *v;
{
      struct vop_fsync_args /* {
              struct vnode *a_vp;
              struct ucred *a_cred;
              int a_waitfor;
              struct proc *a_p;
      } */ *ap = v;

      struct vnode *syncvp = ap->a_vp;
      struct mount *mp = syncvp->v_mount;
      int asyncflag;

      /*
       * We only need to do something if this is a lazy evaluation.
       */
      if (ap->a_waitfor != MNT_LAZY)
              return (0);

      /*
       * Move ourselves to the back of the sync list.
       */
      LIST_REMOVE(syncvp, v_synclist);
      vn_syncer_add_to_worklist(syncvp, syncdelay);

      /*
       * Walk the list of vnodes pushing all that are dirty and
       * not already on the sync list.
       */
      simple_lock(&mountlist_slock);
      if (vfs_busy(mp, LK_NOWAIT, &mountlist_slock, ap->a_p) == 0) {
              asyncflag = mp->mnt_flag & MNT_ASYNC;
              mp->mnt_flag &= ~MNT_ASYNC;
              VFS_SYNC(mp, MNT_LAZY, ap->a_cred, ap->a_p);
              if (asyncflag)
                      mp->mnt_flag |= MNT_ASYNC;
              vfs_unbusy(mp, ap->a_p);
      }
      return (0);
}

/*
 * The syncer vnode is no longer needed and is being decommissioned.
 */
int
sync_inactive(v)
	void *v;
	
{
      struct vop_inactive_args /* {
               struct vnode *a_vp;
               struct proc *a_p;
      } */ *ap = v;

      struct vnode *vp = ap->a_vp;

      if (vp->v_usecount == 0)
              return (0);
      vp->v_mount->mnt_syncer = NULL;
      LIST_REMOVE(vp, v_synclist);
      vp->v_writecount = 0;
      vput(vp);
      return (0);
}

/*
 * Print out a syncer vnode.
 */
int
sync_print(v)
	void *v;

{
      struct vop_print_args /* {
              struct vnode *a_vp;
      } */ *ap = v;
      struct vnode *vp = ap->a_vp;

      printf("syncer vnode");
      if (vp->v_vnlock != NULL)
              lockmgr_printinfo(vp->v_vnlock);
      printf("\n");
      return (0);
}
@


1.15
log
@Fixed hang on shutdown
Disabled vop_nolock for now. Filesystems still need to be cleaned up.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.14 1997/11/06 22:46:19 csapuntz Exp $	*/
a116 5
struct mount *getvfs __P((fsid_t *));
long makefstype __P((char *));
void vattr_null __P((struct vattr *));
int getnewvnode __P((enum vtagtype, struct mount *, int (**)(void *),
		     struct vnode **));
a117 6
int vinvalbuf __P((struct vnode *, int, struct ucred *, struct proc *, int,
		   int));
void vflushbuf __P((struct vnode *, int));
void brelvp __P((struct buf *));
int bdevvp __P((dev_t, struct vnode **));
int cdevvp __P((dev_t, struct vnode **));
a118 4
struct vnode *checkalias __P((struct vnode *, dev_t, struct mount *));
void vref __P((struct vnode *));
void vput __P((struct vnode *));
void vrele __P((struct vnode *));
a119 15
void vhold __P((struct vnode *));
void holdrele __P((struct vnode *));
int vflush __P((struct mount *, struct vnode *, int));
void vgoneall __P((struct vnode *));
void vgone __P((struct vnode *));
void vgonel __P((struct vnode *, struct proc *));
int vcount __P((struct vnode *));
void vprint __P((char *, struct vnode *));
int vfs_mountedon __P((struct vnode *));
int vfs_export __P((struct mount *, struct netexport *, struct export_args *));
struct netcred *vfs_export_lookup __P((struct mount *, struct netexport *,
				       struct mbuf *));
int vaccess __P((mode_t, uid_t, gid_t, mode_t, struct ucred *));
void vfs_unmountall __P((void));
void vfs_shutdown __P((void));
d1130 1
d1144 1
a1144 1

@


1.14
log
@DEBUG now compiles
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.11 1997/10/06 15:12:42 csapuntz Exp $	*/
d1066 1
d1110 9
@


1.13
log
@Updates for VFS Lite 2 + soft update.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.12 1997/10/06 20:20:12 deraadt Exp $	*/
a1065 1
#ifdef notyet
a1108 9
#else /* for now */
	/*
	 * Since we are not using the lock manager, we must clear
	 * the interlock here.
	 */
	if (ap->a_flags & LK_INTERLOCK)
		simple_unlock(&ap->a_vp->v_interlock);
 	return (0);
#endif
d1798 1
a1798 1
	for (mp = mountlist.cqh_first; mp != (void *)&mountlist;
d1801 1
a1801 1
			nmp = mp->mnt_list.cque_next;
d1809 1
d1811 1
a1811 1
		nmp = mp->mnt_list.cqe_next;
@


1.12
log
@back out vfs lite2 till after 2.2
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.10 1997/04/25 09:33:24 deraadt Exp $	*/
d54 1
d92 4
a95 1
TAILQ_HEAD(freelst, vnode) vnode_free_list;	/* vnode free list */
d97 17
a130 1
int vget __P((struct vnode *, int));
d134 1
d140 1
d151 1
a151 1
static int vfs_hang_addrlist __P((struct mount *, struct netexport *,
d153 2
a154 2
static int vfs_free_netcred __P((struct radix_node *, void *));
static void vfs_free_addrlist __P((struct netexport *));
d167 4
d172 1
d174 7
d183 1
d185 2
a186 2
 * Lock a filesystem.
 * Used to prevent access to it while mounting and unmounting.
d188 1
d190 5
a194 2
vfs_lock(mp)
	register struct mount *mp;
d196 1
d198 3
a200 1
	while (mp->mnt_flag & MNT_MLOCK) {
d202 12
a213 1
		tsleep((caddr_t)mp, PVFS, "vfslock", 0);
d215 6
a220 2
	mp->mnt_flag |= MNT_MLOCK;
	return (0);
d223 1
d225 1
a225 2
 * Unlock a locked filesystem.
 * Panic if filesystem is not locked.
d228 3
a230 2
vfs_unlock(mp)
	register struct mount *mp;
d232 1
a232 8

	if ((mp->mnt_flag & MNT_MLOCK) == 0)
		panic("vfs_unlock: not locked");
	mp->mnt_flag &= ~MNT_MLOCK;
	if (mp->mnt_flag & MNT_MWAIT) {
		mp->mnt_flag &= ~MNT_MWAIT;
		wakeup((caddr_t)mp);
	}
d236 4
a239 2
 * Mark a mount point as busy.
 * Used to synchronize access and to delay unmounting.
d241 41
d283 1
a283 2
vfs_busy(mp)
	register struct mount *mp;
d285 14
a298 9

	while(mp->mnt_flag & MNT_MPBUSY) {
		mp->mnt_flag |= MNT_MPWANT;
		tsleep((caddr_t)&mp->mnt_flag, PVFS, "vfsbusy", 0);
	}
	if (mp->mnt_flag & MNT_UNMOUNT)
		return (1);
	mp->mnt_flag |= MNT_MPBUSY;
	return (0);
d300 1
a300 19

/*
 * Free a busy filesystem.
 * Panic if filesystem is not busy.
 */
void
vfs_unbusy(mp)
	register struct mount *mp;
{

	if ((mp->mnt_flag & MNT_MPBUSY) == 0)
		panic("vfs_unbusy: not busy");
	mp->mnt_flag &= ~MNT_MPBUSY;
	if (mp->mnt_flag & MNT_MPWANT) {
		mp->mnt_flag &= ~MNT_MPWANT;
		wakeup((caddr_t)&mp->mnt_flag);
	}
}

d305 1
a305 1
getvfs(fsid)
d310 1
d312 1
a312 1
	     mp = mp->mnt_list.cqe_next)
d314 2
a315 1
		    mp->mnt_stat.f_fsid.val[1] == fsid->val[1])
d317 3
d323 1
d328 1
a328 1
getnewfsid(mp, mtype)
a329 1
	int mtype;
d334 1
d336 3
a338 1
	mp->mnt_stat.f_fsid.val[0] = makedev(nblkdev + 11, 0);	/* XXX */
d342 1
a342 1
	tfsid.val[0] = makedev((nblkdev + mtype) & 0xff, xxxfs_mntid);
d345 1
a345 1
		while (getvfs(&tfsid)) {
d351 1
d410 4
a413 1
	register struct vnode *vp;
d418 25
a442 3
	if ((vnode_free_list.tqh_first == NULL &&
	     numvnodes < 2 * desiredvnodes) ||
	    numvnodes < desiredvnodes) {
d448 12
a459 1
		if ((vp = vnode_free_list.tqh_first) == NULL) {
d468 1
a468 1
		TAILQ_REMOVE(&vnode_free_list, vp, v_freelist);
d470 2
a471 1
		vp->v_freelist.tqe_prev = (struct vnode **)0xdeadb;
d474 3
a476 1
			vgone(vp);
d516 1
a516 1

d520 1
d526 3
a528 3
	if ((vp->v_mount = mp) == NULL)
		return;
	LIST_INSERT_HEAD(&mp->mnt_vnodelist, vp, v_mntvnodes);
d567 1
a567 1
	if (flags & V_SAVE) {
d574 2
a575 1
		if ((blist = vp->v_cleanblkhd.lh_first) && flags & V_SAVEMETA)
d695 1
d697 1
a697 1
	if (bp->b_vp == (struct vnode *) 0)
d702 1
d705 2
a706 1
	vp = bp->b_vp;
d712 108
a819 3
 * Reassign a buffer from one vnode to another.
 * Used to assign file specific control information
 * (indirect blocks) to the vnode to which they belong.
d826 3
a828 1
	register struct buflists *listheadp;
d837 1
d844 5
a848 1
	if (bp->b_flags & B_DELWRI)
d850 17
a866 2
	else
		listheadp = &newvp->v_cleanblkhd;
d912 2
a913 1
	if (dev == NODEV)
d915 1
d945 1
d954 1
d956 1
d963 2
a964 1
			vgone(vp);
d967 2
a968 1
		if (vget(vp, 1))
d970 1
d979 1
a979 1
		nvp->v_specflags = 0;
d981 1
d983 1
a983 1
		if (vp != NULL) {
d990 4
a993 2
	VOP_UNLOCK(vp);
	vclean(vp, 0);
d1010 4
a1013 3
vget(vp, lockflag)
	register struct vnode *vp;
	int lockflag;
d1015 1
a1015 1

d1019 8
a1026 11
	 * return failure. Cleaning is determined either by checking
	 * that the VXLOCK flag is set, or that the use count is
	 * zero with the back pointer set to show that it has been
	 * removed from the free list by getnewvnode. The VXLOCK
	 * flag may not have been set yet because vclean is blocked in
	 * the VOP_LOCK call waiting for the VOP_INACTIVE to complete.
	 */
	if ((vp->v_flag & VXLOCK) ||
	    (vp->v_usecount == 0 &&
	     vp->v_freelist.tqe_prev == (struct vnode **)0xdeadb)) {
		vp->v_flag |= VXWANT;
d1028 17
a1044 1
		return (1);
d1046 1
a1046 5
	if (vp->v_usecount == 0)
		TAILQ_REMOVE(&vnode_free_list, vp, v_freelist);
	vp->v_usecount++;
	if (lockflag)
		VOP_LOCK(vp);
d1051 111
a1161 1
 * Vnode reference, just increment the count
d1167 1
a1167 1

d1171 1
d1174 4
a1177 6
/*
 * vput(), just unlock and vrele()
 */
void
vput(vp)
	register struct vnode *vp;
d1179 25
d1205 1
a1205 2
	VOP_UNLOCK(vp);
	vrele(vp);
d1209 1
a1209 2
 * Vnode release.
 * If count drops to zero, call inactive routine and return to freelist.
d1212 1
a1212 1
vrele(vp)
d1215 1
d1217 1
a1217 1
#ifdef DIAGNOSTIC
d1219 1
a1219 1
		panic("vrele: null vp");
d1221 1
d1223 3
a1225 1
	if (vp->v_usecount > 0)
d1227 1
d1229 3
a1231 3
	if (vp->v_usecount != 0 || vp->v_writecount != 0) {
		vprint("vrele: bad ref count", vp);
		panic("vrele: ref cnt");
d1237 23
a1259 2
	TAILQ_INSERT_TAIL(&vnode_free_list, vp, v_freelist);
	VOP_INACTIVE(vp);
d1262 1
d1271 19
d1291 1
d1302 1
d1306 14
d1321 1
d1342 1
d1346 1
a1346 2
	if ((mp->mnt_flag & MNT_MPBUSY) == 0)
		panic("vflush: not busy");
d1357 2
d1362 2
a1363 1
		if ((flags & SKIPSYSTEM) && (vp->v_flag & VSYSTEM))
d1365 1
d1371 2
a1372 1
		    (vp->v_writecount == 0 || vp->v_type != VREG))
d1374 1
d1380 3
a1382 1
			vgone(vp);
d1391 1
d1393 1
a1393 1
				vgone(vp);
d1395 1
a1395 1
				vclean(vp, 0);
d1399 1
d1406 1
d1409 1
d1417 1
d1420 1
a1420 1
vclean(vp, flags)
d1423 1
d1434 11
a1444 1
		VREF(vp);
d1452 2
a1453 8
	VOP_LOCK(vp);
	/*
	 * Prevent the vnode from being recycled or
	 * brought into use while we clean it out.
	 */
	if (vp->v_flag & VXLOCK)
		panic("vclean: deadlock");
	vp->v_flag |= VXLOCK;
d1458 1
a1458 6
		vinvalbuf(vp, V_SAVE, NOCRED, NULL, 0, 0);
	/*
	 * Any other processes trying to obtain this lock must first
	 * wait for VXLOCK to clear, then call the new lock operation.
	 */
	VOP_UNLOCK(vp);
d1461 2
a1462 1
	 * deactivated before being reclaimed.
d1466 8
a1473 2
			VOP_CLOSE(vp, FNONBLOCK, NOCRED, NULL);
		VOP_INACTIVE(vp);
d1475 1
d1479 1
a1479 1
	if (VOP_RECLAIM(vp))
d1481 13
a1493 2
	if (active)
		vrele(vp);
d1511 3
a1513 3
void
vgoneall(vp)
	register struct vnode *vp;
d1515 11
a1525 1
	register struct vnode *vq;
d1527 3
d1537 3
a1539 2
			tsleep((caddr_t)vp, PINOD, "vgoneall", 0);
			return;
d1546 1
d1548 1
d1553 1
d1563 1
d1566 25
a1590 1
	vgone(vp);
d1601 14
d1624 1
d1631 1
a1631 1
	vclean(vp, DOCLOSE);
d1635 2
a1636 1
	insmntque(vp, (struct mount *)0);
d1638 2
a1639 1
	 * If special device, remove it from special device alias list.
d1641 2
a1642 1
	if (vp->v_type == VBLK || vp->v_type == VCHR) {
d1671 1
d1677 14
a1690 16
	 * move it to the head of the list. The test of the back
	 * pointer and the reference count of zero is because
	 * it will be removed from the free list by getnewvnode,
	 * but will not have its reference count incremented until
	 * after calling vgone. If the reference count were
	 * incremented first, vgone would (incorrectly) try to
	 * close the previous instance of the underlying object.
	 * So, the back pointer is explicitly set to `0xdeadb' in
	 * getnewvnode after removing it from the freelist to ensure
	 * that we do not try to move it here.
	 */
	if (vp->v_usecount == 0 &&
	    vp->v_freelist.tqe_prev != (struct vnode **)0xdeadb &&
	    vnode_free_list.tqh_first != vp) {
		TAILQ_REMOVE(&vnode_free_list, vp, v_freelist);
		TAILQ_INSERT_HEAD(&vnode_free_list, vp, v_freelist);
d1705 1
d1707 1
d1712 2
a1713 1
		return (1);
d1715 2
a1716 1
	return (0);
d1724 1
a1724 1
	register struct vnode *vp;
d1726 1
a1726 1
	register struct vnode *vq, *vnext;
d1732 1
d1741 1
d1747 1
d1802 2
a1803 1
	register struct mount *mp;
d1807 1
d1809 5
a1813 1
	     mp = mp->mnt_list.cqe_next) {
d1816 1
a1816 1
		     vp = vp->v_mntvnodes.le_next)
d1819 49
d1869 1
d1871 1
a1871 1
#endif
d1882 1
a1882 1
sysctl_vnode(where, sizep)
d1885 1
d1888 1
a1888 1
	struct vnode *vp;
d1901 1
d1903 2
a1904 2
		nmp = mp->mnt_list.cqe_next;
		if (vfs_busy(mp))
d1906 1
d1911 1
a1911 1
		     vp = vp->v_mntvnodes.le_next) {
d1918 1
d1924 1
d1926 1
d1934 1
d1936 5
a1940 1
		vfs_unbusy(mp);
d1943 2
d1957 1
d1959 1
a1959 1
	if (vp->v_specflags & SI_MOUNTEDON)
d1962 1
d1967 6
a1972 3
			if (vq->v_specflags & SI_MOUNTEDON)
				return (EBUSY);
		}
d1974 1
a1974 1
	return (0);
d1981 1
a1981 1
static int
d2055 1
a2055 1
static int
d2070 1
a2070 1
static void
d2207 1
a2207 1
		if ((error = dounmount(mp, MNT_FORCE, &proc0)) != 0) {
d2317 158
@


1.11
log
@VFS Lite2 Changes
@
text
@a53 1
#include <sys/kernel.h>
d91 1
a91 4

struct freelst vnode_hold_list;   /* list of vnodes referencing buffers */
struct freelst vnode_free_list;   /* vnode free list */

a92 17
struct simplelock mountlist_slock;
static struct simplelock mntid_slock;
struct simplelock mntvnode_slock;
struct simplelock vnode_free_list_slock;
static struct simplelock spechash_slock;

/*
 * The workitem queue.
  */ 
#define SYNCER_MAXDELAY       32
int syncer_maxdelay = SYNCER_MAXDELAY;        /* maximum delay time */
time_t syncdelay = 30;                        /* time to delay syncing vnodes */
 
static int syncer_delayno = 0;
static long syncer_mask;
LIST_HEAD(synclist, vnode);
static struct synclist *syncer_workitem_pending;
d110 1
a113 1
int vunref __P((struct vnode *));
a118 1
void vgonel __P((struct vnode *, struct proc *));
d129 1
a129 1
int vfs_hang_addrlist __P((struct mount *, struct netexport *,
d131 2
a132 2
int vfs_free_netcred __P((struct radix_node *, void *));
void vfs_free_addrlist __P((struct netexport *));
a144 4
	simple_lock_init(&mntvnode_slock);
	simple_lock_init(&mntid_slock);
	simple_lock_init(&spechash_slock);
	TAILQ_INIT(&vnode_hold_list);
a145 1
	simple_lock_init(&vnode_free_list_slock);
a146 7
	/*
	 * Initialize the filesystem syncer.
	 */
	syncer_workitem_pending = hashinit(syncer_maxdelay, M_VNODE,
					   &syncer_mask);
	syncer_maxdelay = syncer_mask + 1;

a148 1

d150 2
a151 2
 * Mark a mount point as busy. Used to synchornize access and to delay
 * unmounting. Interlock is not released n failure.
a152 1

d154 2
a155 5
vfs_busy(mp, flags, interlkp, p)
	struct mount *mp;
	int flags;
	struct simplelock *interlkp;
	struct proc *p;
a156 1
	int lkflags;
d158 1
a158 3
	if (mp->mnt_flag & MNT_UNMOUNT) {
		if (flags & LK_NOWAIT)
			return (ENOENT);
d160 1
a160 12
		if (interlkp)
			simple_unlock(interlkp);
		/*
		 * Since all busy locks are shared except the exclusive
		 * lock granted when unmounting, the only place that a
		 * wakeup needs to be done is at the release of the
		 * exclusive lock at the end of dounmount.
		 */
 		sleep((caddr_t)mp, PVFS);
		if (interlkp)
			simple_lock(interlkp);
		return (ENOENT);
d162 2
a163 6
	lkflags = LK_SHARED;
	if (interlkp)
		lkflags |= LK_INTERLOCK;
	if (lockmgr(&mp->mnt_lock, lkflags, interlkp, p))
		panic("vfs_busy: unexpected lock failure");
        return (0);
a165 1

d167 2
a168 1
 * Free a busy file system
d171 2
a172 3
vfs_unbusy(mp, p)
	struct mount *mp;
	struct proc *p;
d174 8
a181 1
	lockmgr(&mp->mnt_lock, LK_RELEASE, NULL, p);
d185 2
a186 4
 * Lookup a filesystem type, and if found allocate and initialize
 * a mount structure for it.
 *
 * Devname is usually updated by mount(8) after booting.
d188 14
d203 7
a209 42
int
vfs_rootmountalloc(fstypename, devname, mpp)
	char *fstypename;
	char *devname;
	struct mount **mpp;
 {
	struct proc *p = curproc;	/* XXX */
	struct vfsconf *vfsp;
	struct mount *mp;
 
	for (vfsp = vfsconf; vfsp; vfsp = vfsp->vfc_next)
		if (!strcmp(vfsp->vfc_name, fstypename))
			break;
	if (vfsp == NULL)
		return (ENODEV);
	mp = malloc((u_long)sizeof(struct mount), M_MOUNT, M_WAITOK);
	bzero((char *)mp, (u_long)sizeof(struct mount));
	lockinit(&mp->mnt_lock, PVFS, "vfslock", 0, 0);
	(void)vfs_busy(mp, LK_NOWAIT, 0, p);
	LIST_INIT(&mp->mnt_vnodelist);
	mp->mnt_vfc = vfsp;
	mp->mnt_op = vfsp->vfc_vfsops;
	mp->mnt_flag = MNT_RDONLY;
	mp->mnt_vnodecovered = NULLVP;
	vfsp->vfc_refcount++;
	mp->mnt_stat.f_type = vfsp->vfc_typenum;
	mp->mnt_flag |= vfsp->vfc_flags & MNT_VISFLAGMASK;
	strncpy(mp->mnt_stat.f_fstypename, vfsp->vfc_name, MFSNAMELEN);
	mp->mnt_stat.f_mntonname[0] = '/';
	(void) copystr(devname, mp->mnt_stat.f_mntfromname, MNAMELEN - 1, 0);
	*mpp = mp;
 	return (0);
 }

/*
 * Find an appropriate filesystem to use for the root. If a filesystem
 * has not been preselected, walk through the list of known filesystems
 * trying those that have mountroot routines, and try them until one
 * works or we have tried them all.
  */
int
vfs_mountroot()
d211 8
a218 14
	struct vfsconf *vfsp;
	extern int (*mountroot)(void);
	int error;
 
	if (mountroot != NULL)
		return ((*mountroot)());
	for (vfsp = vfsconf; vfsp; vfsp = vfsp->vfc_next) {
		if (vfsp->vfc_mountroot == NULL)
			continue;
		if ((error = (*vfsp->vfc_mountroot)()) == 0)
			return (0);
		printf("%s_mountroot failed: %d\n", vfsp->vfc_name, error);
 	}
	return (ENODEV);
d220 1
a220 1
 
d225 1
a225 1
vfs_getvfs(fsid)
a229 1
	simple_lock(&mountlist_slock);
d231 1
a231 1
	     mp = mp->mnt_list.cqe_next) {
d233 1
a233 2
		    mp->mnt_stat.f_fsid.val[1] == fsid->val[1]) {
			simple_unlock(&mountlist_slock);
a234 3
		}
	}
	simple_unlock(&mountlist_slock);
a237 1

d242 1
a242 1
vfs_getnewfsid(mp)
d244 1
a248 1
	int mtype;
d250 1
a250 3
	simple_lock(&mntid_slock);
	mtype = mp->mnt_vfc->vfc_typenum;
	mp->mnt_stat.f_fsid.val[0] = makedev(nblkdev + mtype, 0);
d254 1
a254 1
	tfsid.val[0] = makedev(nblkdev + mtype, xxxfs_mntid);
d257 1
a257 1
		while (vfs_getvfs(&tfsid)) {
a262 1
	simple_unlock(&mntid_slock);
d321 1
a321 4
	struct proc *p = curproc;			/* XXX */
	struct freelst *listhd;
	static int toggle;
	struct vnode *vp;
d326 3
a328 25
	/*
	 * We must choose whether to allocate a new vnode or recycle an
	 * existing one. The criterion for allocating a new one is that
	 * the total number of vnodes is less than the number desired or
	 * there are no vnodes on either free list. Generally we only
	 * want to recycle vnodes that have no buffers associated with
	 * them, so we look first on the vnode_free_list. If it is empty,
	 * we next consider vnodes with referencing buffers on the
	 * vnode_hold_list. The toggle ensures that half the time we
	 * will use a buffer from the vnode_hold_list, and half the time
	 * we will allocate a new one unless the list has grown to twice
	 * the desired size. We are reticent to recycle vnodes from the
	 * vnode_hold_list because we will lose the identity of all its
	 * referencing buffers.
	 */
	toggle ^= 1;
	if (numvnodes > 2 * desiredvnodes)
		toggle = 0;


	simple_lock(&vnode_free_list_slock);
	if ((numvnodes < desiredvnodes) ||
	    ((TAILQ_FIRST(listhd = &vnode_free_list) == NULL) &&
	     ((TAILQ_FIRST(listhd = &vnode_hold_list) == NULL) || toggle))) {
		simple_unlock(&vnode_free_list_slock);
d334 1
a334 12
		for (vp = TAILQ_FIRST(listhd); vp != NULLVP;
		     vp = TAILQ_NEXT(vp, v_freelist)) {
			if (simple_lock_try(&vp->v_interlock))
				break;
		}
		/*
		 * Unless this is a bad time of the month, at most
		 * the first NCPUS items on the free list are
		 * locked, so this is close enough to being empty.
		 */
		if (vp == NULLVP) {
			simple_unlock(&vnode_free_list_slock);
d343 1
a343 1
		TAILQ_REMOVE(listhd, vp, v_freelist);
d345 1
a345 2
		vp->v_flag |= VGONEHACK;
		simple_unlock(&vnode_free_list_slock);
d348 1
a348 3
			vgonel(vp, p);
		else
			simple_unlock(&vp->v_interlock);
d388 1
a388 1
	simple_lock(&mntvnode_slock);
a391 1
	
d397 3
a399 3
	if ((vp->v_mount = mp) != NULL)
		LIST_INSERT_HEAD(&mp->mnt_vnodelist, vp, v_mntvnodes);
	simple_unlock(&mntvnode_slock);
d438 1
a438 1
	if ((flags & V_SAVE) && vp->v_dirtyblkhd.lh_first != NULL) {
d445 1
a445 2
		if ((blist = vp->v_cleanblkhd.lh_first) && 
		    (flags & V_SAVEMETA))
a564 1
	struct buf *wasdirty;
d566 1
a566 1
	if ((vp = bp->b_vp) == (struct vnode *) 0)
a570 1
	wasdirty = vp->v_dirtyblkhd.lh_first;
d573 1
a573 2
	if (wasdirty && LIST_FIRST(&vp->v_dirtyblkhd) == NULL)
		LIST_REMOVE(vp, v_synclist);
d579 3
a581 108
 * The workitem queue.
 * 
 * It is useful to delay writes of file data and filesystem metadata
 * for tens of seconds so that quickly created and deleted files need
 * not waste disk bandwidth being created and removed. To realize this,
 * we append vnodes to a "workitem" queue. When running with a soft
 * updates implementation, most pending metadata dependencies should
 * not wait for more than a few seconds. Thus, mounted on block devices
 * are delayed only about a half the time that file data is delayed.
 * Similarly, directory updates are more critical, so are only delayed
 * about a third the time that file data is delayed. Thus, there are
 * SYNCER_MAXDELAY queues that are processed round-robin at a rate of
 * one each second (driven off the filesystem syner process). The
 * syncer_delayno variable indicates the next queue that is to be processed.
 * Items that need to be processed soon are placed in this queue:
 *
 *	syncer_workitem_pending[syncer_delayno]
 *
 * A delay of fifteen seconds is done by placing the request fifteen
 * entries later in the queue:
 *
 *	syncer_workitem_pending[(syncer_delayno + 15) & syncer_mask]
 *
 */

/*
 * Add an item to the syncer work queue.
 */
void
vn_syncer_add_to_worklist(vp, delay)
	struct vnode *vp;
	int delay;
{
	int s, slot;

	s = splbio();
	if (delay > syncer_maxdelay - 2)
		delay = syncer_maxdelay - 2;
	slot = (syncer_delayno + delay) & syncer_mask;
	LIST_INSERT_HEAD(&syncer_workitem_pending[slot], vp, v_synclist);
	splx(s);
}

/*
 * System filesystem synchronizer daemon.
 */

extern int lbolt;

void 
sched_sync(p)
	struct proc *p;
{
	struct synclist *slp;
	struct vnode *vp;
	long starttime;
	int s;

	for (;;) {
		starttime = time.tv_sec;

		/*
		 * Push files whose dirty time has expired.
		 */
		s = splbio();
		slp = &syncer_workitem_pending[syncer_delayno];
		syncer_delayno += 1;
		if (syncer_delayno == syncer_maxdelay)
			syncer_delayno = 0;
		splx(s);
		while ((vp = LIST_FIRST(slp)) != NULL) {
			vn_lock(vp, LK_EXCLUSIVE | LK_RETRY, p);
			(void) VOP_FSYNC(vp, p->p_ucred, MNT_LAZY, p);
			VOP_UNLOCK(vp, 0, p);
			if (LIST_FIRST(slp) == vp) {
				if (LIST_FIRST(&vp->v_dirtyblkhd) == NULL)
					panic("sched_sync: fsync failed");
				/*
				 * Move ourselves to the back of the sync list.
				 */
				LIST_REMOVE(vp, v_synclist);
				vn_syncer_add_to_worklist(vp, syncdelay);
			}
		}

		/*
		 * Do soft update processing.
		 */
		if (bioops.io_sync)
			(*bioops.io_sync)(NULL);

		/*
		 * If it has taken us less than a second to process the
		 * current work, then wait. Otherwise start right over
		 * again. We can still lose time if any single round
		 * takes more than two seconds, but it does not really
		 * matter as we are just trying to generally pace the
		 * filesystem activity.
		 */
		if (time.tv_sec == starttime)
			tsleep(&lbolt, PPAUSE, "syncer", 0);
	}
}

/*
 * Reassign a buffer from one vnode to another. Used to assign buffers
 * to the appropriate clean or dirty list and to add newly dirty vnodes
 * to the appropriate filesystem syncer list.
d588 1
a588 3
	struct buflists *listheadp;
	struct buf *wasdirty;
	int delay;
a596 1
	wasdirty = newvp->v_dirtyblkhd.lh_first;
d603 3
a605 1
	if ((bp->b_flags & B_DELWRI) == 0) {
a606 21
		if (wasdirty && LIST_FIRST(&newvp->v_dirtyblkhd) == NULL)
			LIST_REMOVE(newvp, v_synclist);
	} else {
		listheadp = &newvp->v_dirtyblkhd;
		if (LIST_FIRST(listheadp) == NULL) {
			switch (newvp->v_type) {
			case VDIR:
				delay = syncdelay / 3;
				break;
			case VBLK:
				if (newvp->v_specmountpoint != NULL) {
					delay = syncdelay / 2;
					break;
				}
				/* fall through */
			default:
				delay = syncdelay;
			}
			vn_syncer_add_to_worklist(newvp, delay);
		}
	}
d652 1
a652 2
	if (dev == NODEV) {
		*vpp = NULLVP;
a653 1
	}
a682 1
	struct proc *p = curproc;
a690 1
	simple_lock(&spechash_slock);
a691 1
		simple_lock(&vp->v_interlock);
d698 1
a698 2
			simple_unlock(&spechash_slock);
			vgonel(vp, p);
d701 1
a701 2
		if (vget(vp, LK_EXCLUSIVE | LK_INTERLOCK, p)) {
			simple_unlock(&spechash_slock);
a702 1
		}
d711 1
a711 1
		nvp->v_specmountpoint = NULL;
a712 1
		simple_unlock(&spechash_slock);
d714 1
a714 1
		if (vp != NULLVP) {
d721 2
a722 4
	simple_unlock(&spechash_slock);
	VOP_UNLOCK(vp, 0, p);
	simple_lock(&vp->v_interlock);
	vclean(vp, 0, p);
d739 3
a741 4
vget(vp, flags, p)
        struct vnode *vp;
	int flags;
	struct proc *p;
d743 1
a743 1
	int error;
d747 11
a757 8
	 * return failure. Cleaning is determined by checking that
	 * the VXLOCK flag is set.
	 */
	if ((flags & LK_INTERLOCK) == 0)
		simple_lock(&vp->v_interlock);
	if (vp->v_flag & VXLOCK) {
 		vp->v_flag |= VXWANT;
		simple_unlock(&vp->v_interlock);
d759 1
a759 9
		return (ENOENT);
 	}
	if (vp->v_usecount == 0) {
		simple_lock(&vnode_free_list_slock);
		if (vp->v_holdcnt > 0)
			TAILQ_REMOVE(&vnode_hold_list, vp, v_freelist);
		else
			TAILQ_REMOVE(&vnode_free_list, vp, v_freelist);
		simple_unlock(&vnode_free_list_slock);
d761 5
a765 9
 	vp->v_usecount++;
	if (flags & LK_TYPE_MASK) {
		if ((error = vn_lock(vp, flags | LK_INTERLOCK, p)) != 0) {
			vunref(vp);
			simple_unlock(&vp->v_interlock);
		}
		return (error);
	}
	simple_unlock(&vp->v_interlock);
d770 1
a770 111
 * Stubs to use when there is no locking to be done on the underlying object.
 * A minimal shared lock is necessary to ensure that the underlying object
 * is not revoked while an operation is in progress. So, an active shared
 * count is maintained in an auxillary vnode lock structure.
 */
int
vop_nolock(v)
	void *v;
{
	struct vop_lock_args /* {
		struct vnode *a_vp;
		int a_flags;
		struct proc *a_p;
	} */ *ap = v;

#ifdef notyet
	/*
	 * This code cannot be used until all the non-locking filesystems
	 * (notably NFS) are converted to properly lock and release nodes.
	 * Also, certain vnode operations change the locking state within
	 * the operation (create, mknod, remove, link, rename, mkdir, rmdir,
	 * and symlink). Ideally these operations should not change the
	 * lock state, but should be changed to let the caller of the
	 * function unlock them. Otherwise all intermediate vnode layers
	 * (such as union, umapfs, etc) must catch these functions to do
	 * the necessary locking at their layer. Note that the inactive
	 * and lookup operations also change their lock state, but this 
	 * cannot be avoided, so these two operations will always need
	 * to be handled in intermediate layers.
	 */
	struct vnode *vp = ap->a_vp;
	int vnflags, flags = ap->a_flags;

	if (vp->v_vnlock == NULL) {
		if ((flags & LK_TYPE_MASK) == LK_DRAIN)
			return (0);
		MALLOC(vp->v_vnlock, struct lock *, sizeof(struct lock),
		    M_VNODE, M_WAITOK);
		lockinit(vp->v_vnlock, PVFS, "vnlock", 0, 0);
	}
	switch (flags & LK_TYPE_MASK) {
	case LK_DRAIN:
		vnflags = LK_DRAIN;
		break;
	case LK_EXCLUSIVE:
	case LK_SHARED:
		vnflags = LK_SHARED;
		break;
	case LK_UPGRADE:
	case LK_EXCLUPGRADE:
	case LK_DOWNGRADE:
		return (0);
	case LK_RELEASE:
	default:
		panic("vop_nolock: bad operation %d", flags & LK_TYPE_MASK);
	}
	if (flags & LK_INTERLOCK)
		vnflags |= LK_INTERLOCK;
	return(lockmgr(vp->v_vnlock, vnflags, &vp->v_interlock, ap->a_p));
#else /* for now */
	/*
	 * Since we are not using the lock manager, we must clear
	 * the interlock here.
	 */
	if (ap->a_flags & LK_INTERLOCK)
		simple_unlock(&ap->a_vp->v_interlock);
 	return (0);
#endif
}
 
/*
 * Decrement the active use count.
 */

int
vop_nounlock(v)
	void *v;
{
	struct vop_unlock_args /* {
		struct vnode *a_vp;
		int a_flags;
		struct proc *a_p;
	} */ *ap = v;

	struct vnode *vp = ap->a_vp;

	if (vp->v_vnlock == NULL)
		return (0);
	return (lockmgr(vp->v_vnlock, LK_RELEASE, NULL, ap->a_p));
}

/*
 * Return whether or not the node is in use.
 */
int
vop_noislocked(v)
	void *v;
{
	struct vop_islocked_args /* {
		struct vnode *a_vp;
	} */ *ap = v;

	struct vnode *vp = ap->a_vp;

	if (vp->v_vnlock == NULL)
		return (0);
	return (lockstatus(vp->v_vnlock));
}

/*
 * Vnode reference.
d776 1
a776 1
	simple_lock(&vp->v_interlock);
a779 1
	simple_unlock(&vp->v_interlock);
d782 6
a787 4

int
vunref(vp)
	struct vnode *vp;
a788 25
#ifdef DIAGNOSTIC
	if (vp == NULL)
		panic("vrele: null vp");
#endif
	simple_lock (&vp->v_interlock);
	vp->v_usecount--;
	if (vp->v_usecount > 0) {
		simple_unlock(&vp->v_interlock);
		return (vp->v_usecount);
	}
#ifdef DIAGNOSTIC
	if (vp->v_usecount < 0 || vp->v_writecount != 0) {
		vprint("vrele: bad ref count", vp);
		panic("vrele: ref cnt");
	}
#endif
	/*
	 * insert at tail of LRU list
	 */
	simple_lock(&vnode_free_list_slock);
	if (vp->v_holdcnt > 0)
		TAILQ_INSERT_TAIL(&vnode_hold_list, vp, v_freelist);
	else
		TAILQ_INSERT_TAIL(&vnode_free_list, vp, v_freelist);
	simple_unlock(&vnode_free_list_slock);
d790 2
a791 1
	return (0);
d795 2
a796 1
 * vput(), just unlock and vrele()
d799 1
a799 1
vput(vp)
a801 1
	struct proc *p = curproc;	/* XXX */
d803 1
a803 1
#ifdef DIGANOSTIC
d805 1
a805 1
		panic("vput: null vp");
a806 1
	simple_lock(&vp->v_interlock);
d808 1
a808 3
	if (vp->v_usecount > 0) {
		simple_unlock(&vp->v_interlock);
		VOP_UNLOCK(vp, 0, p);
a809 1
	}
d811 3
a813 3
	if (vp->v_usecount < 0 || vp->v_writecount != 0) {
		vprint("vput: bad ref count", vp);
		panic("vput: ref cnt");
d819 2
a820 23
	simple_lock(&vnode_free_list_slock);
	if (vp->v_holdcnt > 0)
		TAILQ_INSERT_TAIL(&vnode_hold_list, vp, v_freelist);
	else
		TAILQ_INSERT_TAIL(&vnode_free_list, vp, v_freelist);
	simple_unlock(&vnode_free_list_slock);
	simple_unlock(&vp->v_interlock);
	VOP_INACTIVE(vp, p);
}

/*
 * Vnode release - use for active VNODES.
 * If count drops to zero, call inactive routine and return to freelist.
 */
void
vrele(vp)
	register struct vnode *vp;
{
	struct proc *p = curproc;

	if (vunref(vp) == 0 &&
	    vn_lock(vp, LK_EXCLUSIVE |LK_INTERLOCK, p) == 0)
		VOP_INACTIVE(vp, p);
a822 1
#ifdef DIAGNOSTIC
a830 19
	/*
	 * If it is on the freelist and the hold count is currently
	 * zero, move it to the hold list.
	 *
	 * The VGONEHACK flag reflects a call from getnewvnode,
	 * which will remove the vnode from the free list, but
	 * will not increment the ref count until after it calls vgone
	 * If the ref count we're incremented first, vgone would
	 * (incorrectly) try to close the previous instance of the
	 * underlying object.
	 */
  	simple_lock(&vp->v_interlock);
	if (!(vp->v_flag & VGONEHACK) &&
	    vp->v_holdcnt == 0 && vp->v_usecount == 0) {
		simple_lock(&vnode_free_list_slock);
		TAILQ_REMOVE(&vnode_free_list, vp, v_freelist);
		TAILQ_INSERT_TAIL(&vnode_hold_list, vp, v_freelist);
		simple_unlock(&vnode_free_list_slock);
	}
a831 1
	simple_unlock(&vp->v_interlock);
a841 1
	simple_lock(&vp->v_interlock);
a844 14
	/*
	 * If it is on the holdlist and the hold count drops to
	 * zero, move it to the free list. 
	 *
	 * See above for VGONEHACK
	 */
	if (!(vp->v_flag & VGONEHACK) &&
	    vp->v_holdcnt == 0 && vp->v_usecount == 0) {
		simple_lock(&vnode_free_list_slock);
		TAILQ_REMOVE(&vnode_hold_list, vp, v_freelist);
		TAILQ_INSERT_TAIL(&vnode_free_list, vp, v_freelist);
		simple_unlock(&vnode_free_list_slock);
	}
	simple_unlock(&vp->v_interlock);
a845 1
#endif /* DIAGNOSTIC */
a865 1
	struct proc *p = curproc;
d869 2
a870 1
	simple_lock(&mntvnode_slock);
a880 2
		
		simple_lock(&vp->v_interlock);
d884 1
a884 2
		if ((flags & SKIPSYSTEM) && (vp->v_flag & VSYSTEM)) {
			simple_unlock(&vp->v_interlock);
a885 1
		}
d891 1
a891 2
		    (vp->v_writecount == 0 || vp->v_type != VREG)) {
			simple_unlock(&vp->v_interlock);
a892 1
		}
d898 1
a898 3
			simple_unlock(&mntvnode_slock);
			vgonel(vp, p);
			simple_lock(&mntvnode_slock);
a906 1
			simple_unlock(&mntvnode_slock);
d908 1
a908 1
				vgonel(vp, p);
d910 1
a910 1
				vclean(vp, 0, p);
a913 1
			simple_lock(&mntvnode_slock);
a919 1
		simple_unlock(&vp->v_interlock);
a921 1
	simple_unlock(&mntvnode_slock);
a928 1
 * The vnode interlock is held on entry.
d931 1
a931 1
vclean(vp, flags, p)
a933 1
	struct proc *p;
d944 9
a952 2
		vp->v_usecount++;

a959 2

	
d961 1
a961 5
	 * Even if the count is zero, the VOP_INACTIVE routine may still
	 * have the object locked while it cleans it out. The VOP_LOCK
	 * ensures that the VOP_INACTIVE routine is done with its work.
	 * For active vnodes, it ensures that no other activity can
	 * occur while the underlying object is being cleaned out.
d963 2
a964 2
	VOP_LOCK(vp, LK_DRAIN | LK_INTERLOCK, p);

d966 2
a967 1
	 * Clean out any buffers associated with the vnode.
d969 1
a969 2
	if (flags & DOCLOSE)
		vinvalbuf(vp, V_SAVE, NOCRED, p, 0, 0);
d972 1
a972 2
	 * deactivated before being reclaimed. Note that the
	 * VOP_INACTIVE will unlock the vnode
d976 2
a977 8
			VOP_CLOSE(vp, FNONBLOCK, NOCRED, p);
		VOP_INACTIVE(vp, p);
	} else {
		/*
		 * Any other processes trying to obtain this lock must first
		 * wait for VXLOCK to clear, then call the new lock operation.
		 */
		VOP_UNLOCK(vp, 0, p);
a978 1

d982 1
a982 1
	if (VOP_RECLAIM(vp, p))
d984 2
a985 13
	if (active) {
		if (vunref(vp) == 0 &&
		    vp->v_holdcnt > 0)
			panic("vclean: not clean");
		simple_unlock(&vp->v_interlock);
	}
	cache_purge(vp);
	if (vp->v_vnlock) {
		if ((vp->v_vnlock->lk_flags & LK_DRAINED) == 0)
			vprint("vclean: lock not drained", vp);
		FREE(vp->v_vnlock, M_VNODE);
		vp->v_vnlock = NULL;
	}
d1003 3
a1005 3
int
vop_revoke(v)
	void *v;
d1007 1
a1007 11
	struct vop_revoke_args /* {
	        struct vnode *a_vp;
		int a_flags;
	} */ *ap = v;
	struct vnode *vp, *vq;
	struct proc *p = curproc;

#ifdef DIAGNOSTIC
	if ((ap->a_flags & REVOKEALL) == 0)
		panic("vop_revoke");
#endif
a1008 3
	vp = ap->a_vp;
	simple_lock(&vp->v_interlock);
 
d1016 2
a1017 3
			simple_unlock(&vp->v_interlock);
			tsleep((caddr_t)vp, PINOD, "vop_revokeall", 0);
			return(0);
a1023 1
		simple_unlock(&vp->v_interlock);
a1024 1
			simple_lock(&spechash_slock);
a1028 1
				simple_unlock(&spechash_slock);
a1037 1
		simple_lock(&vp->v_interlock);
d1040 1
a1040 25
	vgonel(vp, p);
	return (0);
}


/*
 * Recycle an unused vnode to the front of the free list.
 * Release the passed interlock if the vnode will be recycled.
 */
int
vrecycle(vp, inter_lkp, p)
	struct vnode *vp;
	struct simplelock *inter_lkp;
	struct proc *p;
{

	simple_lock(&vp->v_interlock);
	if (vp->v_usecount == 0) {
		if (inter_lkp)
			simple_unlock(inter_lkp);
		vgonel(vp, p);
		return (1);
	}
	simple_unlock(&vp->v_interlock);
	return (0);
a1050 14
	struct proc *p = curproc;

	simple_lock (&vp->v_interlock);
	vgonel(vp, p);
}

/*
 * vgone, with the vp interlock held.
 */
void
vgonel(vp, p)
	struct vnode *vp;
	struct proc *p;
{
a1059 1
		simple_unlock(&vp->v_interlock);
d1066 1
a1066 1
	vclean(vp, DOCLOSE, p);
d1070 1
a1070 2
	if (vp->v_mount != NULL)
		insmntque(vp, (struct mount *)0);
d1072 1
a1072 2
	 * If special device, remove it from special device alias list 
	 * if it is on one.
d1074 1
a1074 2
	if ((vp->v_type == VBLK || vp->v_type == VCHR) && vp->v_specinfo != 0) {
		simple_lock(&spechash_slock);
a1102 1
		simple_unlock(&spechash_slock);
d1108 16
a1123 14
	 * move it to the head of the list. 
	 *
	 * See above about the VGONEHACK
	 */
	if (vp->v_usecount == 0) {
		simple_lock(&vnode_free_list_slock);
		if (vp->v_holdcnt > 0)
			panic("vgonel: not clean");		
		if (!(vp->v_flag & VGONEHACK) &&
		    TAILQ_FIRST(&vnode_free_list) != vp) {
			TAILQ_REMOVE(&vnode_free_list, vp, v_freelist);
			TAILQ_INSERT_HEAD(&vnode_free_list, vp, v_freelist);
		}
		simple_unlock(&vnode_free_list_slock);
a1137 1
	int rc =0;
a1138 1
	simple_lock(&spechash_slock);
d1143 1
a1143 2
		rc = 1;
		break;
d1145 1
a1145 2
	simple_unlock(&spechash_slock);
	return (rc);
d1153 1
a1153 1
	struct vnode *vp;
d1155 1
a1155 1
	struct vnode *vq, *vnext;
a1160 1
	simple_lock(&spechash_slock);
a1168 1
			simple_unlock(&spechash_slock);
a1173 1
	simple_unlock(&spechash_slock);
d1228 1
a1228 2
	struct proc *p = curproc;
	register struct mount *mp, *nmp;
a1231 1
	simple_lock(&mountlist_slock);
d1233 1
a1233 5
	     mp = nmp) {
		if (vfs_busy(mp, LK_NOWAIT, &mountlist_slock, p)) { 
			nmp = mp->mnt_list.cque_next;
			continue;
		}
d1236 1
a1236 1
		     vp = vp->v_mntvnodes.le_next) {
d1239 1
a1239 6
		simple_lock(&mountlist_slock);
		nmp = mp->mnt_list.cqe_next;
		vfs_unbusy(mp, p);
 	}
	simple_unlock(&mountlist_slock);

a1242 45
/*
 * Top level filesystem related information gathering.
 */
int
vfs_sysctl(name, namelen, oldp, oldlenp, newp, newlen, p)
	int *name;
	u_int namelen;
	void *oldp;
	size_t *oldlenp;
	void *newp;
	size_t newlen;
	struct proc *p;
{
	struct vfsconf *vfsp;

	/* all sysctl names at this level are at least name and field */
	if (namelen < 2)
		return (ENOTDIR);		/* overloaded */
	if (name[0] != VFS_GENERIC) {
		for (vfsp = vfsconf; vfsp; vfsp = vfsp->vfc_next)
			if (vfsp->vfc_typenum == name[0])
				break;
		if (vfsp == NULL)
			return (EOPNOTSUPP);
		return ((*vfsp->vfc_vfsops->vfs_sysctl)(&name[1], namelen - 1,
		    oldp, oldlenp, newp, newlen, p));
	}
	switch (name[1]) {
	case VFS_MAXTYPENUM:
		return (sysctl_rdint(oldp, oldlenp, newp, maxvfsconf));
	case VFS_CONF:
		if (namelen < 3)
			return (ENOTDIR);	/* overloaded */
		for (vfsp = vfsconf; vfsp; vfsp = vfsp->vfc_next)
			if (vfsp->vfc_typenum == name[2])
				break;
		if (vfsp == NULL)
			return (EOPNOTSUPP);
		return (sysctl_rdstruct(oldp, oldlenp, newp, vfsp,
		    sizeof(struct vfsconf)));
	}
	return (EOPNOTSUPP);
}


d1252 1
a1252 1
sysctl_vnode(where, sizep, p)
a1254 1
	struct proc *p;
d1257 1
a1257 1
	struct vnode *vp, *nvp;
a1269 1
	simple_lock(&mountlist_slock);
d1271 2
a1272 2
		if (vfs_busy(mp, LK_NOWAIT, &mountlist_slock, p)) {
			nmp = mp->mnt_list.cqe_next;
a1273 1
		}
d1278 1
a1278 1
		     vp = nvp) {
a1284 1
				simple_unlock(&mntvnode_slock);
a1289 1
			nvp = vp->v_mntvnodes.le_next;
a1290 1
				simple_unlock(&mntvnode_slock);
a1297 1
			simple_lock(&mntvnode_slock);
d1299 1
a1299 5

		simple_unlock(&mntvnode_slock);
		simple_lock(&mountlist_slock);
		nmp = mp->mnt_list.cqe_next;
		vfs_unbusy(mp, p);
a1301 2
	simple_unlock(&mountlist_slock);

a1313 1
	int error = 0;
d1315 1
a1315 1
 	if (vp->v_specmountpoint != NULL)
a1317 1
		simple_lock(&spechash_slock);
d1322 3
a1324 6
			if (vq->v_specmountpoint != NULL) {
				error = EBUSY;
				break;
			}
 		}
		simple_unlock(&spechash_slock);
d1326 1
a1326 1
	return (error);
d1333 1
a1333 1
int
d1407 1
a1407 1
int
d1422 1
a1422 1
void
a1668 158

/*
 * Routine to create and manage a filesystem syncer vnode.
 */
#define sync_close nullop
int   sync_fsync __P((void *));
int   sync_inactive __P((void *));
#define sync_reclaim nullop
#define sync_lock vop_nolock
#define sync_unlock vop_nounlock
int   sync_print __P((void *));
#define sync_islocked vop_noislocked
 
int (**sync_vnodeop_p) __P((void *));
struct vnodeopv_entry_desc sync_vnodeop_entries[] = {
      { &vop_default_desc, vn_default_error },
      { &vop_close_desc, sync_close },                /* close */
      { &vop_fsync_desc, sync_fsync },                /* fsync */
      { &vop_inactive_desc, sync_inactive },          /* inactive */
      { &vop_reclaim_desc, sync_reclaim },            /* reclaim */
      { &vop_lock_desc, sync_lock },                  /* lock */
      { &vop_unlock_desc, sync_unlock },              /* unlock */
      { &vop_print_desc, sync_print },                /* print */
      { &vop_islocked_desc, sync_islocked },          /* islocked */
      { (struct vnodeop_desc*)NULL, (int(*) __P((void *)))NULL }
};
struct vnodeopv_desc sync_vnodeop_opv_desc =
      { &sync_vnodeop_p, sync_vnodeop_entries };

/*
 * Create a new filesystem syncer vnode for the specified mount point.
 */
int
vfs_allocate_syncvnode(mp)
      struct mount *mp;
{
      struct vnode *vp;
      static long start, incr, next;
      int error;

      /* Allocate a new vnode */
      if ((error = getnewvnode(VT_VFS, mp, sync_vnodeop_p, &vp)) != 0) {
              mp->mnt_syncer = NULL;
              return (error);
      }
      vp->v_writecount = 1;
      vp->v_type = VNON;
      /*
       * Place the vnode onto the syncer worklist. We attempt to
       * scatter them about on the list so that they will go off
       * at evenly distributed times even if all the filesystems
       * are mounted at once.
       */
      next += incr;
      if (next == 0 || next > syncer_maxdelay) {
              start /= 2;
              incr /= 2;
              if (start == 0) {
                      start = syncer_maxdelay / 2;
                      incr = syncer_maxdelay;
              }
              next = start;
      }
      vn_syncer_add_to_worklist(vp, next);
      mp->mnt_syncer = vp;
      return (0);
}

/*
 * Do a lazy sync of the filesystem.
 */
int
sync_fsync(v)
	void *v;
{
      struct vop_fsync_args /* {
              struct vnode *a_vp;
              struct ucred *a_cred;
              int a_waitfor;
              struct proc *a_p;
      } */ *ap = v;

      struct vnode *syncvp = ap->a_vp;
      struct mount *mp = syncvp->v_mount;
      int asyncflag;

      /*
       * We only need to do something if this is a lazy evaluation.
       */
      if (ap->a_waitfor != MNT_LAZY)
              return (0);

      /*
       * Move ourselves to the back of the sync list.
       */
      LIST_REMOVE(syncvp, v_synclist);
      vn_syncer_add_to_worklist(syncvp, syncdelay);

      /*
       * Walk the list of vnodes pushing all that are dirty and
       * not already on the sync list.
       */
      simple_lock(&mountlist_slock);
      if (vfs_busy(mp, LK_NOWAIT, &mountlist_slock, ap->a_p) == 0) {
              asyncflag = mp->mnt_flag & MNT_ASYNC;
              mp->mnt_flag &= ~MNT_ASYNC;
              VFS_SYNC(mp, MNT_LAZY, ap->a_cred, ap->a_p);
              if (asyncflag)
                      mp->mnt_flag |= MNT_ASYNC;
              vfs_unbusy(mp, ap->a_p);
      }
      return (0);
}

/*
 * The syncer vnode is no longer needed and is being decommissioned.
 */
int
sync_inactive(v)
	void *v;
	
{
      struct vop_inactive_args /* {
               struct vnode *a_vp;
               struct proc *a_p;
      } */ *ap = v;

      struct vnode *vp = ap->a_vp;

      if (vp->v_usecount == 0)
              return (0);
      vp->v_mount->mnt_syncer = NULL;
      LIST_REMOVE(vp, v_synclist);
      vp->v_writecount = 0;
      vput(vp);
      return (0);
}

/*
 * Print out a syncer vnode.
 */
int
sync_print(v)
	void *v;

{
      struct vop_print_args /* {
              struct vnode *a_vp;
      } */ *ap = v;
      struct vnode *vp = ap->a_vp;

      printf("syncer vnode");
      if (vp->v_vnlock != NULL)
              lockmgr_printinfo(vp->v_vnlock);
      printf("\n");
      return (0);
}

@


1.10
log
@proper mask check; mike@@fast.cs.utah.edu
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.9 1997/04/14 04:23:26 tholo Exp $	*/
d54 1
d92 4
a95 1
TAILQ_HEAD(freelst, vnode) vnode_free_list;	/* vnode free list */
d97 17
a130 1
int vget __P((struct vnode *, int));
d134 1
d140 1
d151 1
a151 1
static int vfs_hang_addrlist __P((struct mount *, struct netexport *,
d153 2
a154 2
static int vfs_free_netcred __P((struct radix_node *, void *));
static void vfs_free_addrlist __P((struct netexport *));
d167 4
d172 1
d174 7
d183 1
d185 2
a186 2
 * Lock a filesystem.
 * Used to prevent access to it while mounting and unmounting.
d188 1
d190 5
a194 2
vfs_lock(mp)
	register struct mount *mp;
d196 1
d198 3
a200 1
	while (mp->mnt_flag & MNT_MLOCK) {
d202 12
a213 1
		tsleep((caddr_t)mp, PVFS, "vfslock", 0);
d215 6
a220 2
	mp->mnt_flag |= MNT_MLOCK;
	return (0);
d223 1
d225 1
a225 2
 * Unlock a locked filesystem.
 * Panic if filesystem is not locked.
d228 3
a230 2
vfs_unlock(mp)
	register struct mount *mp;
d232 1
a232 8

	if ((mp->mnt_flag & MNT_MLOCK) == 0)
		panic("vfs_unlock: not locked");
	mp->mnt_flag &= ~MNT_MLOCK;
	if (mp->mnt_flag & MNT_MWAIT) {
		mp->mnt_flag &= ~MNT_MWAIT;
		wakeup((caddr_t)mp);
	}
d236 4
a239 2
 * Mark a mount point as busy.
 * Used to synchronize access and to delay unmounting.
d241 41
d283 1
a283 2
vfs_busy(mp)
	register struct mount *mp;
d285 14
a298 9

	while(mp->mnt_flag & MNT_MPBUSY) {
		mp->mnt_flag |= MNT_MPWANT;
		tsleep((caddr_t)&mp->mnt_flag, PVFS, "vfsbusy", 0);
	}
	if (mp->mnt_flag & MNT_UNMOUNT)
		return (1);
	mp->mnt_flag |= MNT_MPBUSY;
	return (0);
d300 1
a300 19

/*
 * Free a busy filesystem.
 * Panic if filesystem is not busy.
 */
void
vfs_unbusy(mp)
	register struct mount *mp;
{

	if ((mp->mnt_flag & MNT_MPBUSY) == 0)
		panic("vfs_unbusy: not busy");
	mp->mnt_flag &= ~MNT_MPBUSY;
	if (mp->mnt_flag & MNT_MPWANT) {
		mp->mnt_flag &= ~MNT_MPWANT;
		wakeup((caddr_t)&mp->mnt_flag);
	}
}

d305 1
a305 1
getvfs(fsid)
d310 1
d312 1
a312 1
	     mp = mp->mnt_list.cqe_next)
d314 2
a315 1
		    mp->mnt_stat.f_fsid.val[1] == fsid->val[1])
d317 3
d323 1
d328 1
a328 1
getnewfsid(mp, mtype)
a329 1
	int mtype;
d334 1
d336 3
a338 1
	mp->mnt_stat.f_fsid.val[0] = makedev(nblkdev + 11, 0);	/* XXX */
d342 1
a342 1
	tfsid.val[0] = makedev((nblkdev + mtype) & 0xff, xxxfs_mntid);
d345 1
a345 1
		while (getvfs(&tfsid)) {
d351 1
d410 4
a413 1
	register struct vnode *vp;
d418 25
a442 3
	if ((vnode_free_list.tqh_first == NULL &&
	     numvnodes < 2 * desiredvnodes) ||
	    numvnodes < desiredvnodes) {
d448 12
a459 1
		if ((vp = vnode_free_list.tqh_first) == NULL) {
d468 1
a468 1
		TAILQ_REMOVE(&vnode_free_list, vp, v_freelist);
d470 2
a471 1
		vp->v_freelist.tqe_prev = (struct vnode **)0xdeadb;
d474 3
a476 1
			vgone(vp);
d516 1
a516 1

d520 1
d526 3
a528 3
	if ((vp->v_mount = mp) == NULL)
		return;
	LIST_INSERT_HEAD(&mp->mnt_vnodelist, vp, v_mntvnodes);
d567 1
a567 1
	if (flags & V_SAVE) {
d574 2
a575 1
		if ((blist = vp->v_cleanblkhd.lh_first) && flags & V_SAVEMETA)
d695 1
d697 1
a697 1
	if (bp->b_vp == (struct vnode *) 0)
d702 1
d705 2
a706 1
	vp = bp->b_vp;
d712 108
a819 3
 * Reassign a buffer from one vnode to another.
 * Used to assign file specific control information
 * (indirect blocks) to the vnode to which they belong.
d826 3
a828 1
	register struct buflists *listheadp;
d837 1
d844 5
a848 1
	if (bp->b_flags & B_DELWRI)
d850 17
a866 2
	else
		listheadp = &newvp->v_cleanblkhd;
d912 2
a913 1
	if (dev == NODEV)
d915 1
d945 1
d954 1
d956 1
d963 2
a964 1
			vgone(vp);
d967 2
a968 1
		if (vget(vp, 1))
d970 1
d979 1
a979 1
		nvp->v_specflags = 0;
d981 1
d983 1
a983 1
		if (vp != NULL) {
d990 4
a993 2
	VOP_UNLOCK(vp);
	vclean(vp, 0);
d1010 4
a1013 3
vget(vp, lockflag)
	register struct vnode *vp;
	int lockflag;
d1015 1
a1015 1

d1019 8
a1026 11
	 * return failure. Cleaning is determined either by checking
	 * that the VXLOCK flag is set, or that the use count is
	 * zero with the back pointer set to show that it has been
	 * removed from the free list by getnewvnode. The VXLOCK
	 * flag may not have been set yet because vclean is blocked in
	 * the VOP_LOCK call waiting for the VOP_INACTIVE to complete.
	 */
	if ((vp->v_flag & VXLOCK) ||
	    (vp->v_usecount == 0 &&
	     vp->v_freelist.tqe_prev == (struct vnode **)0xdeadb)) {
		vp->v_flag |= VXWANT;
d1028 17
a1044 1
		return (1);
d1046 1
a1046 5
	if (vp->v_usecount == 0)
		TAILQ_REMOVE(&vnode_free_list, vp, v_freelist);
	vp->v_usecount++;
	if (lockflag)
		VOP_LOCK(vp);
d1051 111
a1161 1
 * Vnode reference, just increment the count
d1167 1
a1167 1

d1171 1
d1174 4
a1177 6
/*
 * vput(), just unlock and vrele()
 */
void
vput(vp)
	register struct vnode *vp;
d1179 25
d1205 1
a1205 2
	VOP_UNLOCK(vp);
	vrele(vp);
d1209 1
a1209 2
 * Vnode release.
 * If count drops to zero, call inactive routine and return to freelist.
d1212 1
a1212 1
vrele(vp)
d1215 1
d1217 1
a1217 1
#ifdef DIAGNOSTIC
d1219 1
a1219 1
		panic("vrele: null vp");
d1221 1
d1223 3
a1225 1
	if (vp->v_usecount > 0)
d1227 1
d1229 3
a1231 3
	if (vp->v_usecount != 0 || vp->v_writecount != 0) {
		vprint("vrele: bad ref count", vp);
		panic("vrele: ref cnt");
d1237 23
a1259 2
	TAILQ_INSERT_TAIL(&vnode_free_list, vp, v_freelist);
	VOP_INACTIVE(vp);
d1262 1
d1271 19
d1291 1
d1302 1
d1306 14
d1321 1
d1342 1
d1346 1
a1346 2
	if ((mp->mnt_flag & MNT_MPBUSY) == 0)
		panic("vflush: not busy");
d1357 2
d1362 2
a1363 1
		if ((flags & SKIPSYSTEM) && (vp->v_flag & VSYSTEM))
d1365 1
d1371 2
a1372 1
		    (vp->v_writecount == 0 || vp->v_type != VREG))
d1374 1
d1380 3
a1382 1
			vgone(vp);
d1391 1
d1393 1
a1393 1
				vgone(vp);
d1395 1
a1395 1
				vclean(vp, 0);
d1399 1
d1406 1
d1409 1
d1417 1
d1420 1
a1420 1
vclean(vp, flags)
d1423 1
d1434 11
a1444 1
		VREF(vp);
d1452 2
a1453 8
	VOP_LOCK(vp);
	/*
	 * Prevent the vnode from being recycled or
	 * brought into use while we clean it out.
	 */
	if (vp->v_flag & VXLOCK)
		panic("vclean: deadlock");
	vp->v_flag |= VXLOCK;
d1458 1
a1458 6
		vinvalbuf(vp, V_SAVE, NOCRED, NULL, 0, 0);
	/*
	 * Any other processes trying to obtain this lock must first
	 * wait for VXLOCK to clear, then call the new lock operation.
	 */
	VOP_UNLOCK(vp);
d1461 2
a1462 1
	 * deactivated before being reclaimed.
d1466 8
a1473 2
			VOP_CLOSE(vp, FNONBLOCK, NOCRED, NULL);
		VOP_INACTIVE(vp);
d1475 1
d1479 1
a1479 1
	if (VOP_RECLAIM(vp))
d1481 13
a1493 2
	if (active)
		vrele(vp);
d1511 3
a1513 3
void
vgoneall(vp)
	register struct vnode *vp;
d1515 11
a1525 1
	register struct vnode *vq;
d1527 3
d1537 3
a1539 2
			tsleep((caddr_t)vp, PINOD, "vgoneall", 0);
			return;
d1546 1
d1548 1
d1553 1
d1563 1
d1566 25
a1590 1
	vgone(vp);
d1601 14
d1624 1
d1631 1
a1631 1
	vclean(vp, DOCLOSE);
d1635 2
a1636 1
	insmntque(vp, (struct mount *)0);
d1638 2
a1639 1
	 * If special device, remove it from special device alias list.
d1641 2
a1642 1
	if (vp->v_type == VBLK || vp->v_type == VCHR) {
d1671 1
d1677 14
a1690 16
	 * move it to the head of the list. The test of the back
	 * pointer and the reference count of zero is because
	 * it will be removed from the free list by getnewvnode,
	 * but will not have its reference count incremented until
	 * after calling vgone. If the reference count were
	 * incremented first, vgone would (incorrectly) try to
	 * close the previous instance of the underlying object.
	 * So, the back pointer is explicitly set to `0xdeadb' in
	 * getnewvnode after removing it from the freelist to ensure
	 * that we do not try to move it here.
	 */
	if (vp->v_usecount == 0 &&
	    vp->v_freelist.tqe_prev != (struct vnode **)0xdeadb &&
	    vnode_free_list.tqh_first != vp) {
		TAILQ_REMOVE(&vnode_free_list, vp, v_freelist);
		TAILQ_INSERT_HEAD(&vnode_free_list, vp, v_freelist);
d1705 1
d1707 1
d1712 2
a1713 1
		return (1);
d1715 2
a1716 1
	return (0);
d1724 1
a1724 1
	register struct vnode *vp;
d1726 1
a1726 1
	register struct vnode *vq, *vnext;
d1732 1
d1741 1
d1747 1
d1802 2
a1803 1
	register struct mount *mp;
d1807 1
d1809 5
a1813 1
	     mp = mp->mnt_list.cqe_next) {
d1816 1
a1816 1
		     vp = vp->v_mntvnodes.le_next)
d1819 49
d1869 1
d1871 1
a1871 1
#endif
d1882 1
a1882 1
sysctl_vnode(where, sizep)
d1885 1
d1888 1
a1888 1
	struct vnode *vp;
d1901 1
d1903 2
a1904 2
		nmp = mp->mnt_list.cqe_next;
		if (vfs_busy(mp))
d1906 1
d1911 1
a1911 1
		     vp = vp->v_mntvnodes.le_next) {
d1918 1
d1924 1
d1926 1
d1934 1
d1936 5
a1940 1
		vfs_unbusy(mp);
d1943 2
d1957 1
d1959 1
a1959 1
	if (vp->v_specflags & SI_MOUNTEDON)
d1962 1
d1967 6
a1972 3
			if (vq->v_specflags & SI_MOUNTEDON)
				return (EBUSY);
		}
d1974 1
a1974 1
	return (0);
d1981 1
a1981 1
static int
d2055 1
a2055 1
static int
d2070 1
a2070 1
static void
d2317 158
@


1.9
log
@Minor performance enhancements from NetBSD
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.8 1997/02/24 14:20:02 niklas Exp $	*/
d1368 1
a1368 1
		error = copyin(argp->ex_addr, (caddr_t)smask, argp->ex_masklen);
@


1.8
log
@OpenBSD tags
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_subr.c,v 1.53 1996/04/22 01:39:13 christos Exp $	*/
d470 1
a470 2
			bremfree(bp);
			bp->b_flags |= B_BUSY;
d507 1
a507 2
		bremfree(bp);
		bp->b_flags |= B_BUSY;
@


1.7
log
@Add fs_id support and random inode generation numbers for ffs.
@
text
@d1 1
@


1.6
log
@spec_advlock() via lf_advlock()
@
text
@d266 2
@


1.5
log
@Make {,f}chown(2) behaviour POSIX.1 compliant with SUID / SGID files
Enable CTL_FS processing by sysctl(3)
Add CTL_FS request to disable clearing SUID / SGID bit when a files owner
or group is changed by root
Make sysctl(8) understand CTL_FS requests
@
text
@d711 1
@


1.4
log
@sync syscalls, no sys/cpu.h
@
text
@d80 1
d1613 53
@


1.3
log
@partial sync with netbsd 960418, more to come
@
text
@d1 1
a1 1
/*	$NetBSD: vfs_subr.c,v 1.52 1996/03/16 23:17:20 christos Exp $	*/
a62 1
#include <sys/cpu.h>
@


1.2
log
@From NetBSD: Merge with NetBSD 960217
@
text
@d1 1
a1 1
/*	$NetBSD: vfs_subr.c,v 1.51 1996/02/09 19:01:01 christos Exp $	*/
d1190 1
a1190 1
	printf("type %s, usecount %d, writecount %d, refcount %d,",
@


1.1
log
@Initial revision
@
text
@d1 1
a1 1
/*	$NetBSD: vfs_subr.c,v 1.47 1995/10/07 06:28:48 mycroft Exp $	*/
d62 2
d93 44
d140 1
d152 1
d187 1
d206 1
d295 3
a297 3
		vap->va_atime.ts_sec = vap->va_atime.ts_nsec =
		vap->va_mtime.ts_sec = vap->va_mtime.ts_nsec =
		vap->va_ctime.ts_sec = vap->va_ctime.ts_nsec =
d305 1
a305 2
extern int (**dead_vnodeop_p)();
extern void vclean();
d311 1
d315 1
a315 1
	int (**vops)();
d319 1
d321 1
d380 1
d402 1
d409 1
a409 1
	if (vp = bp->b_vp) {
d436 1
a436 1
		if (error = VOP_FSYNC(vp, cred, MNT_WAIT, p))
d536 1
d559 1
d582 1
d614 1
d627 1
d641 1
d653 1
a653 1
	error = getnewvnode(VT_NON, (struct mount *)0, spec_vnodeop_p, &nvp);
d660 1
a660 1
	if (nvp = checkalias(vp, dev, (struct mount *)0)) {
d858 1
d941 1
a941 1
	if (active = vp->v_usecount)
d1068 1
a1068 4
	if (vp->v_mount != NULL) {
		LIST_REMOVE(vp, v_mntvnodes);
		vp->v_mount = NULL;
	}
d1129 1
d1223 1
d1249 1
d1359 2
a1360 1
	if (error = copyin(argp->ex_addr, (caddr_t)saddr, argp->ex_addrlen))
d1408 1
a1408 1
	caddr_t w;
d1428 2
a1429 3
		if (rnh = nep->ne_rtable[i]) {
			(*rnh->rnh_walktree)(rnh, vfs_free_netcred,
			    (caddr_t)rnh);
d1448 1
a1448 1
		if (error = vfs_hang_addrlist(mp, nep, argp))
a1503 2
	int i;
	register gid_t *gp;
d1557 1
a1557 1
		if (error = dounmount(mp, MNT_FORCE, &proc0)) {
d1589 1
a1589 1
		sys_sync(&proc0, (void *)0, (int *)0);
d1596 1
a1596 1
	sys_sync(&proc0, (void *)0, (int *)0);
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@
